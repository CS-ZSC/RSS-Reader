<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet type='text/xsl' href='http://spectrum.ieee.org/rss/style.xsl'?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0">
<channel>
<title>IEEE Spectrum Computing</title>
<link>http://spectrum.ieee.org/computing</link>
<description>IEEE Spectrum Computing recent content</description>
<pubDate>Fri, 28 Oct 2016 20:30:00 GMT</pubDate>
<atom:link rel="self" type="application/rss+xml" href="http://spectrum.ieee.org/rss/computing/fulltext"/>
<item>
<title>Why the Next Denial-of-Service Attack Could Be Against Your Car</title>
<link>http://spectrum.ieee.org/view-from-the-valley/transportation/safety/why-the-next-denial-of-service-attack-could-be-against-your-car</link>
<description>The "Jeep hacker" says denial-of-service attacks against cars are easy hacks—and urges people not to buy any car dongles</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The "Jeep hacker" says denial-of-service attacks against cars are easy hacks—and urges people not to buy any car dongles&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyNTg4NQ.jpeg"/&gt;
&lt;figcaption&gt;Illustration: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;We haven’t seen the last of the car hacks, says Charlie Miller, the security researcher who &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/systems/black-hat-2014-hacking-the-smart-car"&gt;in 2014 helped show&lt;/a&gt; that hackers can take control of certain models of cars, messing with brakes and steering and other systems while the cars are in motion.&lt;/p&gt;
&lt;p&gt;Speaking this week at &lt;a shape="rect" href="http://www.armtechcon.com/"&gt;ARM TechCon&lt;/a&gt;, held in Santa Clara, Calif., Miller said that carmakers “are not in good shape now,” but there’s hope for the future as the companies begin to understand the risks when vehicles are connected to the outside world.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="a list of vehicles vulnerable to car hacking, including 2013, 2014, and 2015 models of Dodge, Chrysler, and Jeep vehicles" src="http://spectrum.ieee.org/image/MjgyNTkzOA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Tekla S. Perry&lt;/figcaption&gt;
&lt;figcaption&gt;Charlie Miller and Chris Valasek were able to scan the Sprint network for vulnerable vehicles. “I’m a good guy, but I was tempted when I found the Viper. I didn’t do it, but I could have changed the radio station so easily,” Miller said.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Miller, &lt;span&gt;currently an engineer at Uber,&lt;/span&gt; pointed out the difference between two categories of car hacks—hacks limited to the mobile app or to the head unit (the centerpiece of the audio system), and hacks that reach into the car’s controller area network (CAN) bus.&lt;/p&gt;
&lt;p&gt;The latter are significantly more dangerous because brakes, steering, and other critical controls connect to the CAN bus. Yet mobile and head-unit hacks can go beyond simply changing the radio station.&lt;/p&gt;
&lt;p&gt;Consider the recently detected vulnerability in the &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/advanced-cars/nissan-disables-app-that-let-you-and-everyone-else-remotely-access-leafs"&gt;Nissan Leaf mobile app&lt;/a&gt;, Miller suggested (it has since been fixed). The password, he says, was the vehicle identification number, typically easy to see through a windshield. You could log in as the owner and, say, turn on the seat heaters. It’s not a safety issue, but, as Miller points out, that’s an easy way to kill a car’s battery. That’s “a denial-of-service attack against a car. Not dangerous particularly, but we’ll see more and more of these,” he says.&lt;/p&gt;
&lt;aside class="inlay pullquote rt med-lrg"&gt;Figuring out how to reprogram the [Jeep] chip wasn’t easy. “I would screw it up, and my head unit wouldn’t work anymore. Thank you, Chrysler and their warranty system; eventually we figured out how to reprogram it without breaking it” &lt;span class="pq-attrib"&gt;—Charlie Miller&lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;The Jeep attack that made Miller and his partner-in-hacking, Chris Valasek, famous was a CAN bus attack. Miller discovered that, although the ARM chip that controlled the entertainment system wasn’t directly connected to the CAN bus, it did connect to a chip that was. And, through that connection, that second chip could be reprogrammed.&lt;/p&gt;
&lt;p&gt;Figuring out how to reprogram the chip wasn’t easy. Recalls Miller:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“I would screw it up, and my head unit wouldn’t work anymore, I would have to go to the dealer, with my busted head unit. It was a real lemon; that thing broke all the time. They would fix it and get me back on the road. Thank you, Chrysler and their warranty system; eventually we figured out how to reprogram it without breaking it.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="The chip that controls the audio system in cars today typically connects to a chip on the CAN bus" src="http://spectrum.ieee.org/image/MjgyNTkzOQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Tekla S. Perry&lt;/figcaption&gt;
&lt;figcaption&gt;The chip that controls the audio system in cars today typically connects to a chip on the controller area network (CAN) bus. Compromising this processor can give a hacker access to steering, brakes, and various safety systems.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The most obvious fix for future car designers, Miller says, would be to simply not connect the two chips. But car owners like the features that such a connection enables—car audio systems that raise the volume as speed and road noise increase or guide lines that appear on rear-camera screens to indicate where your current path will lead.&lt;/p&gt;
&lt;p&gt;“We will get more and more of these features,” Miller said, “including cars talking to other cars. There isn’t an option to disconnect this, so we need to figure out how to protect it.”&lt;/p&gt;
&lt;p&gt;And some fixes, he said, work better than others. To shut down Jeep’s vulnerability, Miller explained, Jeep eventually went to Sprint, who provided cellular links for Jeep vehicles, and Sprint made that connection more secure. “They didn’t fix the code signing piece,” Miller said. “If I could get into a Jeep [head unit] I could still reprogram the gateway” to the CAN bus.&lt;/p&gt;
&lt;p&gt;By contrast, Miller pointed out, &lt;a shape="rect" href="https://www.wired.com/2016/09/tesla-responds-chinese-hack-major-security-upgrade/"&gt;after Tesla was similarly hacked&lt;/a&gt; by researchers from China this year, Tesla changed the gateway between the two processors to require that any code sent from one to the other be signed to prove it is authorized by the manufacturer. “So now when you read about a Tesla hack,” he said, “it is limited to whatever the head unit can do,” and doesn’t affect the car controls.&lt;/p&gt;
&lt;aside class="inlay pullquote rt med-lrg"&gt;You know those dongles that allow insurers to track your car’s operations or allow you to do so yourself via an app? Don’t even think of using one&lt;/aside&gt;
&lt;p&gt;While these kinds of security patches are made public, Miller says, generally the car companies aren’t talking about what efforts they are making to improve security. “I would like more transparency; I’d like to see white papers from car companies that explain how they are designing systems for security,” he says.&lt;/p&gt;
&lt;p&gt;In the meantime, is there anything a car owner can do?&lt;/p&gt;
&lt;p&gt;Not much, says Miller. “You can’t download antivirus software” or add in other security patches yourself.&lt;/p&gt;
&lt;p&gt;But you can avoid making your car’s vulnerabilities worse. You know those dongles that allow insurers to track your car’s operations or allow you to &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/advanced-cars/smartphone-gadget-makes-old-cars-smart"&gt;do so yourself&lt;/a&gt; via an app? (Some examples include Progressive’s Snapshot and Allstate’s Drive Wise.) Don’t even think about using one, Miller says: The safety risks are not worth the insurance discounts or convenience.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 28 Oct 2016 20:30:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/transportation/safety/why-the-next-denial-of-service-attack-could-be-against-your-car</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyNTg5Nw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyNTg5NQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Breaking the Multicore Bottleneck</title>
<link>http://spectrum.ieee.org/semiconductors/processors/breaking-the-multicore-bottleneck</link>
<description>Simple hardware speeds core-to-core communication</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Simple hardware speeds core-to-core communication&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMjYxNw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Intel&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="/image/MjgyMjYxNg.jpeg" src="http://spectrum.ieee.org/image/MjgyMjYxNg.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Intel&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;It’s Getting Crowded:&lt;/strong&gt;  This Intel Haswell-EX Xeon E7 V3 processor has 18 cores trying to work together without messing up one another’s calculations. A bit of additional hardware could speed up communication among the cores.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Researchers at North Carolina State University and at Intel have come up with a solution to one of the modern microprocessor’s most persistent problems: communication among the processor’s many cores. Their answer is a dedicated set of logic circuits they call the Queue Management Device, or QMD. In simulations, integrating the QMD with the processor’s on-chip network at a minimum doubled core-to-core communication speed and, in some cases, boosted it much further. Even better, as the number of cores was increased, the speedup became more pronounced.&lt;/p&gt;
&lt;p&gt;In the last decade, microprocessor designers started putting &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/processors/multicore-cpu-processor-proliferation"&gt;multiple copies of processor cores on a single die&lt;/a&gt; as a way to continue the rate of performance improvement computer makers had enjoyed without causing chip-killing hot spots to form on the CPU. But that solution comes with &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/the-trouble-with-multicore"&gt;complications&lt;/a&gt;. For one, it means that software programs have to be written so that work is divided among processor cores. The result: Sometimes different cores need to work on the same data or must coordinate the passing of data from one core to another.&lt;/p&gt;
&lt;aside class="inlay pullquote lt med"&gt; “We have to improve performance by improving energy efficiency. The only way to do that is to move some software to hardware”&lt;span class="pq-attrib"&gt;—Yan Solihin, North Carolina State University &lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;To prevent the cores from wantonly overwriting one another’s information, processing data out of order, or committing other errors, multicore processors use lock-protected software queues. These are data structures that coordinate the movement of and access to information according to software-defined rules. But all that extra software comes with significant overhead, which only gets worse as the number of cores increases. “Communications between cores is becoming a bottleneck,” says&lt;a shape="rect" href="http://www.ece.ncsu.edu/people/solihin"&gt; Yan Solihin&lt;/a&gt;, a professor of electrical and computer engineering who led the work at NC State, in Raleigh.&lt;/p&gt;
&lt;p&gt;The solution—born of a discussion with Intel researchers and executed by Solihin’s student, Yipeng Wang, at Intel and at NC State—was to turn the software queue into hardware. This effectively turned three multistep ­software-queue operations into three simple instructions: Add data to the queue, take data from the queue, and put data close to where it’s going to be needed next. Compared with just using the software solution, the QMD sped up a sample task such as packet ­processing—like network nodes do on the Internet—by a greater and greater amount the more cores were involved. For 16 cores, QMD worked 20 times as fast as the software could.&lt;/p&gt;
&lt;p&gt;Once they achieved this result, the researchers realized that the QMD might be able to do a few other tricks—such as turning more software into hardware. They added more logic to the QMD and found it could speed up several other core-communications-dependent functions, including MapReduce, a technology Google pioneered for distributing work to different cores and collecting the results.&lt;/p&gt;
&lt;p&gt;
&lt;a shape="rect" href="https://people.csail.mit.edu/devadas/"&gt;Srini Devadas&lt;/a&gt;, an expert in cache control systems at MIT, says the QMD addresses “a very important problem.” Devadas’s own solution for the use of caches by multiple cores—or even multiple processors—is more radical than the QMD. Called &lt;a shape="rect" href="https://people.csail.mit.edu/devadas/pubs/tardis.pdf"&gt;Tardis&lt;/a&gt; [PDF], it’s a complete rewrite of the cache management rules, and so it is a solution aimed at processors and systems of processors further in the future. But QMD, Devadas says, has nearer-term potential. “It’s the kind of work that would motivate Intel—putting in a small piece of hardware for a significant improvement.”&lt;/p&gt;
&lt;p&gt;The Intel researchers involved couldn’t comment on whether QMD would find its way into future processors. However, they are actively researching its potential. (Wang is now a research scientist at Intel.) The researchers hope that QMD, among other extensions of the concept, can simplify communication among the cores and the CPU’s input/output system.&lt;/p&gt;
&lt;p&gt;Solihin, meanwhile, is inventing other types of hardware accelerators. “We have to improve performance by improving energy efficiency. The only way to do that is to move some software to hardware. The challenge is to figure out which software is used frequently enough that we could justify implementing it in hardware,” he says. “There is a sweet spot.”&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 28 Oct 2016 15:00:00 GMT</pubDate>
<dc:creator>Samuel K. Moore</dc:creator>
<guid>http://spectrum.ieee.org/semiconductors/processors/breaking-the-multicore-bottleneck</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMjYyNA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMjYyMg.jpg" height="225" width="300"/>
</item>
<item>
<title>ZCash Will Be a Truly Anonymous Blockchain-Based Currency</title>
<link>http://spectrum.ieee.org/tech-talk/computing/networks/zcash-truly-anonymous-blockchainbased-cryptocurrency</link>
<description>Set to launch Friday, ZCash is built to do what Bitcoin can't—provide privacy</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Set to launch Friday, ZCash is built to do what Bitcoin can't—provide privacy&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyNDM5NQ.jpeg"/&gt;
&lt;figcaption&gt;Photo-Illustration: IEEE Spectrum&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="Zooko Wilcox, CEO of ZCash, holding a smartphone displaying 'ZCash Accepted Here'" src="http://spectrum.ieee.org/image/MjgyNDM4OA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Morgen E. Peck&lt;/figcaption&gt;
&lt;figcaption&gt;Zooko Wilcox, CEO of ZCash, which runs a new, anonymous, blockchain-based digital currency&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Last Friday, I was in a van in Denver, Colorado with &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Zooko_Wilcox-O%27Hearn"&gt;Zooko Wilcox&lt;/a&gt; the CEO of &lt;a shape="rect" href="https://z.cash/"&gt;ZCash&lt;/a&gt;, a company that on 28 October will launch a new blockchain-based digital currency of the same name. On the floor next to me was a bunch of newly purchased computer equipment. I knew we were going to a hotel, but didn’t know where. I only knew that I’d be there for the next two days straight and that it would be my job to watch, ask questions, stave off sleep, and document as much as I possibly could.&lt;/p&gt;
&lt;p&gt;That day began a cryptographic ceremony of sorts, one that will make or break a new digital currency. ZCash is identical to &lt;a shape="rect" href="http://spectrum.ieee.org/tag/Bitcoin"&gt;Bitcoin&lt;/a&gt; in a lot of ways. It’s founded on a digital ledger of transactions called a &lt;a shape="rect" href="http://spectrum.ieee.org/video/computing/networks/video-the-bitcoin-blockchain-explained"&gt;blockchain&lt;/a&gt; that exists on an army of computers that can be anywhere in the world. But it differs from Bitcoin in one critical way: It will be completely anonymous. Although privacy was a motivating factor for Bitcoin’s flock of early adopters, it doesn’t deliver the goods. For those who want to digitally replicate the experience of slipping on a ski mask and handing over an envelope of unmarked bills, ZCash is the new way to go. &lt;/p&gt;
&lt;aside class="inlay pullquote lt med"&gt;The problem with Bitcoin today is that the entire history is public. If users are not extremely careful, network analysis can reveal  the real identities of the people behind the accounts&lt;/aside&gt;
&lt;p&gt;To deliver on this anonymity, however, the ZCash protocol requires an initial dose of randomness, a set of parameters that functions as a reference point for the rest of the software. But, the process comes with an unfortunate byproduct. The software that generates the parameters also creates pieces of a cryptographic key, which if combined could be used to generate new coins out of thin air. The ceremony that I was carted off to will serve as a public demonstration that the cryptographic fragments were created and disposed of in such a way that the complete key never came into existence.&lt;/p&gt;
&lt;p&gt;But why make a currency that faces its first existential threat at the very moment of its creation? Because for the subset of people who like their currency digital and free from government control, anonymity really matters.&lt;/p&gt;
&lt;p&gt;“ZCash is really exciting because it’s the first combination of the blockchain properties with the encryption properties,” says Wilcox. This layer of encryption means that in ZCash, transactions will leave no trace on the blockchain of who spent a coin or in what digital pocket it landed. All that will be visible is the fact that a transaction occurred.&lt;/p&gt;
&lt;p&gt;Bitcoin, the first and most widely used digital currency established the blockchain as a revolutionary technology. Blockchains provide a way for disparate, mistrustful parties to jointly maintain a public ledger of transactions and to do so in a way that renders all entries permanent.&lt;/p&gt;
&lt;p&gt;The problem with Bitcoin as it is implemented today is that the entire history is public. Transactions are attributed to random identifiers that in themselves carry no information about the person controlling the accounts. But if users are not extremely careful, network analysis can reveal both the financial behavior and the real identities of the people behind the accounts. (Several companies, such as &lt;a shape="rect" href="https://www.chainalysis.com/"&gt;Chainalysis&lt;/a&gt;, now provide such a service.)&lt;/p&gt;
&lt;p&gt;ZCash too has a blockchain that records and publicly broadcasts every transaction ever made with it. But it hides all identifying information about who made the transactions and how much was spent.&lt;/p&gt;
&lt;p&gt;“ZCash solves this privacy problem by encrypting each transaction. We use standard, modern, high-tech encryption, which is the same kind of encryption that is used to protect websites and emails and everything on the Internet,” says Wilcox.&lt;/p&gt;
&lt;p&gt; This, however creates a new problem. In Bitcoin, having all the details of transactions available in cleartext enables miners—the people running the software that updates and secures the blockchain—to validate new spending requests by referencing previous transactions in the record. When that data is hidden from view, validation becomes more complex and requires a special kind of computation called a &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Zero-knowledge_proof"&gt;zero-knowledge proof&lt;/a&gt;. That computation enables users to prove that that they own the coins they want to spend without revealing any information about where the coins came from or where they are going. Such proofs are used in many other contexts around the Internet. For instance, zero-knowledge proofs allow you to type in a password on a website and have it verified by the site’s server without actually transmitting the password.&lt;/p&gt;
&lt;p&gt;The broad strokes  for ZCash were designed in 2013 at a Johns Hopkins University applied cryptography lab led by &lt;a shape="rect" href="http://spectrum.ieee.org/computing/networks/whos-who-in-bitcoin-zerocoin-hero-matthew-green"&gt;Matthew Green&lt;/a&gt;. He later joined forces with &lt;a target="_blank" shape="rect" data-saferedirecturl="https://www.google.com/url?hl=en&amp;amp;q=http://eli.net.technion.ac.il/&amp;amp;source=gmail&amp;amp;ust=1477682148970000&amp;amp;usg=AFQjCNGqQbbDjx-puW9Dh8N39gdCc-ymGw" href="http://eli.net.technion.ac.il/"&gt;Eli Ben-Sasson&lt;/a&gt;, a computer scientist at the Israel Institute of Technology, and a &lt;a shape="rect" href="http://zerocash-project.org/media/pdf/zerocash-extended-20140518.pdf"&gt;group of researchers&lt;/a&gt; [pdf] at MIT and Tel Aviv University—all of whom now work for the Zcash company. Together they developed a new zero-knowledge proof, called a zk-SNARK, that is much less computationally intensive and thus crucial for scaling the currency.  &lt;/p&gt;
&lt;p&gt;Now ZCash, is in the hands of Wilcox. Privacy is an issue that is near to his heart. As a teenager, he delayed going to college to work with cryptographer David Chaum on DigiCash, the first implementation of a privacy-centric digital cash. When that project crashed in the 1990s, he continued the crusade.&lt;/p&gt;
&lt;p&gt;Enhancing financial privacy will likely enhance the ability of criminals to go about their business undetected, and that’s a legitimate fear. Bitcoin itself found its first, and arguably thus far only, killer app when sellers and buyers realized that they could use it for illegal purchases in Dark Web markets.&lt;/p&gt;
&lt;p&gt;But Wilcox, who regards privacy as a right, argues that there are important, legitimate reasons why someone would want to use an anonymous currency.&lt;/p&gt;
&lt;p&gt;“There are regulatory and commercial and moral reasons for privacy from all sectors,” he says. To give a commercial example: Apple wouldn’t want Samsung to be able to track its transactions and gain valuable competitive intelligence.&lt;/p&gt;
&lt;p&gt;Or the motivating factor could be regulatory compliance. Multiple laws in the United States and the U.K., such as the data privacy rules of the &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act"&gt;Health Insurance Portability and Accountability Act of 1996&lt;/a&gt; , require companies to keep consumer information hidden from view, a feature ZCash can reliably offer.&lt;/p&gt;
&lt;p&gt;There are also strictly technical considerations that make strong privacy a necessary feature in a digital currency. Ideally, for the system to function, coins should be fungible, which is to say, each coin should be indistinguishable from the next. When a coin carries the history, and potentially the smear, of every past transaction—as bitcoins do—this can be difficult to achieve.&lt;/p&gt;
&lt;p&gt;“The laws of economics are almost as immutable as the laws of physics. And good money means that every unit of that money is the same as any other unit of that money. The only way to have that be the case for digital currencies is to have it be private,” says Roger Ver, a ZCash investor for whom fungibility is a central concern.&lt;/p&gt;
&lt;p&gt;But perhaps the most intriguing feature of ZCash is that users can toggle the level of privacy that it provides. Although the ZCash protocol encrypts all information about transactions by default, people will be able to selectively disclose this data and they will have control over what parts get revealed as well as who gets to see them.&lt;/p&gt;
&lt;p&gt;Let’s say I’m in college and my parents are funding my studies. They could send me ZCash and then I could lift the veil on all the transactions I make with that money in a way that only they could see.&lt;/p&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/networks/the-bitcoin-for-is-a-coup"&gt;Adam Back&lt;/a&gt;, a cryptographer who has himself endeavored to strengthen Bitcoin’s privacy guarantees with a scheme called Confidential Transactions, says that ZCash is able to offer this degree of flexibility because, unlike Bitcoin, it starts with the strongest privacy-guaranteeing tools available.&lt;/p&gt;
&lt;p&gt;“It’s very hard to build something stronger on something that’s weak,” he says. “If you start with a perfect electronic cash system building block, then you can build an electronic cash system with selective weakening in a way that makes sense for society.”&lt;/p&gt;
&lt;p&gt;But cryptographers like Back do have reservations.&lt;/p&gt;
&lt;p&gt;There is, of course, the problem of it requiring that one moment of infallibility on the part of human beings—the destruction of the key fragments—to guarantee its security.&lt;/p&gt;
&lt;p&gt;Also, the zk-SNARK computations that validate transactions are quite exotic, at least in comparison to the well-worn standards used in Bitcoin.  &lt;/p&gt;
&lt;p&gt;“The number of people who understand and have read the math and could develop an attack would be very small, maybe a dozen researchers worldwide. And so, you run the risk that maybe not enough people have looked at it to have the insight of what’s wrong with it,” says Back.&lt;/p&gt;
&lt;p&gt;The ZCash company, which developed the open source software is itself a bit of an experiment. It has a direct stake in the coins that are generated by the ZCash protocol. As in Bitcoin, miners periodically create new coins. But in ZCash, the miners only get to keep ninety percent of those coins. The rest gets dumped into accounts controlled by the ZCash company, which has stated that it will divvy up these earnings between founders, private investors, and a non-profit foundation responsible for working on future versions of the protocol. But it is up to the company to report transparently on where that money flows. &lt;/p&gt;
&lt;p&gt; One of the biggest unknowns is whether enough people care deeply enough about privacy to bring ZCash into the mainstream. When DigiCash declared bankruptcy in 1998, the failure was partially attributed to a lack of interest in financial privacy on the part of the everyday consumer.&lt;/p&gt;
&lt;p&gt;Wilcox is confident that it will be different this time around. “I feel that privacy is an important personal and social value, that it uplifts individuals and communities, that it protects them,” he says. “And it’s been really gratifying that once word got out about the ZCash project there have been people approaching me either over the Internet or in real life, in person, at conferences just to tell me that they feel this too and that they care about this and that they’re glad we’re working on it and they want us to succeed.”&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 25 Oct 2016 17:00:00 GMT</pubDate>
<dc:creator>Morgen E. Peck</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/networks/zcash-truly-anonymous-blockchainbased-cryptocurrency</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyNDQwOQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyNDQwNw.jpg" height="225" width="300"/>
</item>
<item>
<title>Which Path to IoT Security? Government Regulation, Third-Party Verification, or Market Forces</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/internet/experts-discuss-3-paths-to-stronger-iot-device-security-government-regulation-thirdparty-verification-and-market-forces</link>
<description>The security flaws within the Internet of Things must be fixed, or denial-of-service attacks will only worsen</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The security flaws within the Internet of Things must be fixed, or denial-of-service attacks will only worsen&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMzk1OA.jpeg"/&gt;
&lt;figcaption&gt;Illustration: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;On Friday, a series of &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/telecom/security/what-is-a-distributed-denialofservice-attack-and-how-did-it-break-twitter"&gt;distributed denial-of-service attacks&lt;/a&gt; hit &lt;a shape="rect" href="http://dyn.com/"&gt;Dyn&lt;/a&gt;, a company that provides a form of traffic control for popular websites, and interrupted some users’ access to sites including Github, Twitter, and Netflix. Since then, it has become clear that these attacks were made possible by security vulnerabilities in millions of devices within the Internet of Things.&lt;/p&gt;
&lt;p&gt;On Monday at the &lt;a shape="rect" href="https://staysafeonline.org/"&gt;National Cyber Security Alliance’s&lt;/a&gt; Cybersecurity Summit in New York City, industry leaders from security firms, Internet service providers, and device manufacturers fretted over the implications. Panelists spoke about the existential dangers that companies in the fast-growing IoT sector face if they continue to fail to secure these devices and debated ways in which the industry can improve security within this ecosystem.&lt;/p&gt;
&lt;p&gt;“Friday showed us that the genie is well out of the bottle at this point,” said &lt;a shape="rect" href="https://www.linkedin.com/in/andilee"&gt;Andrew Lee&lt;/a&gt;, CEO at security company &lt;a shape="rect" href="https://www.eset.com/us/"&gt;ESET North America&lt;/a&gt;. “This should probably be the wake-up call to manufacturers to start taking this seriously.”&lt;/p&gt;
&lt;p&gt;While it’s still not clear who executed Friday’s attacks, &lt;a shape="rect" href="http://dyn.com/blog/dyn-statement-on-10212016-ddos-attack/"&gt;Dyn has announced&lt;/a&gt; that hackers orchestrated it across “tens of millions” of IP addresses gathered through &lt;a shape="rect" href="https://krebsonsecurity.com/2016/10/source-code-for-iot-botnet-mirai-released/"&gt;Mirai&lt;/a&gt;, malware that scans the Internet for connected devices with weak security. The malware then enlists these devices into a massive global network called a botnet. Increasingly, hackers have used these networks to launch distributed denial-of-service attacks, in which they instruct many devices to send traffic to a target at once in order to overload its capacity and prevent real users from accessing a website or service.&lt;/p&gt;
&lt;aside class="inlay pullquote lt med"&gt;Experts agreed that the responsibility for securing IoT devices rests with the companies that manufacture them rather than the consumers who bring them home&lt;/aside&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;Distributed denial-of-service attacks have been around &lt;a shape="rect" href="https://www.cert.org/historical/advisories/CA-1996-26.cfm"&gt;for decades&lt;/a&gt;, but Mirai has made it much easier for hackers to quickly assemble a large botnet by co-opting IoT devices, since many have &lt;a shape="rect" href="http://spectrum.ieee.org/riskfactor/telecom/security/android-in-my-rice-cooker-gateway-to-future-cyber-home-invasion"&gt;weaker security&lt;/a&gt; than laptops or smartphones. A hacker can often access a device by simply logging in through a default password that a manufacturer assigned to millions of devices—a lazy strategy that would be akin to Honda handing out identical keys for all of its 2017 Civics.   &lt;/p&gt;
&lt;p&gt;In September, the hacker who created the Mirai malware released its source code to the public, essentially setting it loose for other hackers to use. Security experts knew it wouldn’t be long before cybercriminals wielded its powers to build vast IoT botnets for attacks that were bigger and more powerful than ever before. Less than a month later, Kyle York, Dyn’s chief strategy officer, wrote&lt;a shape="rect" href="http://dyn.com/blog/dyn-statement-on-10212016-ddos-attack/"&gt; in a post&lt;/a&gt; on the company’s site that Friday’s assault is likely to be seen as “an historic attack.”&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The fear of becoming vulnerable to hackers through poorly secured IoT devices isn’t just a problem for consumers—it also impacts industrial projects. &lt;a shape="rect" href="http://blogs.cisco.com/author/anthonygrieco"&gt;Anthony Grieco&lt;/a&gt;, a senior director for security at &lt;a shape="rect" href="http://www.cisco.com/c/en/us/index.html"&gt;Cisco&lt;/a&gt; said 39 percent of Cisco’s corporate customers have halted a major digitization project because of concerns about IoT security.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Right now, it’s clear that many manufactures are still failing to incorporate adequate protections into their products, and there’s no gold standard of security for IoT devices that the industry has agreed upon. Selling products with vulnerabilities can certainly hurt business—Chinese manufacturer &lt;a shape="rect" href="http://www.xiongmaitech.com/en/"&gt;Hangzhou Xiongmai Technology&lt;/a&gt; has &lt;a shape="rect" href="http://www.bbc.com/news/technology-37750798"&gt;recalled webcams&lt;/a&gt; made with its components after learning that they were among the devices used in the Dyn attacks. And a consumer survey of 1,527 U.S. adults released Monday by ESET and the National Cyber Security Alliance found that 50 percent of consumers have declined to purchase an IoT device because of security concerns.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;But &lt;strong&gt;market forces&lt;/strong&gt; alone may not be enough to persuade manufacturers to spend the extra time or money to ensure that their products are safe. An alternative solution would be for legislators to pass laws or government agencies to implement policies that require IoT manufacturers to build certain protections into their devices and to provide regular updates and patches as new bugs are discovered.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;However, Lee at ESET North America said &lt;strong&gt;government regulation&lt;/strong&gt; is difficult to apply to cybersecurity. By the time regulations are in place, new threats and solutions have inevitably popped up.&lt;/p&gt;
&lt;p&gt;The National Institute of Standards and Technology has developed a voluntary &lt;a shape="rect" href="https://www.nist.gov/sites/default/files/documents/cyberframework/cybersecurity-framework-021214.pdf"&gt;cybersecurity framework&lt;/a&gt; that companies and organizations can use as a guide to identify and protect against cyberrisks. However, it was intended to protect critical infrastructure such as the electricity grid and water treatment plants and does not have specific recommendations for IoT devices.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;At Monday’s summit, &lt;a shape="rect" href="https://www.linkedin.com/in/saminassar"&gt;Sami Nassar&lt;/a&gt;, vice president for cyber security at &lt;a shape="rect" href="http://www.nxp.com/"&gt;NXP Semiconductors&lt;/a&gt;, suggested a &lt;strong&gt;third-party verification&lt;/strong&gt; program in which an independent group would assign a stamp or endorsement to the packaging of IoT devices if it met certain minimum security standards. This approach would be analogous to the &lt;a shape="rect" href="http://fairtradeusa.org/certification"&gt;Fair Trade USA certification&lt;/a&gt; or the way the U.S. Department of Agriculture regulates the use of the term “organic” for food products. “You need to have minimum rules with a minimum level of security that a product must have to enter the ecosystem,” he said.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Other industry-driven solutions have also begun to surface—for example, &lt;a shape="rect" href="http://www.apple.com/ios/home/"&gt;Apple’s HomeKit&lt;/a&gt; includes products such as fans, locks, humidifiers, and thermostats from many manufacturers but supports only models that have met Apple’s security specifications. Lee of ESET North America also suggested that insurance companies could play a role in improving the security of connected cars by refusing to insure those which remain vulnerable to hackers.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;However, &lt;a shape="rect" href="https://twitter.com/panopticonmatt"&gt;Matthew Cook&lt;/a&gt;, cofounder of video game security company &lt;a shape="rect" href="https://www.panopticonlabs.com/"&gt;Panopticon Labs&lt;/a&gt;, pointed out that all of these solutions assume it’s possible for cybersecurity experts to agree on a set of security standards for IoT products. In reality, a product’s threat model might change significantly from the time it is designed to the moment it hits store shelves.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Regardless of the approach, the experts agreed that the responsibility for securing IoT devices rests with the companies that manufacture them rather than the consumers who bring them home. Any action that the average consumer must complete (such as changing a default password) also represents a potential vulnerability if that action remains undone. &lt;a shape="rect" href="https://www.linkedin.com/in/daswani"&gt;Neil Daswani&lt;/a&gt;, chief information security officer at identity protection firm &lt;a shape="rect" href="https://www.lifelock.com/test/product-home/"&gt;LifeLock&lt;/a&gt;, offered a simple litmus test for any cybersecurity features in a consumer product: They should be as easy to operate as a TV remote control, if manufacturers want consumers to actually use them.   &lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 25 Oct 2016 13:00:00 GMT</pubDate>
<dc:creator>Amy Nordrum</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/internet/experts-discuss-3-paths-to-stronger-iot-device-security-government-regulation-thirdparty-verification-and-market-forces</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMzk3Mw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMzk3MQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Social Media’s Dial-Up Ancestor: The Bulletin Board System</title>
<link>http://spectrum.ieee.org/computing/networks/social-medias-dialup-ancestor-the-bulletin-board-system</link>
<description>The history of the BBS shows that pre-Internet social media was pretty great</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The history of the BBS shows that pre-Internet social media was pretty great&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMzAwNg.jpeg"/&gt;
&lt;figcaption&gt;Image: Original BBSs/Jason Scott&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;style type="text/css"&gt;.carousel-inner{
    height:525px !important;
  }
&lt;/style&gt;
&lt;div id="103318989" class="carousel slide"&gt;
&lt;div class="carousel-inner"&gt;
&lt;div class="item active"&gt;
&lt;img id="103318989_0" alt="Chicago 786" src="http://spectrum.ieee.org/image/MjgyMzE0Mw.jpeg" data-original="/image/MjgyMzE0Mw.jpeg"/&gt;
&lt;span class="item-num"&gt;1/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Chicago 786 &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_1" alt="Spock's Adventure" src="http://spectrum.ieee.org/image/MjgyMzE0Ng.jpeg" data-original="/image/MjgyMzE0Ng.jpeg"/&gt;
&lt;span class="item-num"&gt;2/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Spock's Adventure &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_2" alt="The Buccanee'rs Den" src="http://spectrum.ieee.org/image/MjgyMzE0OQ.jpeg" data-original="/image/MjgyMzE0OQ.jpeg"/&gt;
&lt;span class="item-num"&gt;3/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;The Buccanee'rs Den &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_3" alt="The Dungeon" src="http://spectrum.ieee.org/image/MjgyMzE1Mg.jpeg" data-original="/image/MjgyMzE1Mg.jpeg"/&gt;
&lt;span class="item-num"&gt;4/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;The Dungeon &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_4" alt="The First Response" src="http://spectrum.ieee.org/image/MjgyMzE1NQ.jpeg" data-original="/image/MjgyMzE1NQ.jpeg"/&gt;
&lt;span class="item-num"&gt;5/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;The First Response &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_5" alt="The Hobbit's Armpit" src="http://spectrum.ieee.org/image/MjgyMzE1OA.jpeg" data-original="/image/MjgyMzE1OA.jpeg"/&gt;
&lt;span class="item-num"&gt;6/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;The Hobbit's Armpit &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_6" alt="The Joker's" src="http://spectrum.ieee.org/image/MjgyMzE2MQ.jpeg" data-original="/image/MjgyMzE2MQ.jpeg"/&gt;
&lt;span class="item-num"&gt;7/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;The Joker's &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_7" alt="Treasure Chest" src="http://spectrum.ieee.org/image/MjgyMzE2NA.jpeg" data-original="/image/MjgyMzE2NA.jpeg"/&gt;
&lt;span class="item-num"&gt;8/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Treasure Chest &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_8" alt="Bates Motel" src="http://spectrum.ieee.org/image/MjgyMzE3MA.jpeg" data-original="/image/MjgyMzE3MA.jpeg"/&gt;
&lt;span class="item-num"&gt;9/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Bates Motel &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="103318989_9" alt="Welcome to the Lil' Italy" src="http://spectrum.ieee.org/image/MjgyMzE3Mw.jpeg" data-original="/image/MjgyMzE3Mw.jpeg"/&gt;
&lt;span class="item-num"&gt;10/10&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Welcome to the Lil' Italy &lt;em/&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;a class="left carousel-control" shape="rect" href="#103318989" data-slide="prev"&gt;
&lt;span class="glyphicon glyphicon-chevron-left"/&gt;
&lt;/a&gt;
&lt;a class="right carousel-control" shape="rect" href="#103318989" data-slide="next"&gt;
&lt;span class="glyphicon glyphicon-chevron-right"/&gt;
&lt;/a&gt;
&lt;ol class="carousel-indicators"&gt;
&lt;li data-target="#103318989" data-slide-to="0" class="active"/&gt;
&lt;li data-target="#103318989" data-slide-to="1" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="2" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="3" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="4" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="5" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="6" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="7" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="8" class=""/&gt;
&lt;li data-target="#103318989" data-slide-to="9" class=""/&gt;
&lt;/ol&gt;
&lt;script&gt;
                $(document).ready(function(){
                    $('#103318989').carousel({
                        pause: true,
                        interval: false
                    });
                });
&lt;/script&gt;
&lt;/div&gt;
&lt;figure class="xlrg" role="slidshow"&gt;
&lt;figcaption class="hi-cap"&gt;Images: Original BBSs/Jason Scott&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Pioneers of Cyberspace: &lt;/strong&gt;Welcome screens from various computer bulletin-board systems show their operator’s wild creativity.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;For millions of people around the globe,&lt;/strong&gt; the Internet is a simple fact of life. We take for granted the invisible network that enables us to communicate, navigate, investigate, flirt, shop, and play. Early on, this network-of-networks connected only select companies and university campuses. Nowadays, it follows almost all of us into the most intimate areas of our lives. And yet, very few people know how the Internet became social.&lt;/p&gt;
&lt;p&gt;Perhaps that’s because most histories of the Internet focus on technical innovations: packet switching, dynamic routing, addressing, and hypertext, for example. But when anyone other than a network engineer talks about the Internet, he or she is rarely thinking about such things. For most folks, the Internet is principally a medium through which we chat with friends, share pictures, read the news, and do our shopping. Indeed, for those who’ve been online only for the last decade or so, the Internet is just social media’s plumbing—a vital infrastructure that we don’t think much about, except perhaps when it breaks down.&lt;/p&gt;
&lt;p&gt;To understand how the Internet became a medium for social life, you have to widen your view beyond networking technology and peer into the makeshift laboratories of microcomputer hobbyists of the 1970s and 1980s. That’s where many of the technical structures and cultural practices that we now recognize as social media were first developed by amateurs tinkering in their free time to build systems for computer-mediated collaboration and communication.&lt;/p&gt;
&lt;p&gt;For years before the Internet became accessible to the general public, these pioneering computer enthusiasts chatted and exchanged files with one another using homespun “bulletin-board systems” or BBSs, which later linked a more diverse group of people and covered a wider range of interests and communities. These BBS operators blazed trails that would later be paved over in the construction of today’s information superhighway. So it takes some digging to reveal what came before.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;How did it all start? &lt;/strong&gt;During the snowy winter of 1978, Ward Christensen and Randy Suess, members of the Chicago Area Computer Hobbyist’s Exchange (CACHE), began to assemble what would become the best known of the first small-scale BBSs. Members of CACHE were passionate about microcomputers, at the time an arcane endeavor, and so the club’s newsletters were an invaluable source of information. Christensen and Suess’s novel idea was to put together an online archive of club newsletters using a custom-built microcomputer and a hot new Hayes modem they had acquired.&lt;/p&gt;
&lt;p&gt;This modem included an auto-answer feature, to which Christensen and Suess added a custom hardware interface between the modem and the hard-reset switch. Every time the telephone rang, the modem would detect the incoming call and then “cold boot” their system directly into a special host program written in Intel 8080 assembly language. Restarting the system with every call offered a blunt but effective means of recovering from hardware and software crashes—a common occurrence on home-brew hardware of the time.&lt;/p&gt;
&lt;figure class="lt sm" role="img"&gt;
&lt;img alt="img" src=" /image/MjgyMjU2Nw.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: QuickHoney&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Founding Father: &lt;/strong&gt;Ward Christensen helped pioneer the computerized bulletin board.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Once a connection was established, the host program welcomed users to the system, provided a list of articles to read, and invited them to leave messages. Christensen and Suess dubbed the system “Ward and Randy’s Computerized Bulletin Board System,” or &lt;a shape="rect" href="https://en.wikipedia.org/wiki/CBBS"&gt;CBBS&lt;/a&gt;. It was, as the name suggested, an electronic version of the community bulletin boards that you still see in libraries, supermarkets, cafés, and churches.&lt;/p&gt;
&lt;p&gt;Anyone with access to a teletype or video terminal could dial into CBBS. And after a few months, a small but lively community began to form around the system. In the hobbyist tradition of sharing information, Christensen and Suess wrote up a report about their project titled “Hobbyist Computerized Bulletin Board,” which appeared in the November 1978 issue of the influential computer magazine &lt;a shape="rect" href="https://archive.org/details/byte-magazine"&gt;
&lt;em&gt;Byte&lt;/em&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The article provided details about the hardware they used and how they organized and implemented their software. The authors even included their phone numbers and invited readers to take CBBS for a spin. Acknowledging the experimental nature of the system, they encouraged readers to “feel free to hang up and try several times if you have problems.” After the issue hit newsstands, calls to their computer started pouring in.&lt;/p&gt;
&lt;p&gt;Over the next few years, hundreds of small-scale systems like CBBS popped up around the country. Perhaps inspired by the &lt;em&gt;Byte&lt;/em&gt; article, many of these new systems were organized by local computer clubs. In 1983, TAB Books, publisher of numerous DIY electronics guides, published &lt;a shape="rect" href="https://www.amazon.com/Create-Your-Computer-Bulletin-Board/dp/0830606335"&gt;
&lt;em&gt;How to Create Your Own Computer Bulletin Board&lt;/em&gt;
&lt;/a&gt;, by Lary L. Myers. In addition to explaining the concept and motivation behind online services, Myers’s book included complete source code in the BASIC programming language for host software. The back of the book also listed the telephone numbers of more than 275 public bulletin-board systems in 43 U.S. states. Some charged a nominal membership fee, while most were free to use. The roots of social media were beginning to take hold.&lt;/p&gt;
&lt;p&gt;In retrospect, 1983 proved to be a critical year for popular computing. In France, the state-sponsored &lt;a shape="rect" href="http://www.bbc.com/news/magazine-18610692"&gt;Minitel&lt;/a&gt; system completed its first full year of operation in Paris, making online news, shopping, and chat accessible to every citizen in that city. In the United States, novel commercial systems gained traction, with &lt;a shape="rect" href="https://en.wikipedia.org/wiki/CompuServe"&gt;CompuServe&lt;/a&gt; reporting more than 50,000 paying subscribers.&lt;/p&gt;
&lt;p&gt;Even Hollywood took interest in cyberspace. The 1983 movie &lt;em&gt;
&lt;a shape="rect" href="http://www.imdb.com/title/tt0086567/"&gt;WarGames&lt;/a&gt;
&lt;/em&gt;, featuring a teenage hacker who explored remote computer networks from his bedroom, became an unlikely box-office smash. Although the &lt;a shape="rect" href="http://www.imsai.net/"&gt;IMSAI microcomputer&lt;/a&gt; and acoustic-coupler modem depicted in the movie once cost as much as a cheap used car, curious computer users inspired by the film could buy serviceable alternatives at the nearest &lt;a shape="rect" href="https://www.radioshack.com/"&gt;Radio Shack&lt;/a&gt; for roughly the cost of a good-quality hi-fi stereo. And as the decade progressed, the online universe expanded rapidly from its original core of microcomputer hobbyists to encompass a much wider group.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Early dial-up BBSs&lt;/strong&gt; were mostly local affairs. And no wonder: In the early 1980s, most Americans paid a flat monthly fee for unlimited local phone calls, but calls to another city or state were billed according to duration, distance, and time of day. Even calls within the same state could be quite costly. For tenderfoot BBS users, running up a monstrous phone bill was considered a rite of passage. To avoid long-distance calling altogether, more seasoned users restricted their online activity to nearby systems that could be reached toll free.&lt;/p&gt;
&lt;p&gt;The local nature of BBSing meant that users could reasonably assume that the people they met online lived nearby. Even though many BBSs encouraged the use of pseudonyms, or “handles,” it was entirely possible that the person you were chatting with online tonight could be bagging your groceries or coaching your daughter’s soccer team tomorrow.&lt;/p&gt;
&lt;p&gt;Many BBS administrators, or “sysops,” reinforced this sense of community by hosting regular in-person get-togethers, often at local parks or favorite watering holes. Online disagreements—flame wars—could be kept in check as well, because the cost of being a jerk escalated with the likelihood of later seeing your interlocutor face to face.&lt;/p&gt;
&lt;p&gt;Another key distinction between the dial-up BBSs of the early 1980s and the social media services we use today is that, for the most part, each BBS was a world unto itself, whirring away in blithe isolation from all others. Over time, each system developed its own idiosyncratic personality. One might focus on trading shareware games, another on arguing politics, and a third on talking about TV shows.&lt;/p&gt;
&lt;figure class="rt sm" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgyMjU2NQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: QuickHoney&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Founding Father:&lt;/strong&gt; Randy Suess helped Christensen create their highly influential BBS.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Experienced BBS users would visit the various systems in their areas, maintaining a separate profile on each one. In big metro areas like Atlanta, Minneapolis, or Houston, there might be a dozen or more local BBSs to tap. But folks in less populated areas were seldom so lucky. Rural users with particularly narrow interests—say, collecting antique clocks—may have had only one or two other people nearby to chat with. They longed for more.&lt;/p&gt;
&lt;p&gt;To overcome the economic cost of long-distance calling and the social cost of isolation, BBS operators needed to find a way to interconnect their systems, to create a network of BBS networks—a people’s Internet. Christensen and Suess had floated the idea of a network of BBSs in the conclusion of their 1978 &lt;em&gt;Byte&lt;/em&gt; article, but it was fellow amateur &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Tom_Jennings"&gt;Tom Jennings&lt;/a&gt; who designed a real inter-network of BBSs.&lt;/p&gt;
&lt;p&gt;In 1984, Jennings was distributing free of charge from his home in the San Francisco Bay Area a BBS host program for Microsoft’s operating system, MS-DOS. He called the program Fido. The popularity of the Fido BBS grew alongside that of MS-DOS, and soon there were a dozen or more BBS systems running the program.&lt;/p&gt;
&lt;p&gt;On a lark, Jennings added an experimental feature to his software that enabled two Fido BBSs to call each other automatically and exchange data. After a coast-to-coast test with John Madill, a Fido BBS operator in Baltimore, Jennings organized the first network of BBSs and called it &lt;a shape="rect" href="https://www.fidonet.org/"&gt;FidoNet.&lt;/a&gt; Within a decade, FidoNet grew into a massive 20,000-node network reaching users as far away as South Africa and New Zealand. Unlike with today’s Internet, machines on this network would typically store data for many hours before forwarding it on to its destination. But FidoNet nevertheless functioned as a valuable global data network.&lt;/p&gt;
&lt;p&gt;The early versions of FidoNet included a number of clever design decisions that facilitated the rapid growth of the network. From the start, for example, Jennings encouraged collaboration by publishing technical documentation on FidoNet’s protocols and file formats. As a result, support for FidoNet was added to other BBS-software packages and soon became a de facto standard for building such networks.&lt;/p&gt;
&lt;p&gt;Because FidoNet was run primarily by hobbyists, it was designed to keep costs down. Exchanges between nodes normally took place in the middle of the night, when long-distance phone rates were lowest. This interval, during which many BBSs refused to answer calls from users, became known to insiders as “national mail hour.”&lt;/p&gt;
&lt;p&gt;Initially, FidoNet provided only inter-BBS email services. But FidoNet allowed BBS users to imagine themselves contributing to a vast conversation with people from all over the world. Newcomers later added other features and capabilities.&lt;/p&gt;
&lt;p&gt;In 1986, Jeff Rush, a BBS operator in Dallas, created a conferencing mechanism for FidoNet. Rush’s system, dubbed &lt;a shape="rect" href="http://www.bbsdocumentary.com/photos/140wynn/FILES/echomail.txt"&gt;Echomail,&lt;/a&gt; worked similarly to the forums on CompuServe or the newsgroups on &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Usenet"&gt;Usenet&lt;/a&gt;: You could post a comment, and anyone on the network could reply to it. But participation in those earlier forums required either a costly monthly subscription or access to a public Unix system, whereas anyone with a PC and a modem could access FidoNet’s Echomail. As a result, Echomail became hugely popular and was particularly useful for people living in rural areas or those with niche interests. For the first time, the whole population of BBS users could engage in discussion together.&lt;/p&gt;
&lt;p&gt;Beyond the United States, BBSs seemed to spring up anywhere that a microcomputer could be connected to a telephone line. In 1987, &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Pablo_Kleinman"&gt;Pablo Kleinman&lt;/a&gt;, a sysop in Buenos Aires, helped to connect the first four FidoNet nodes in Argentina, and a year later, Juan Dávila, a sysop in Puerto Rico, announced the creation of Latino Net, a Spanish-language conference. Following the dissolution of the Soviet Union in 1991, a FidoNet user based in the United States circulated technical information about reading and writing Cyrillic messages and encouraged fellow FidoNet enthusiasts to reach out to the “overwhelming” number of Russian users joining the network.&lt;/p&gt;
&lt;p/&gt;
&lt;div id="661791719" class="carousel slide"&gt;
&lt;div class="carousel-inner"&gt;
&lt;div class="item active"&gt;
&lt;img id="661791719_0" alt="Stacy Horn ran the Echo BBS in New York City." src="http://spectrum.ieee.org/image/MjgyMzAyMA.jpeg" data-original="/image/MjgyMzAyMA.jpeg"/&gt;
&lt;span class="item-num"&gt;1/5&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Stacy Horn ran the Echo BBS in New York City. &lt;em&gt;Photo: James Estrin/The New York Times/Redux&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="661791719_1" alt="Sister Mary Elizabeth Clark ran the AIDS Education General Information System." src="http://spectrum.ieee.org/image/MjgyMzAyMw.jpeg" data-original="/image/MjgyMzAyMw.jpeg"/&gt;
&lt;span class="item-num"&gt;2/5&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Sister Mary Elizabeth Clark ran the AIDS Education General Information System. &lt;em&gt;Photo: Bob Grieser/Los Angeles Times/Getty Images&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="661791719_2" alt="CompuServe served up news." src="http://spectrum.ieee.org/image/MjgyMzAyNg.jpeg" data-original="/image/MjgyMzAyNg.jpeg"/&gt;
&lt;span class="item-num"&gt;3/5&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;CompuServe served up news. &lt;em&gt;Photo: AP Photo&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="661791719_3" alt="Matthew Broderick and Ally Sheedy starred in &amp;lt;i&amp;gt;WarGames&amp;lt;/i&amp;gt;." src="http://spectrum.ieee.org/image/MjgyMzAyOQ.jpeg" data-original="/image/MjgyMzAyOQ.jpeg"/&gt;
&lt;span class="item-num"&gt;4/5&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Matthew Broderick and Ally Sheedy starred in &lt;i&gt;WarGames&lt;/i&gt;. &lt;em&gt;Photo: United Archive/Alamy&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="661791719_4" alt="Minitel provided online access in France." src="http://spectrum.ieee.org/image/MjgyMzAzMg.jpeg" data-original="/image/MjgyMzAzMg.jpeg"/&gt;
&lt;span class="item-num"&gt;5/5&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Minitel provided online access in France. &lt;em&gt;Photo: Owen Franken/Corbis/Getty Images&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;a class="left carousel-control" shape="rect" href="#661791719" data-slide="prev"&gt;
&lt;span class="glyphicon glyphicon-chevron-left"/&gt;
&lt;/a&gt;
&lt;a class="right carousel-control" shape="rect" href="#661791719" data-slide="next"&gt;
&lt;span class="glyphicon glyphicon-chevron-right"/&gt;
&lt;/a&gt;
&lt;ol class="carousel-indicators"&gt;
&lt;li data-target="#661791719" data-slide-to="0" class="active"/&gt;
&lt;li data-target="#661791719" data-slide-to="1" class=""/&gt;
&lt;li data-target="#661791719" data-slide-to="2" class=""/&gt;
&lt;li data-target="#661791719" data-slide-to="3" class=""/&gt;
&lt;li data-target="#661791719" data-slide-to="4" class=""/&gt;
&lt;/ol&gt;
&lt;script&gt;
                $(document).ready(function(){
                    $('#661791719').carousel({
                        pause: true,
                        interval: false
                    });
                });
&lt;/script&gt;
&lt;/div&gt;
&lt;p/&gt;
&lt;p&gt;By 1993, Randy Bush, a long-time advocate of FidoNet in the developing world, estimated that 59 percent of the nodes were located in North America, 30 percent in Europe, 4 percent in Australia and New Zealand, with the remaining 7 percent split among Asia, Latin America, and Africa. The African systems may have been small relative to the whole network, but these small networks could have an outsize impact for local users. In some regions of Africa, noted Bush, a FidoNet gateway provided an important means for poorly funded academics and NGO staffers to keep up to date with the latest research and news from abroad.&lt;/p&gt;
&lt;p&gt;The growth of FidoNet during the 1980s was part of a larger movement toward greater connectivity among computer networks. Those interconnections were made using special gateways that translated messages between otherwise incompatible networks. In 1986, the administrators of several institutional Unix systems opened gateways to nearby BBSs, thereby enabling the exchange of messages among users of FidoNet, Usenet, and the nascent Internet. Indeed, several Usenet newsgroups began as automatically created copies of material posted in popular FidoNet Echomail conferences.&lt;/p&gt;
&lt;p&gt;It’s tempting to imagine that these gateways introduced BBS users to the Internet. But the Internet at that point held no particular appeal for most BBS users, who were already enmeshed in their own lively online worlds. The early Internet was limited to people with access to a large university or research center, whereas BBSs were open to anyone. As a result, they could be wacky and weird—independent free agents in cyberspace. Indeed, it may be more accurate to say that gateways exposed the relatively staid Internet to the wild ways of BBSers.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;As the 1980s&lt;/strong&gt; passed into the 1990s, falling prices for computers as well as the availability of used PCs allowed many more people to start using the technology. Modems also shifted from an expensive option to a standard component on new PCs. As these barriers to participation fell, a wave of new recruits came online, and the culture of BBSing began to reflect their more diverse interests. Teens, educators, artists, and entrepreneurs joined the growing population of BBS hosts and users.&lt;/p&gt;
&lt;p&gt;Hundreds of BBSs appeared in the late 1980s and early 1990s to serve particular communities and interests. &lt;a shape="rect" href="http://173.26.241.52/public/batboard.htm"&gt;The Batboard&lt;/a&gt; in Columbia, Mo., was dedicated to all things Batman, for example, while the &lt;a shape="rect" href="http://bbsmates.com/viewbbs.aspx?id=110360"&gt;Complete Baseball BBS&lt;/a&gt; in Cambridge, Mass., featured a different sort of bat. Followers of the Grateful Dead (“deadheads”) arranged tape trades and carpooling on &lt;a shape="rect" href="http://www.well.com/"&gt;the WELL&lt;/a&gt; in Sausalito, Calif., and hard-core gamers exchanged homemade &lt;em&gt;Doom&lt;/em&gt; levels &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Software_Creations_(US)"&gt;on Software Creations&lt;/a&gt; in Clinton, Mass. &lt;a shape="rect" href="http://www.bbsmates.com/viewbbs.aspx?id=63845"&gt;The Back Door&lt;/a&gt; in San Francisco supported a lively LGBT community, while the Backdraft in Key Largo, Fla., provided a meeting place for firefighters. The Christian-themed Winplus in Kent, Wash., and the pagan-oriented Brewer’s Witch in Houston were both well known for their friendly off-line get-togethers, while DharmaNet linked up hundreds of Buddhist BBSs.&lt;/p&gt;
&lt;p&gt;Free from the staggering size and profit imperative of commercial services, such small-scale community-oriented BBSs were sites of experimentation. Given the preponderance of men in the early BBS culture, many sysops endeavored to create welcoming environments for the relatively few women who called in. &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Stacy_Horn"&gt;Stacy Horn&lt;/a&gt;, proprietor of the long-running &lt;a shape="rect" href="http://www.echonyc.com/"&gt;Echo&lt;/a&gt; BBS in New York City, was especially tenacious in her appeals to women, offering new female users 12 months of free service and women-only online conferences. “I thought it would be a cooler place if there were women there,” she remarked in an interview from 1996. “Who cares to talk to 20-year-old white guys all the time?”&lt;/p&gt;
&lt;p&gt;Unlike with social-media services today, it was common for small-scale BBSs to vet new users before granting them full access. The TARDIS BBS in Indiana, for example, verified all of its users with a voice telephone call. Users claiming to be women were invited into the “Ladies Only” sections of the board only after being cleared by one of its female moderators. Even the board’s male founder, Tom O’Nan, was shut out. “To this day,” he recently joked, “I don’t know what went on in that room!”&lt;/p&gt;
&lt;figure class="lt sm" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgyMjU2Ng.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: QuickHoney&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Founding Father: &lt;/strong&gt;Tom Jennings created FidoNet, allowing distant BBSs to link up.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For communities in crisis, a BBS could be an important hub for sharing information. At the outset of the AIDS epidemic, as thousands were dying amid media coverage that was mostly suspicious or hostile, BBSs provided an invaluable source of health information and social support. Between 1985 and 1993, more than 100 computer bulletin boards were set up to share information regarding HIV and AIDS.&lt;/p&gt;
&lt;p&gt;One cornerstone in this network was the &lt;a shape="rect" href="http://www.aidsmap.com/org/6651/page/1411896/"&gt;AIDS Education General Information System&lt;/a&gt; (AEGIS) operated by Sister Mary Elizabeth Clark out of her home in San Juan Capistrano, Calif. Each day, Clark searched medical databases for the latest research about treatment and prevention and then used FidoNet to circulate this information to BBSs in more than 40 countries throughout North America, Europe, and Africa. Meanwhile, the United Methodist Church’s Computerized AIDS Ministries Network (CAM) in New York City helped combat the isolation faced by many people affected by the crisis, by answering visitors’ questions and generally providing support.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;strong&gt;For two decades, &lt;/strong&gt;bulletin-board systems provided countless people with an accessible platform for online community. Jason Scott, director of &lt;a shape="rect" href="http://www.bbsdocumentary.com/"&gt;
&lt;em&gt;BBS: The Documentary&lt;/em&gt;
&lt;/a&gt;, estimates that there were at least 106,418 BBSs in operation at one time or another between 1978 and 2004. And yet, in spite of their wide-ranging scale, geographic reach, and cultural influence, BBSs seem to be all but forgotten today. Why?&lt;/p&gt;
&lt;p&gt;Part of the reason is their swift demise. With the commercialization of the Internet in the mid-1990s, thousands of BBSs seemed to vanish almost overnight. Some attempted to move, wholesale, to Usenet or the World Wide Web, while others simply pulled the plug and turned off the lights.&lt;/p&gt;
&lt;p&gt;But a curious thing happened along the way to oblivion: Thousands of BBSs quietly metamorphosed into Internet Service Providers, or ISPs. In December 1995, Jack Rickard, editor of &lt;em&gt;
&lt;a shape="rect" href="https://en.wikipedia.org/wiki/Boardwatch"&gt;Boardwatch&lt;/a&gt;
&lt;/em&gt; magazine, noted that the available statistics on Internet usage didn’t add up: The major ISPs, like UUNET, Netcom, PSINet, and InternetMCI, were not reporting enough subscribers to account for the total number of active users online.&lt;/p&gt;
&lt;p&gt;Using a database compiled by his magazine, Rickard estimated that more than 95 percent of the 3,240 ISPs created in the previous two years were former BBSs operating under new names. The same telephone lines and modems that once connected local callers to one another were now being used to provide direct connections to the global Internet. The dial-up BBSs were bulldozed, it seems, to build Internet on-ramps.&lt;/p&gt;
&lt;p&gt;For Internet users today, there is more to the BBS story than mere nostalgia. As our online lives are channeled through an ever-smaller number of service providers, reflecting on the long-forgotten dial-up era can open our eyes to the value of diversity and local control.&lt;/p&gt;
&lt;p&gt;After all, people often get frustrated that today’s social-media platforms can’t deal with small-scale disputes or apply different policies to different constituencies. BBS users of the past could complain directly to the sysop—maybe even with a phone call—and he or she could act immediately and unilaterally on their behalf. Try that with &lt;a shape="rect" href="https://twitter.com/?lang=en"&gt;Twitter&lt;/a&gt; or &lt;a shape="rect" href="https://www.facebook.com/"&gt;Facebook&lt;/a&gt;. You’ll gain a much better appreciation for the systems often running on little more than solder, duct tape, and a dream that first enabled regular people to connect with one another through their computers.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the November 2016 print issue as “&lt;span&gt;Social Media’s Dial-up Roots.&lt;/span&gt;”&lt;/em&gt;
&lt;/p&gt;
&lt;h2&gt;About the Author&lt;/h2&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://kevindriscoll.info/"&gt;Kevin Driscoll&lt;/a&gt; is an assistant professor in the department of media studies at the University of Virginia, in Charlottesville.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 24 Oct 2016 15:00:00 GMT</pubDate>
<dc:creator>Kevin Driscoll</dc:creator>
<guid>http://spectrum.ieee.org/computing/networks/social-medias-dialup-ancestor-the-bulletin-board-system</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMzAxOQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMzAxNw.jpg" height="225" width="300"/>
</item>
<item>
<title>What Is a Distributed Denial-of-Service Attack and How Did It Break Twitter?</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/security/what-is-a-distributed-denialofservice-attack-and-how-did-it-break-twitter</link>
<description>On Friday, multiple distributed denial-of-service attacks hit a common Domain Name System provider for popular sites including Twitter and Netflix</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;On Friday, multiple distributed denial-of-service attacks hit a common Domain Name System provider for popular sites including Twitter and Netflix&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMzIxMw.jpeg"/&gt;
&lt;figcaption&gt;Photo: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;On Friday, multiple distributed denial-of-service (DDoS) attacks hit the Internet services company &lt;a shape="rect" href="http://dyn.com/"&gt;Dyn&lt;/a&gt;. The cyberattack prevented many users on the U.S. East Coast from navigating to the most popular websites of Dyn customers, which include Twitter, Reddit, and Netflix.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Dyn detected the first attack at 7:10 a.m. Eastern time on Friday and &lt;a shape="rect" href="https://www.dynstatus.com/"&gt;restored normal service&lt;/a&gt; about two hours later. Then at 11:52 a.m. ET, Dyn began investigating a second attack. By 2:00 p.m., the company said it was still working to resolve “several attacks” at once.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The interruptions inconvenienced many Internet users, and the daily operation of Internet giants in entertainment, e-commerce, and social media. There still aren’t many details available about Dyn’s predicament, and the company did not immediately respond to an interview request. But we do know from Dyn’s posts that the first two assaults on its network were DDoS attacks. Its customers’ outages again show that major Internet companies remain vulnerable to this common hacker scheme—one that has plagued networks &lt;a shape="rect" href="https://www.britannica.com/topic/denial-of-service-attack"&gt;since 2000&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;A denial-of-service attack aims to slow or stop users from accessing content or services by impeding the ability of a network or server to respond to their requests. The word “distributed” means that hackers executed the Dyn attacks by infecting and controlling a large network of computers called a &lt;a shape="rect" href="https://www.microsoft.com/security/sir/story/default.aspx#!botnetsection_dos"&gt;botnet&lt;/a&gt;, rather than running it from a single machine that they own.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Hackers can assemble a botnet by spreading malware, which is often done by prompting unsuspecting users to click a link or download a file. That malware can be programmed to periodically check with a host computer owned by hackers for further instructions. To launch an attack, the hackers, or bot-herders, send a message through this “command and control” channel, prompting infected computers to send many requests for a particular website, server, or service all at once. Some of the biggest botnets in history have boasted &lt;a shape="rect" href="http://www.welivesecurity.com/2015/02/25/nine-bad-botnets-damage/"&gt;2 million computers&lt;/a&gt;, capable of sending up to &lt;a shape="rect" href="http://www.welivesecurity.com/2015/02/25/nine-bad-botnets-damage/"&gt;74 billion spam emails&lt;/a&gt; a day.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The sudden onslaught of requests quickly gobbles up all the network's bandwidth, disk space, or processing power. That means real users can’t get their requests through because the system is too busy trying to respond to all the bots. In the worst cases, a DDoS can crash a system, taking it completely offline.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Both of Friday’s attacks targeted Dyn’s &lt;a shape="rect" href="http://dyn.com/managed-dns/"&gt;Managed Domain Name System&lt;/a&gt;. Through this system, Dyn provides a routing service that translates Web addresses that users type into a browser, such as spectrum.ieee.org. Users who type in a Web address are first sent through a Dyn server that looks up the IP address for a server that hosts the content the user is trying to reach. The Dyn server passes this information on to the user's browser.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;To disrupt this process, says &lt;a shape="rect" href="http://www.albany.edu/~goel/"&gt;Sanjay Goel&lt;/a&gt;, a professor of information technology at the State University of New York (SUNY) at Albany, the bot-herders probably sent tons of translation requests directly to Dyn’s servers by looking up the servers’ IP addresses. They could have also simply asked the bots to send requests for Amazon.com and Twitter.com to cause similar issues. Attacking a DNS or a content delivery provider such as Dyn or Akamai in this manner gives hackers the ability to interrupt many more companies than they could by directly attacking corporate servers, because several companies share Dyn's network.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;In Dyn’s case, it has built its Managed DNS on an architecture called &lt;a shape="rect" href="http://dyn.com/blog/unicast-vs-anycast-dns-nameserver-routing/"&gt;Anycast&lt;/a&gt;, in which any particular IP address for a server in its system can actually be routed through servers in more than a dozen data centers. So, if the IP address of one server is targeted, 10 others may still be able to handle the normal traffic while it's beseiged with bot requests. &lt;a shape="rect" href="https://resources.sei.cmu.edu/library/author.cfm?authorid=35176"&gt;Art Manion&lt;/a&gt;, a technical manager at Carnegie Mellon University’s Software Engineering Institute, says this system should make Dyn more resilient to DDoS attacks, and the company has &lt;a shape="rect" href="http://dyn.com/ddos/"&gt;touted it&lt;/a&gt; as highly secure.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Dyn said on Friday in an update to its website that the first attack mainly impacted services in the “US East.” The Anycast network includes &lt;a shape="rect" href="http://dyn.com/dns/network-map/"&gt;data centers&lt;/a&gt; in Washington, D.C., Miami, and Newark, N.J., as well as in Dallas and Chicago, though it’s not clear whether these locations were specifically targeted.    &lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Even in the affected region, only certain users experienced issues. One reason could be that other users' browsers had previously used Dyn to locate the specific server they needed to recover, say, Twitter.com. Because that information is now cached in their browsers, &lt;span&gt;those users can bypass Dyn to fetch the desired content,&lt;/span&gt; so long as the servers that store Twitter’s website are still functioning.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Another reason for the inconsistent impacts could be that a common mechanism for handling DDoS attacks is to simply drop every fifth request from the queue in order to relieve the network of traffic. The result: Some requests from legitimate users wind up being dropped along with those from bots.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Once an attack begins, companies can bring backup servers online to manage the blizzard of requests. Victims can also work with Internet service providers to block the IP addresses of devices generating the most traffic, which means that they're likely part of the botnet. "You start blocking the different addresses where it's coming from, so depending on how massive the botnet is, it may take some time," says SUNY Albany's Goel.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Increasingly, bot-herders have &lt;a shape="rect" href="https://krebsonsecurity.com/2016/10/ddos-on-dyn-impacts-twitter-spotify-reddit/"&gt;recruited Internet of Things devices&lt;/a&gt;, which often have poor security, to their ranks. This allows them to launch ever more powerful attacks because of the sheer numbers of such devices. Two of the largest DDoS attacks on record have occurred within the past two months: first, &lt;a shape="rect" href="https://krebsonsecurity.com/2016/10/ddos-on-dyn-impacts-twitter-spotify-reddit/"&gt;a 620-gigabit-per-second attack&lt;/a&gt; targeting independent security reporter Brian Krebs of &lt;a shape="rect" href="https://krebsonsecurity.com/"&gt;KrebsonSecurity.com&lt;/a&gt;, and then &lt;a shape="rect" href="http://www.forbes.com/sites/thomasbrewster/2016/09/25/brian-krebs-overwatch-ovh-smashed-by-largest-ddos-attacks-ever/#92a7d7c6fb61"&gt;a 1,100-Gb/s siege&lt;/a&gt; on the French hosting company OVH.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Even with state-of-the-art protections and mitigation strategies, companies are limited by the amount of bandwidth they have to handle such sudden onslaughts. “Ultimately, Akamai has total &lt;em&gt;x&lt;/em&gt; amount of bandwidth, and if the attacker is sending &lt;em&gt;x&lt;/em&gt;-plus-10 traffic, the attacker still wins,” says Carnegie Mellon's Manion. “It mathematically favors whoever has more bandwidth or more traffic, and the attackers today can have more traffic.”&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Dyn’s global network manages &lt;a shape="rect" href="http://dyn.com/blog/research-end-users-web-traffic-geographic-distribution-points-of-presence/"&gt;over 500 billion queries&lt;/a&gt; a month, so the culprits would have had to send many millions or even billions of requests simultaneously in order to stall it. Manion says that to prevent DDoS attacks, companies must address root causes such as poor IoT security, rather than scrambling to stop them once they’ve begun.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 21 Oct 2016 21:00:00 GMT</pubDate>
<dc:creator>Amy Nordrum</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/security/what-is-a-distributed-denialofservice-attack-and-how-did-it-break-twitter</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMzIzMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMzIyOA.jpg" height="225" width="300"/>
</item>
<item>
<title>New Computer Combines Electronic Circuits with Light Pulses</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/new-computer-combines-electronic-circuits-with-light-pulses</link>
<description>A specialized computer based partly on light pulses could solve some of the trickiest problems for conventional computers</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;A specialized computer based partly on light pulses could solve some of the trickiest problems for conventional computers&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMjgyNA.jpeg"/&gt;
&lt;figcaption&gt;Photo: Linda Cicero/Stanford News Service&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;Modern computers still lack the capability to find the best solution for the classic “traveling salesman” problem. Even finding approximate solutions is challenging. But finding the shortest traveling salesman route among many different cities is more than just an academic exercise. This class of problems lies at the heart of many real-world business challenges such as scheduling delivery truck routes or discovering new pharmaceutical drugs. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Today&lt;/span&gt;
&lt;span&gt;’s &lt;/span&gt;
&lt;span&gt;computers handle&lt;/span&gt;
&lt;span&gt; combinatorial optimization problems by skipping some of the weaker solutions instead of considering all possibilities to find the very best solution. But U.S. and Japanese researchers have unveiled a new specialized computer that could someday solve the traveling salesman scenario and similar problems more efficiently. Their hybrid machine combines digital electronic circuits with optical devices similar to lasers.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“&lt;/span&gt;
&lt;span&gt;The simplest way to solve the traveling salesman problem is to consider all possible paths, but that is not the way that a conventional computer solves it,” &lt;/span&gt;
&lt;span id="docs-internal-guid-d3b5ca6a-e2a3-d13f-4357-adf7c79b0d77"&gt;says &lt;/span&gt;
&lt;a style="color: rgb(3, 166, 227); font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://web.stanford.edu/~pmcmahon/"&gt;Peter McMahon&lt;/a&gt;
&lt;span&gt;, a physicist at the Quantum Information Science group of Stanford University. “If you try to solve that problem on your laptop, it’s not going to naively evaluate all paths because it’s not possible for larger problem sizes.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;McMahon is part of a research group based out of Stanford University and various Japanese research institutions. The group, led by &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://web.stanford.edu/group/yamamotogroup/YY-HP/"&gt;Yoshihisa Yamamoto&lt;/a&gt;
&lt;span&gt; at the &lt;/span&gt;National Institute of Informatics in Tokyo, Japan, &lt;span&gt;published two separate but complementary papers showing the possibilities of their new computer in the 21 October 2016 online release of the journal &lt;em&gt;Science&lt;/em&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The new computer aims to s&lt;/span&gt;
&lt;span&gt;olve&lt;/span&gt;
&lt;span&gt; optimization problems through a mathematical approach known &lt;/span&gt;
&lt;span&gt;as an Ising model&lt;/span&gt;
&lt;span&gt;. The Ising model is a mathematical model that describes how magnetic materials have&lt;/span&gt;
&lt;span&gt; atomic spins that exist in either up or down states.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;By mimicking an arrangement of such tiny magnets, the specialized “Ising machine” computer can represent an optimization problem as a unique configuration of up or down spin states that each interact with one another through couplings, McMahon says. The Ising machine’s solution consists of the “ground state” configuration that minimizes the system’s overall energy given that set of couplings.&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span&gt;‘The coupling encodes the problem you want to solve,” McMahon says. “When you specify an Ising problem, the input to the computer is the couplings between the spins. The output is ideally the ground state, the configuration of spins that minimizes the energy given that set of couplings.”&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span&gt;Engineers and physicists have spent years experimenting with&lt;/span&gt;
&lt;span&gt; different “Ising machine” computers to solve such optimization problems. One approach has tried brain-inspired &lt;a shape="rect" href="http://spectrum.ieee.org/searchContent?q=neural+networks"&gt;neural networks&lt;/a&gt; built with electronic circuits. Another based on adiabatic quantum computing involves the &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/hardware/googles-first-quantum-computer-will-build-on-dwaves-approach"&gt;D-Wave quantum annealing machines&lt;/a&gt; being tested by Google, NASA and Lockheed Martin. A third approach has even experimented with encoding optimization problems within biological DNA molecules as a form of &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/devices/whatever-happened-to-the-molecular-computer"&gt;molecular computing&lt;/a&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The combined U.S.-Japan research group took a very different approach to building Ising machines. They used pulses of light from a device similar to a laser—called an &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Optical_parametric_oscillator"&gt;optical parametric oscillator&lt;/a&gt;—to represent the magnetic spins. Those light pulses were measured individually and combined with one another to form larger systems simulating arrangements of tiny magnets.&lt;/p&gt;
&lt;p&gt;Those light pulses run through about 300 meters of optical fiber. Most of the hardware used to build the Ising machines involved off-the-shelf equipment that is standard in the telecommunications industry.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“Picture a plastic spool with a lot of optical fiber wound around it,” McMahon says. “One of nice things about this experiment is that we’re running at telecom wavelength, so all the components are standard telecom fiber, connectors, detectors, etc.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The research group, led by Yamamoto, first developed a small working prototype of this unusual Ising machine two years ago. But they soon realized that the expensive equipment and engineering challenges necessary to make an Ising machine completely based on light pulses would be very difficult to scale, McMahon explains.&lt;/p&gt;
&lt;p&gt;For example, a 100-spin machine would have required 99 optical delay lines and 99 modulators to control and combine the light pulses. Each of the modulators would have cost anywhere between $5,000 and $10,000.&lt;/p&gt;
&lt;p&gt;The engineering challenge comes from the need to keep the light wavelength stabilized within 100 nanometers across the 300-meter length of optical fiber over a long period of time. That challenge would have been multiplied by 99 times if the Ising machine had 99 optical delay lines.&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;McMahon and his Stanford-based colleagues did succeed in creating a 100-spin machine as &lt;a shape="rect" href="http://science.sciencemag.org/lookup/doi/10.1126/science.aah5178"&gt;described in their paper&lt;/a&gt; published in &lt;em&gt;Science&lt;/em&gt;. But they accomplished this by replacing the optical delay lines with digital electronic circuits. That digital component of the machine simulates the light pulse interactions that would have occurred and then translates the information back into the optical portion of the system.&lt;/p&gt;
&lt;p&gt;“&lt;span id="docs-internal-guid-d3b5ca6a-e2e0-fce3-4111-cf1459c8081e"&gt;
&lt;span&gt;Instead of having all optical machine, now we have a hybrid that is part digital and part optical,” McMahon says.”&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The Stanford group&lt;/span&gt;
&lt;span&gt; performed exhaustive testing with their 100-spin machine. They ran the machine through 4000 versions of optimization &lt;/span&gt;
&lt;span&gt;problems to show that the machine&lt;/span&gt;
&lt;span&gt;’s solutions are not limited to specific optimization problems. They also tested the machine’s capabilities from just a few artificial spins up to 100 spins so that they could see its performance at different system sizes. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;By comparison, the Japanese researchers focused on building a much larger Ising machine that can simulate 2048 artificial spins. Their &lt;a shape="rect" href="http://science.sciencemag.org/lookup/doi/10.1126/science.aah4243"&gt;research paper&lt;/a&gt; showed that this Ising machine can still work at fairly large configurations. They pushed the machine’s upper limits by focusing on just three optimization problems involving all 2048 artificial spins.&lt;/p&gt;
&lt;p&gt;But unlike the Stanford group’s 100-spin machine, the Japanese group’s larger machine had less fine-grained resolution in terms of the coupling interactions between each artificial spin. (Imagine trying to solve the traveling salesman problem with a computer that could only figure out solutions within 100-mile segments instead of within much smaller distances.)&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Still, the two papers published by the U.S. and Japanese research groups complement one another by highlighting different aspects of the new Ising machine. The Stanford group’s exhaustive testing proved the Ising machine’s principles seemed solid. The Japanese group showed that the Ising machine can be scaled up to larger sizes that could someday make it a commercially viable technology. Pushing the upper boundaries of the technology was of particular interest to the researchers affiliated with NTT Corporation, a Japanese telecommunications company.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The big question with this version of an Ising machine is whether it can beat the best software algorithms running on classical computers. Both the U.S. and Japanese research groups will be carrying out such “speedup” benchmark testing over the next several years.&lt;/p&gt;
&lt;p&gt;But the early results from their papers seem promising. The Stanford group’s 100-spin machine was able to solve certain optimization problems with about 99 percent success in about 100 milliseconds (thousandths of a second). That performance is “very comparable” to a classical digital computer, McMahon says.&lt;/p&gt;
&lt;p&gt;The Japanese group’s 2048-spin machine actually proved 50 times faster than a “simulated annealing” classical computing algorithm on a test run involving a dense graph problem. But &lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://www.brl.ntt.co.jp/people/htakesue/"&gt;Hiroki Takesue&lt;/a&gt;
&lt;span&gt;, a senior research scientist at the NTT Basic Research Laboratories in Japan,&lt;/span&gt; cautioned that such performance on one test does not prove the Ising machine beats classical computers in general.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“This is in fact our first serious computation experiment, but surprisingly, the coherent Ising machine is already much faster than a modern computer,” Takesue says&lt;/span&gt;
&lt;span&gt;. “But note that this is confirmed only with a particular problem, so we need more research to clarify the advantages and disadvantages of the [Ising machine] over the conventional computers.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Takesue and his Japanese colleagues plan to confirm whether or not the 2048-spin Ising machine does indeed have an advantage over classical computers in solving such dense graph problems. They also aim to boost the number of artificial spins within the Ising machine by at least 10 times over the next three years. And they hope to develop a Web interface that enables outside Internet users to test their Ising machine online.&lt;/p&gt;
&lt;p&gt;The Japanese researchers will also begin seeing how the Ising machines could tackle real-world commercial applications. For example, NTT Corporation is interested in using the new computer for optimizing its mobile network communications. &lt;a shape="rect" href="http://www.nii.ac.jp/en/faculty/informatics/utsunomiya_shoko/"&gt;Shoko Utsunomiya&lt;/a&gt;, an associate professor at the National Institutes of Informatics in Japan and coauthor on one of the &lt;em&gt;Science&lt;/em&gt; papers, is studying whether the Ising machine could help uncover new molecule configurations to discover new pharmaceutical drugs.&lt;/p&gt;
&lt;p&gt;Meanwhile, the Stanford University group plans to continue investigating the fundamental workings of the Ising machine and to see whether it can truly outperform classical computers in general on optimization problems. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“U&lt;/span&gt;
&lt;span&gt;ltimately, you need to build a machine of&lt;/span&gt;
&lt;span&gt; a particul&lt;/span&gt;
&lt;span&gt;ar&lt;/span&gt;
&lt;span&gt; size and carefully benchmark it against classical solvers,&lt;/span&gt;
&lt;span&gt;” McMahon says.&lt;/span&gt;
&lt;span&gt; &lt;/span&gt;
&lt;span&gt;“&lt;/span&gt;
&lt;span&gt;There will be a lot of work in that direction in the next one to five years.&lt;/span&gt;
&lt;span&gt;”&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 20 Oct 2016 18:00:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/new-computer-combines-electronic-circuits-with-light-pulses</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMjgzNg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMjgzNA.jpg" height="225" width="300"/>
</item>
<item>
<title>Can Stanford’s Deep Dive Into Virtual Reality Help Save the Oceans?</title>
<link>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gadgets/can-stanfords-deep-dive-into-virtual-reality-help-save-the-oceans</link>
<description>Stanford researchers are releasing free VR software for the HTC Vive in hopes that it will change people for the better</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Stanford researchers are releasing free VR software for the HTC Vive in hopes that it will change people for the better&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMjA1OA.jpeg"/&gt;
&lt;figcaption&gt;Image: Virtual Human Interaction Lab/Stanford University&lt;/figcaption&gt;
&lt;figcaption&gt;The Stanford Ocean Acidification Experience uses virtual reality to take people to a dying coral reef, where they can pick up objects and hunt for ocean life&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Stanford professor &lt;a shape="rect" href="https://comm.stanford.edu/faculty-bailenson/"&gt;Jeremy Bailenson&lt;/a&gt; and fellow researchers at the school’s &lt;a shape="rect" href="https://vhil.stanford.edu/"&gt;Virtual Human Interaction Lab (VHIL)&lt;/a&gt; have been exploring the effects of virtual reality on human behavior &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/consumer-electronics/portable-devices/virtual-reality-pioneer-looks-beyond-entertainment"&gt;since the late 1990s&lt;/a&gt;. They’ve written countless papers documenting the fact that experiences in a virtual world—like exercising more, saving for retirement, using less paper, or showing more empathy—change behavior in the real one. They initially used expensive, custom-built hardware for their research; the kind of VR systems available today, like the &lt;a shape="rect" href="https://www3.oculus.com/en-us/rift/"&gt;Oculus Rift&lt;/a&gt; and the &lt;a shape="rect" href="https://www.vive.com/us/"&gt;HTC Vive&lt;/a&gt;, didn’t exist when they conducted most of their experiments.&lt;/p&gt;
&lt;p&gt;But now that VR systems have gotten out of the lab and into the world, the team is beginning to let some of its work loose as well.&lt;/p&gt;
&lt;p&gt;This week, for the first time, the researchers publicly released one of their potentially behavior-changing VR simulations &lt;a shape="rect" href="http://vhil.stanford.edu/soae/"&gt;for free download&lt;/a&gt; for the HTC Vive. &lt;a shape="rect" href="http://vhil.stanford.edu/soae/"&gt;The Ocean Acidification Experience&lt;/a&gt; is intended to teach users about the chemistry behind ocean acidification, as well as the problems it causes, and what they can do to help prevent it. To hit those marks, of course, the simulation has to be engaging enough to keep users involved.&lt;/p&gt;
&lt;p&gt;Bailenson hopes he’s hit that sweet spot, and that the software will go viral. How would the group measure success? Says Bailenson: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At the very least, people who become aware of it will now at least have heard of ocean acidification. Even better would be if all the 120,000 people who have the Vive hardware would download it and show it to friends. A home run for us would be if Google and Facebook and Oculus and Sony and all the other companies making VR hardware would include this with the hardware. Because if the tech companies embrace it fully, when VR gets into the classroom, our software will go with it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And why shouldn’t the lab hit a home run? Bailenson arguably has more experience creating virtual worlds than anybody out there. And, today, there’s a dearth of high-quality VR experiences available. Plus, consumers always like free stuff. “Nobody is making money off of it. It isn’t activism; it is marine science, pure education,” he says. (The development effort was funded by the Gordon and Betty Moore Foundation.)&lt;/p&gt;
&lt;p&gt;Bailenson, along with Roy Pea, a professor in the Stanford Graduate School of Education, and &lt;span&gt;Fiorenza Micheli and Kristy Kroeker, &lt;/span&gt;two Stanford marine biologists (Kroeker is now at the University of California at Santa Cruz), began developing the simulation in 2013. “There were whispers in the air that consumer virtual reality was coming,” Bailenson said. “So we started this project thinking we were going to share it.”&lt;/p&gt;
&lt;p&gt;Bailenson describes the general story line:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It starts with a globe. We talk about how we can see climate on the coastlines, but nobody can see how carbon dioxide affects the oceans. We then take you into a crowded city. You touch an exhaust pipe, and you then see carbon dioxide go into the atmosphere, and you’re told to follow one particular molecule.&lt;/p&gt;
&lt;p&gt;Then you are in a boat, on the ocean, you see your molecule come towards you. You touch it and push it into the water; when it lands you see the chemical reaction that creates acid; that’s the chemistry lesson.&lt;/p&gt;
&lt;p&gt;Then you are underwater, at this special reef in Ischia, Italy. This reef has naturally occurring carbon dioxide from underwater volcanoes; it shows how all our oceans will look by 2100. We take you to a normal reef, where you see coral, and count sea snails and species of fish. Then you go to an acidified reef; you see that algae have taken over the reef, there is no coral; there are fewer fish species, and no sea snails.&lt;/p&gt;
&lt;p&gt;The final scene tells you what you can do to help, prevent this future, including managing your own carbon footprint, talking to decision makers, and supporting research organizations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;People without an HTC Vive can download &lt;a shape="rect" href="https://www.youtube.com/watch?v=mtB1NCrs1Dw"&gt;a related 360-degree video&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Stanford Ocean Acidification Experience is the Virtual Human Interaction Labs’ first attempt, as Bailenson puts it, “to come up with prosocial content that will help people lead better lives.” But it’s far from the last. Next up, Bailenson says, is an empathy-building virtual reality experience that shows what it feels like to go from having a home to being homeless. Look for that in a couple of months…or so. “We are a strong research lab that studies how VR affects people,” he says. “Production doesn’t come naturally to us.”&lt;/p&gt;
&lt;p&gt;But for the chance to go from just studying how VR can change the world, to actually using VR for good, it’s worth the effort.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 19 Oct 2016 18:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gadgets/can-stanfords-deep-dive-into-virtual-reality-help-save-the-oceans</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMjA3Mg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMjA3MA.jpg" height="225" width="300"/>
</item>
<item>
<title>Software, the Invisible Technology</title>
<link>http://spectrum.ieee.org/computing/software/software-the-invisible-technology</link>
<description>We used to go to stores to buy it. Now software is so ubiquitous, we don’t even notice it</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;We used to go to stores to buy it. Now software is so ubiquitous, we don’t even notice it&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMDUzMw.jpeg"/&gt;
&lt;figcaption&gt;Illustration: Greg Mably&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgyMDUzMg.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: Greg Mably&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Five years ago &lt;a shape="rect" href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460"&gt;Marc Andreessen&lt;/a&gt;, the Web pioneer and celebrated tech investor, predicted software would eat the world.&lt;/p&gt;
&lt;p&gt;He turned out to be right. Too right. Software is eating the world, and also eating itself.&lt;/p&gt;
&lt;p&gt;The cannibalization of software defies easy explanation. Software is the motor of the world’s digital economy. Code is the ground of our computationally rich existence. Software applications and platforms are the source of vast wealth for Apple, Google, Facebook, Amazon, and many other tech titans. Yet even as software grows in importance, code becomes less visible, less tangible, less understood, and—perhaps most paradoxically—less valuable in monetary terms.&lt;/p&gt;
&lt;p&gt;How has this great shift happened? Software originally coevolved with computers themselves. The &lt;a shape="rect" href="https://en.wikipedia.org/wiki/IBM_System/360"&gt;IBM System/360&lt;/a&gt;, the business computer of choice in the 1960s, came bundled with code, and if customers needed more or different programs, they asked IBM. In 1980, IBM chose to rely on outsiders, notably Microsoft, for PC code, igniting a ferocious race to sell programs as distinct products.&lt;/p&gt;
&lt;p&gt;Over the past 20 years, the program as artifact has vanished. Consumers download new versions, patches, and feature improvements as easily as switching on their devices. They rarely pay for this. Software battles now occur over platforms, which define an experience such as shopping (Amazon), searching (Google), or networking (LinkedIn). Competitive advantage is now achieved through superior software, but software supremacy is neither the aim nor the result of the new game.&lt;/p&gt;
&lt;p&gt;“&lt;a shape="rect" href="http://spectrum.ieee.org/transportation/systems/this-car-runs-on-code"&gt;Software on wheels&lt;/a&gt;,” for instance, now defines the car of the future, and is the reason why Google and Apple, the reigning kings of code, each are pushing for a big role in next-gen autos. No surprise that Toyota, Ford, and Daimler want their own software expertise, not to peddle programs to drivers but to enhance their new models. Without cool software, these venerable automakers might not even survive (see Tesla’s and Google’s &lt;a shape="rect" href="http://spectrum.ieee.org/static/the-self-driving-car"&gt;self-driving car&lt;/a&gt;s).&lt;/p&gt;
&lt;p&gt;View Uber in this same light. The company is best understood not as a taxi service on steroids but as a software-management system for personal transportation.&lt;/p&gt;
&lt;p&gt;The bottom line: No one gets rich making software anymore. The days of Bill Gates building a fortune on the strength of shrink-wrapped programs sold like disposable diapers is gone. Software today can make you rich only by enabling you to do something else that people pay for.&lt;/p&gt;
&lt;p&gt;Netflix, Facebook, and Google don’t get a dime from selling software, yet their revenue-producing services depend on continuously and seamlessly improving their code. Similar examples are legion.&lt;/p&gt;
&lt;p&gt;The new logic of software has different implications for those who make it, sell it, and use it. For makers, code no longer spawns tycoons and celebrities. The last person to get famous from writing software was &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/linux-at-25-qa-with-linus-torvalds"&gt;Linus Torvalds;&lt;/a&gt; and this year he celebrates the &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/linux-at-25-why-it-flourished-while-others-fizzled"&gt;25th anniversary&lt;/a&gt; of his seminal achievement. Today’s top coders are largely if not wholly unknown by the wider public; at best, they are cult figures, revered in underground communities.&lt;/p&gt;
&lt;p&gt;Another paradox: As software becomes ever more essential to creating the digital experience, the invisibility of software is a victory for the apostles of “the free.” Hippies, misfits, and dropouts improbably created fabulous wealth through pricey apps in the last two decades of the 20th century. Some among these characters, notably &lt;a shape="rect" href="https://stallman.org/"&gt;Richard Stallman&lt;/a&gt;, promoted a counter-ethos that conceived of software as a public good, available without charge to all.&lt;/p&gt;
&lt;p&gt;That free software sits at the heart of an explosion of profitable digital services is the latest, greatest riddle of global capitalism—and a sobering message for the poets of programming. Code writers are essential and well paid but increasingly interchangeable and anonymous.&lt;/p&gt;
&lt;p&gt;For this, we should not pity the programmer, and we should remember: Hope springs from the unseen.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the November 2016 print issue as “&lt;span&gt;The Invisible Technology&lt;/span&gt;.”&lt;/em&gt;
&lt;/p&gt;
&lt;h2&gt;About the Author&lt;/h2&gt;
&lt;p&gt;
&lt;a shape="rect" href="https://webapp4.asu.edu/directory/person/1643533"&gt;
&lt;strong&gt;G. Pascal Zachary&lt;/strong&gt;
&lt;/a&gt; is a professor of practice at Arizona State University’s School for the Future of Innovation in Society.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 19 Oct 2016 15:00:00 GMT</pubDate>
<dc:creator>G. Pascal Zachary</dc:creator>
<guid>http://spectrum.ieee.org/computing/software/software-the-invisible-technology</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMDU0OQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMDU0Nw.jpg" height="225" width="300"/>
</item>
<item>
<title>How Analog and Neuromorphic Chips Will Rule the Robotic Age</title>
<link>http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/analog-and-neuromorphic-chips-will-rule-robotic-age</link>
<description>AI is going to make analog computing popular again</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;AI is going to make analog computing popular again&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMTAwMw.jpeg"/&gt;
&lt;figcaption&gt;Images: DJI; Google; Amazon; Amazon Robotics, iRobot&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;
&lt;em&gt;This is a guest post. The views expressed here are solely those of the author and do not represent positions of &lt;/em&gt;IEEE Spectrum&lt;em&gt; or the IEEE.&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;When it comes to new technologies and products, we tend to think of “digital” as synonymous with advanced, modern, and high-def, while “analog” is considered retrograde, outmoded, and low-resolution.&lt;/p&gt;
&lt;p&gt;But if you think analog is dead, you’d be wrong. Analog processing not only remains at the heart of many vital systems we depend on today, it is now going to make its way into a new breed of compute and intelligent systems that will power some of the most exciting technologies of the future: artificial intelligence and robotics.&lt;/p&gt;
&lt;p&gt;Before discussing the upcoming analog renaissance—and why engineers and innovators working on AI and robot applications should be paying close attention to it—it might be helpful to understand the significance and legacy of the Old Analog Age.&lt;/p&gt;
&lt;h4&gt;OUR LOVE FOR ANALOG&lt;/h4&gt;
&lt;p&gt;
&lt;span&gt;During &lt;/span&gt;
&lt;span&gt;World War II, &lt;/span&gt;analog circuits played a key role in enabling the first automatic anti-aircraft &lt;a shape="rect" href="http://books.google.com/books?id=Hc5NfBKdl_wC&amp;amp;lpg=PA87&amp;amp;ots=UHl4x5Yh_D&amp;amp;dq=analog%20anti%20aircraft%20mindell&amp;amp;pg=PA1#v=onepage&amp;amp;q&amp;amp;f=false"&gt;fire-control systems&lt;/a&gt; and, in the decades that followed, analog computers became an essential tool for calculating flight trajectories for rockets and spacecraft.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Analog also became prevalent in control and communication systems used in &lt;/span&gt;
&lt;span&gt;aircraft, ships, and power plants, and some of those systems are still in operation to this day. U&lt;/span&gt;ntil not long ago, analog circuits controlled vast portions of our telecommunications infrastructure (remember the rotary dial?), and they even ruled the office copy room, where early photocopier machines reproduced images without handling a single digital bit.&lt;/p&gt;
&lt;p&gt;Our love for analog persisted for so long because this technology proved, again and again, to be accurate, simple, and fast. It steered our rockets, sailed our ships, recorded and played back our music and videos, and connected us to each other for many decades. And then, starting in the mid- to late-1960s, digital came along and rapidly took over.&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgyMDkxOA.jpeg"/&gt;
&lt;figcaption&gt;
&lt;p&gt;Feedback carburetors were introduced to mix fuel more efficiently as a function of exhaust emissions. They were complex, fragile, unreliable devices that were quickly rendered extinct by &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Fuel_injection#Electronic_injection"&gt;electronic fuel injection systems&lt;/a&gt; that digitally calculated air/fuel mixtures to control emissions. It is unlikely that these types of precision control systems will go back to being analog. However, the intelligent machines that run on top of them—like the self-driving AI on a car—may run on analog and neuromorphic computers.&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h4&gt;DIGITAL DOMAIN&lt;/h4&gt;
&lt;p&gt;Why was analog abandoned for digital? The biggest weakness of analog is that it’s rigid, and it gets exponentially more complex when you try to make it flexible. More complexity brings a disproportionate reduction in reliability (bad), so engineers began to notice that &lt;a shape="rect" href="http://spectrum.ieee.org/static/special-report-50-years-of-moores-law"&gt;Moore’s Law&lt;/a&gt; was making dense compute extremely reliable and cheap.&lt;/p&gt;
&lt;p&gt;Meanwhile, MEMS and microfabrication techniques commoditized sensors that capture physical signals and convert them to digital. Soon enough, operational amplifiers were abandoned for logic gates that got exponentially cheaper just as Moore’s Law predicted. Mechanical linkages became “&lt;a shape="rect" href="https://en.wikipedia.org/wiki/Fly-by-wire"&gt;fly-by-wire&lt;/a&gt;,” and designers pushed digitization further to the edge.&lt;/p&gt;
&lt;aside class="inlay pullquote xlrg"&gt;Now, after a long hiatus, Carver Mead’s prediction of the return to analog is starting to become a reality&lt;/aside&gt;
&lt;p&gt;In today’s consumer electronics world, analog is only used to interface with humans, capturing and producing sounds, images, and other sensations. In larger systems, analog is used to physically turn the wheels and steer rudders on machines that move us in our analog world. But for most other electronic applications, engineers rush to dump signals into the digital domain whenever they can. The upshot is that the benefits of digital logic&lt;span&gt;—&lt;/span&gt;
&lt;span&gt;cheap, fast, robust, flexible—&lt;/span&gt;have made engineers practically allergic to analog processing.&lt;/p&gt;
&lt;p&gt;Now, however, after a long hiatus, &lt;a shape="rect" href="https://web.stanford.edu/group/brainsinsilicon/documents/MeadNeuroMorphElectro.pdf"&gt;Carver Mead’s prediction&lt;/a&gt; of the return to analog is starting to become a reality.&lt;/p&gt;
&lt;p&gt;“Large-scale adaptive analog systems are more robust to component degradation and failure than are more conventional systems, and they use far less power&lt;span&gt;,” &lt;/span&gt;
&lt;a shape="rect" href="http://www.carvermead.caltech.edu/"&gt;Mead&lt;/a&gt;
&lt;span&gt;, a Caltech professor and microelectronics pioneer, wrote in an influential &lt;/span&gt;
&lt;em&gt;Proceedings of the IEEE&lt;/em&gt;
&lt;span&gt; &lt;/span&gt;
&lt;a shape="rect" href="http://web.stanford.edu/group/brainsinsilicon/documents/MeadNeuroMorphElectro.pdf"&gt;paper&lt;/a&gt;
&lt;span&gt; in 1990. “&lt;/span&gt;For this reason, adaptive analog technology can be expected to utilize the full potential of wafer-scale silicon fabrication.”&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgyMDkxNw.jpeg"/&gt;
&lt;figcaption&gt;Analog computing conjures visions of big, clunky machines, while digital is viewed as smart and sleek. Now, however, a wave of emerging AI applications may revive the notion of neuromorphic and analog compute. Future robot butlers will “think” more like the meaty computer in our heads than the processors in our PCs and smartphones.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h4&gt;AI LIKES ANALOG&lt;/h4&gt;
&lt;p&gt;
&lt;span&gt;Hardware engineers typically view analog as a necessary evil to interact with the physical world. But it turns out that AI and &lt;/span&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/tag/deep+learning"&gt;deep learning&lt;/a&gt;
&lt;span&gt; algorithms seem to conform better to analog and &lt;/span&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/hardware/ibms-braininspired-chip-tested-on-deep-learning"&gt;neuromorphic compute&lt;/a&gt; platforms.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;At my firm Lux Capital, &lt;/span&gt;
&lt;a shape="rect" href="https://medium.com/@lux_capital/a-coming-semiconductor-nervana-64007aa82ca6#.28lbbwluz"&gt;we funded Nervana&lt;/a&gt;,&lt;span&gt; which built ASICs that ran convolutional neural networks&lt;/span&gt;
&lt;span&gt; natively to accelerate the training of deep learning algorithms. Although the mathematical operations were done in the digital domain, the overall system architecture mimicked the human brain at a very high level.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The approach worked well for training AIs in the cloud, though it still consumed too much power to run a portable device like an untethered robot. Nevertheless, Nervana’s expected performance boost was so compelling &lt;/span&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/hardware/the-chip-that-intel-wants-from-nervana-systems"&gt;that Intel rushed to acquire the company&lt;/a&gt;
&lt;span&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;aside class="inlay pullquote xlrg"&gt;We’re at a unique intersection where the neural networks we’re trying to implement are more suitable to analog designs, while demand for these types of AI circuits is expected to explode&lt;/aside&gt;
&lt;h4&gt;INSPIRED BY NATURE&lt;/h4&gt;
&lt;p&gt;Ask almost anyone (even kids) to sketch out a picture of a robot, and you’ll likely get an image that resembles Rosie, the robot maid from &lt;em&gt;The Jetsons&lt;/em&gt;, or C-3PO from &lt;em&gt;Star Wars&lt;/em&gt;. That’s not surprising—that’s the vision of robots that science-fiction has portrayed in books, television, and movies for decades. More recently, however, the idea of what a robot is or looks like seems to be evolving. Ask a millennial for an example of a robot and the answer might be Roomba, Amazon’s Echo, or maybe even Siri.&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxODczOA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Images: AF Archive/Alamy (Bender); Ronald Grant Archive/Alamy (C-3PO); United Archives GmbH/Alamy (Rosie); DJI; Google; Amazon; Amazon Robotics, iRobot&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;p&gt;Science fiction promised us a future populated by humanoid robots like C-3PO and Rosie. The robots we have today are something very different, though: They are task-specific machines like automatic vacuums, camera drones, and voice-based assistants. Analog sensing and compute systems could significantly improve the capabilities of today’s robots and AI, making them faster, smaller, and less power-hungry. They may not give us Rosie, but they’ll enable robotic applications we have yet to envision.&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;There’s a clear trend that more and more gadgets and other systems that are part of our lives—from thermostats to cars—will become more intelligent, more robotic. These systems will require computers that are small, portable, and low power; they will also need to be super responsive and alert at all times. This is a tough set of requirements for today’s compute systems, which typically consume sizable amounts of power (unless they’re on standby) and need to be cloud-connected to perform useful functions. That’s where analog can help.&lt;/p&gt;
&lt;p&gt;Inspired by nature, scientists &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/imaging/neuromorphic-vision-chips"&gt;have been experimenting&lt;/a&gt; with analog circuits to better &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/devices/how-neuromorphic-image-sensors-steal-tricks-from-the-human-eye"&gt;see&lt;/a&gt; and &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/biomedical/devices/silicon-cochlea-mimics-human-hearing"&gt;hear&lt;/a&gt; while consuming a fraction of the power. &lt;a shape="rect" href="https://web.stanford.edu/group/brainsinsilicon/"&gt;Stanford’s Brains in Silicon&lt;/a&gt; project and the &lt;a shape="rect" href="http://www.eecs.umich.edu/micl/"&gt;University of Michigan IC Lab&lt;/a&gt;, with backing from &lt;a shape="rect" href="http://www.artificialbrains.com/darpa-synapse-program"&gt;DARPA’s SyNAPSE&lt;/a&gt; and the &lt;a shape="rect" href="http://www.onr.navy.mil/Science-Technology/Departments/Code-31/All-Programs/312-Electronics-Sensors/Nanoscale-Electronics.aspx"&gt;U.S. Office of Naval Research&lt;/a&gt;, are building tools to make it easier to build analog neuromorphic systems. Stealthy startups are also beginning to emerge. Rather than attempt to run deep nets on standard digital circuits, they have designed analog systems that can perform similar computations with much less power, with inspiration from our analog brains.&lt;/p&gt;
&lt;h4&gt;NOISE IS NO PROBLEM&lt;/h4&gt;
&lt;p/&gt;
&lt;p&gt;Why the move to analog &lt;em&gt;now&lt;/em&gt;? The answer is simple: We’re at a unique intersection where the neural nets we’re trying to implement are more suitable to analog designs, while demand for these types of AI circuits is expected to explode.&lt;/p&gt;
&lt;p&gt;Traditional hard-coded algorithms only function when the innards of the compute are accurate and precise. If the circuits that run traditional hard-coded algorithms aren’t precise, errors will grow out of control as they propagate through the system. Not so with neural nets, where the internal states don’t have to be precise and the system adapts to produce the desired output for a given input. Indeed, our brains are incredibly noisy systems that work just fine. Engineers are learning that they can implement deep nets in silicon using noisy analog approaches as well—yielding an energy savings on the order of 100x.&lt;/p&gt;
&lt;p&gt;The implications are huge. Imagine a future wearable device or an Amazon Echo-type assistant that uses almost no power—or that can even harness it from the environment—requiring no power cables or batteries. Or picture a gadget that won’t have to be “cloud connected” to be smart—it will carry enough “intelligence” to be able to work even where no Wi-Fi or cellular coverage is available. And this is just the beginning of what I expect to be a whole new category of amazing AI and robot products to emerge in the not-too-distant future—all thanks to good ol’ analog.&lt;/p&gt;
&lt;div&gt;
&lt;p&gt;
&lt;em&gt;
&lt;a shape="rect" href="http://www.luxcapital.com/team/shahin-farshchi/"&gt;Shahin Farshchi&lt;/a&gt;, an IEEE Member, is a partner at &lt;a shape="rect" href="http://www.lux.vc/"&gt;Lux Capital&lt;/a&gt;, where he invests in hardware, robotics, AI, and space startups. Follow him on Twitter:&lt;a shape="rect" href="https://twitter.com/farshchi"&gt;@farshchi&lt;/a&gt;
&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;section role="menu"/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 17 Oct 2016 19:45:00 GMT</pubDate>
<dc:creator>Shahin Farshchi</dc:creator>
<guid>http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/analog-and-neuromorphic-chips-will-rule-robotic-age</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMTAxOA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMTAxNg.jpg" height="225" width="300"/>
</item>
<item>
<title>Pittsburgh's AI Traffic Signals Will Make Driving Less Boring</title>
<link>http://spectrum.ieee.org/cars-that-think/robotics/artificial-intelligence/pittsburgh-smart-traffic-signals-will-make-driving-less-boring</link>
<description>AI-powered traffic light coordination would cut time spent in the car, traffic congestion, and emissions</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;AI-powered traffic light coordination would cut time spent in the car, traffic congestion, and emissions&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMDk1MQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Idling in rush-hour traffic can be mind-numbing. It also carries other costs. Traffic congestion costs the U.S. economy $121 billion a year, mostly due to lost productivity, and produces about 25 billion kilograms of carbon dioxide emissions, Carnegie Mellon University professor of robotics Stephen Smith told the audience at a &lt;a shape="rect" href="http://www.frontiersconference.org"&gt;White House Frontiers Conference&lt;/a&gt; last week. In urban areas, drivers spend 40 percent of their time idling in traffic, he added.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The big reason is that today’s traffic signals are dumb. Smith is developing smart &lt;a shape="rect" href="http://spectrum.ieee.org/robotics/artificial-intelligence"&gt;artificial-intelligence&lt;/a&gt;-fueled traffic signals that adapt to changing traffic conditions on the fly. His startup &lt;a shape="rect" href="https://www.surtrac.net"&gt;Surtrac&lt;/a&gt; is commercializing the technology.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;In pilot tests in Pittsburgh, the smart traffic-management system has gotten impressive results. It reduced travel time by 25 percent and idling time by over 40 percent. That means less time spent staring out the windshield and more time working, being with your family, or doing anything else. I’m a Pittsburgh resident who has witnessed the city’s rapidly evolving urban landscape. And I can attest to the mostly frustration-free driving that has resulted from this system despite the city’s growing population. &lt;/p&gt;
&lt;p&gt;The researchers also estimate that the system cuts emissions by 21 percent. &lt;span&gt;It could also save cities the cost of road widening or eliminating street parking by boosting traffic throughput.&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Conventional traffic lights have preprogrammed timing that’s updated every few years. But as traffic patterns evolve, the systems can fall out of date much more quickly than that.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The Surtrac system instead relies on computerized traffic lights coordinating closely with each other. Radar sensors and cameras at each light detect traffic. Sophisticated AI algorithms use that data to build a timing plan “that moves all the vehicles it knows about through the intersection in the most efficient way possible,” Smith says. The computer also sends the data to traffic intersections downstream so they can plan ahead.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Unlike other &lt;a shape="rect" href="http://spectrum.ieee.org/geek-life/history/the-man-who-invented-intelligent-traffic-control-a-century-too-early"&gt;smart traffic-management systems&lt;/a&gt;, such as one used in Los Angeles, Smith emphasized that this one is decentralized. So each signal makes its own timing decisions, making it a truly smart system.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Smith’s team started by implementing the AI traffic control system at nine intersections in Pittsburgh’s busy East Liberty neighborhood in 2012. The network now spans 50 intersections, with plans to expand it citywide. &lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The next step is to have traffic signals talk to cars. Smith’s group has already installed &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/infrastructure/cars-talk-to-cars-on-the-autobahn"&gt;short-range radios&lt;/a&gt; at 24 intersections. Such systems are expected to begin being built into some cars in 2017, he said. Traffic signals could then let drivers know of upcoming traffic conditions or let them know that lights are about to change, increasing safety and relieving congestion.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;A &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/advanced-cars/researchers-prove-connected-cars-can-be-tracked"&gt;vehicle-to-infrastructure communication system&lt;/a&gt; could also prioritize certain vehicles. The CMU team is working with the Pittsburgh Port Authority to develop a system that prioritizes public transport buses.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Pittsburgh is the &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/uber-will-start-driverless-service-in-pittsburghthis-month"&gt;test bed for Uber’s self-driving cars&lt;/a&gt;, and Smith’s work on AI-enhanced traffic signals that talk with self-driving cars is paving the way for the ultimately &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/the-scary-efficiency-of-autonomous-intersections"&gt;fluid and efficient autonomous intersections&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 17 Oct 2016 18:32:00 GMT</pubDate>
<dc:creator>Prachi Patel</dc:creator>
<guid>http://spectrum.ieee.org/cars-that-think/robotics/artificial-intelligence/pittsburgh-smart-traffic-signals-will-make-driving-less-boring</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMDk2OA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMDk2Ng.jpg" height="225" width="300"/>
</item>
<item>
<title>Analyst: Ugly Year for Tech Layoffs, and It’s Going to Get Worse</title>
<link>http://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/analyst-ugly-year-for-tech-layoffs-and-its-going-to-get-worse</link>
<description>This year’s big company tech layoffs are just the beginning; the layoff tsunami is about to hit startups, says Trip Chowdhry</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;This year’s big company tech layoffs are just the beginning; the layoff tsunami is about to hit startups, says Trip Chowdhry&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgyMDE3NA.jpeg"/&gt;
&lt;figcaption&gt;Photo: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Early this year, analyst Trip Chowdhry from &lt;a shape="rect" href="http://www.globalequitiesresearch.com/"&gt;Global Equities Research&lt;/a&gt; predicted that the tech world was going to see big layoffs in 2016—some 330,000 in all at major tech companies. At the time, these numbers seemed way over the top. Then IBM started &lt;a shape="rect" href="http://spectrum.ieee.org/view-from-the-valley/computing/it/reports-coming-in-of-mass-us-layoffs-underway-today-at-ibm"&gt;slashing jobs in March&lt;/a&gt;—and continued to wield the ax over and over as the year progressed. Yahoo began layoffs of some 15 percent of its employees in February. Intel announced in April that it would &lt;a shape="rect" href="http://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/intel-confirms-major-layoff-11-percent-of-employees-to-go"&gt;lay off 12,000&lt;/a&gt; this year.&lt;/p&gt;
&lt;p&gt;So, was Chowdhry right? “Yes,” he told me when I asked him this week. “The layoffs I predicted have been occurring.” And worse, he says, these laid-off workers are never again going to find tech jobs: “They will always remain unemployed,” at least in tech, he said. “Their skills will be obsolete.”&lt;/p&gt;
&lt;p&gt;Some of these layoffs are due to a sea change in the industry, as it transforms to the world of mobile and cloud. But some are signs of a bubble about to pop.&lt;/p&gt;
&lt;p&gt;It’s all going to get worse in 2017, he predicts, because that’s when the tech bubble will burst. Chowdhry, someone who has never been reluctant &lt;a shape="rect" href="http://bgr.com/2015/08/13/apple-heads-up-display-prediction/"&gt;to go out on a limb&lt;/a&gt;, is predicting that’ll happen in March.&lt;/p&gt;
&lt;p&gt;Before I turn to Chowdhry’s scenario for future gloom, take a look at the table below. These are his predictions for layoffs this year, based on an upward revision of his original estimate (to 369,000 in total), along with a few big ones that he missed. Not all have been announced to date, but he still stands by his overall numbers, and notes that many companies are trying to hide layoff activity. Microsoft, he pointed out in an October report, is “letting go of 200 to 250 people every week, and none of these are announced.” And IBM has been laying off wave after wave of people this year, according to anecdotal evidence being collected by the group &lt;a shape="rect" href="https://www.facebook.com/alliancemember/?fref=ts"&gt;Watching IBM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That’s what’s going on at the big companies. But the transformation, he says, is just going to get more painful, because the layoff tsunami is about to hit startups.&lt;/p&gt;
&lt;p&gt;“When you see the large companies laying off, that is an indication that the customer base is struggling,” he says. “And the startups have the same set of enterprise customers as the bigger companies. The only thing protecting them now is that they have funding that takes them to the end of this year or the middle of next year, but by March or April it’s going to get very bloody.”&lt;/p&gt;
&lt;p&gt;They won’t be able to get more funding, he says, “because the startup companies have exhausted the number of fools. They exhausted the fools in Silicon Valley, then they exhausted the fools in New York City, in Europe, and now they’ve exhausted even the Middle East. There are no fools left.”&lt;/p&gt;
&lt;p&gt;“The bubble will burst,” he says, “and the impacts on the tech industry will last two years.”&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;A tech layoff scorecard:&lt;/strong&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;table cellpadding="0" border="1" cellspacing="0"&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;
&lt;em&gt;Company Name&lt;/em&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;
&lt;em&gt;Prediction&lt;/em&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;
&lt;em&gt;Announced to date&lt;/em&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;EMC&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;28,000 (40 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;VMWare&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;5100 (30 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;900 (5 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;HP Enterprise&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;72,000 (30 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;HP Inc&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;14,000 (30 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;IBM&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;150,000 (40 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Cisco&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;28,000 (40 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;5500 (7 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Juniper&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;3500 (40 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Oracle&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;33,000 (25 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Microsoft&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;23,000 (20 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Network Appliance&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;3200 (25 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;1500 (12 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Symantec&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;4750 (25 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;F5 Networks&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;1125 (25 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Yahoo&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;6250 (50 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;1600 (15 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Yelp&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;1800 (50 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Intel&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Not predicted&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;12,000 (11 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Cypress&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Not predicted&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;500 (8 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Autodesk&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Not predicted&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;925 (10 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Seagate&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Not predicted&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;1600 (NA)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Zenefits&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;Not predicted&lt;/p&gt;
&lt;/td&gt;
&lt;td colspan="1" style="width:118px" rowspan="1"&gt;
&lt;p&gt;250 (17 percent)&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
&lt;em&gt;Source: Trip Chowdhry&lt;/em&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 14 Oct 2016 14:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/analyst-ugly-year-for-tech-layoffs-and-its-going-to-get-worse</guid>
<media:content url="http://spectrum.ieee.org/image/MjgyMDE5Mw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgyMDE5MQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Is Silicon Valley Discriminating Against Men and Asians?</title>
<link>http://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/is-silicon-valley-discriminating-against-men-and-asians</link>
<description>The latest discrimination lawsuits against Silicon Valley companies come from members of populations well represented in the world of tech</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The latest discrimination lawsuits against Silicon Valley companies come from members of populations well represented in the world of tech&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxODc0Mg.jpeg"/&gt;
&lt;figcaption&gt;Photo: Rena Pacheco-Theard/Prepify&lt;/figcaption&gt;
&lt;figcaption&gt;Bathroom lines at Silicon Valley events are one measure of diversity, like those shown here at last month's TechCrunch Disrupt&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;We know Silicon Valley has a diversity problem. &lt;a shape="rect" href="https://www.eeoc.gov/eeoc/statistics/reports/hightech/index.cfm"&gt;According to the U.S. Equal Employment Opportunity Commissio&lt;/a&gt;n, women make up 30 percent of the Silicon Valley tech workforce; 47 percent of the tech workers are white, 41 percent Asian American, 6 percent Hispanic, and 3 percent black.&lt;/p&gt;
&lt;p&gt;So lawsuits charging discrimination at Silicon Valley companies, given the underrepresentation of some groups, aren’t unexpected. The surprise is when such charges come from well-represented groups. And that is what has happened in the past couple of weeks.&lt;/p&gt;
&lt;p&gt;At the end of September, the U.S. Department of Labor &lt;a shape="rect" href="http://www.sfgate.com/business/technology/article/Feds-accuse-Silicon-Valley-firm-of-hiring-bias-9289597.php"&gt;filed a lawsuit&lt;/a&gt; against Palo Alto’s dominant company, &lt;a shape="rect" href="https://www.palantir.com/"&gt;Palantir&lt;/a&gt;, accusing it of systematically discriminating against Asian job applicants, hiring far fewer than a general statistical analysis would suggest is reasonable. (The lawsuit referred to Asians, not Asian-Americans. However, that appears to be a matter of semantics.)&lt;/p&gt;
&lt;p&gt;And last week, a former Yahoo employee &lt;a shape="rect" href="http://www.sfgate.com/news/article/Yahoo-lawsuit-Marissa-Mayer-discrimination-men-9926263.php"&gt;filed a lawsuit&lt;/a&gt; against the company charging CEO Marissa Mayer of using the performance review system to discriminate against men by intentionally hiring and promoting women.&lt;/p&gt;
&lt;p&gt;Palantir is not one of the companies that has made its diversity statistics public, but it has long spoken of &lt;a shape="rect" href="https://www.palantir.com/diversity/"&gt;a commitment to diversity&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Yahoo reported diversity numbers in 2014 and 2015; the proportion of women in the overall workforce held steady at 37 percent, while the company increased the number of women in tech from 15 percent in &lt;a shape="rect" href="https://yahoo.tumblr.com/post/89085398949/workforce-diversity-at-yahoo"&gt;2014&lt;/a&gt; to 16 percent &lt;a shape="rect" href="https://yahoo.tumblr.com/post/123472998984/please-see-here-for-our-eeo-1-report"&gt;in 2015&lt;/a&gt;. In 2014, the company’s Chief Development Officer, Jacqueline Reses (now at Square), &lt;a shape="rect" href="https://yahoo.tumblr.com/post/89085398949/workforce-diversity-at-yahoo"&gt;wrote in a blog post&lt;/a&gt; that Yahoo is “committed to attracting, developing and retaining a diverse workforce.”&lt;/p&gt;
&lt;p&gt;Are lawsuits like these a good thing, or are they going to stall efforts to move towards more diversity in Silicon Valley?&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 14 Oct 2016 13:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/at-work/tech-careers/is-silicon-valley-discriminating-against-men-and-asians</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxODc1Mw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxODc1MQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Google's Deep Mind Gives AI a Memory Boost That Lets It Navigate London's Underground</title>
<link>http://spectrum.ieee.org/tech-talk/computing/software/googles-deep-mind-boosts-memory-to-navigate-london-underground</link>
<description>The Deep Mind AI lab is giving deep learning a memory boost</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The Deep Mind AI lab is giving deep learning a memory boost&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxOTE1Nw.jpeg"/&gt;
&lt;figcaption&gt;Photo: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Google’s DeepMind &lt;span&gt;artificial intelligence lab &lt;/span&gt;does more than just develop computer programs capable of beating the world’s best human players in the ancient game of Go. The DeepMind unit has also been working on the next generation of deep learning software that combines the ability to recognize data patterns with the memory required to decipher more complex relationships within the data.&lt;/p&gt;
&lt;p&gt;Deep learning is the latest buzz word for artificial intelligence algorithms called neural networks that can learn over time by filtering huge amounts of relevant data through many “deep” layers. The brain-inspired neural network layers consist of nodes (also known as neurons). Tech giants such as Google, Facebook, Amazon, and Microsoft have been training neural networks to learn how to better handle tasks such as recognizing images of dogs or making better Chinese-to-English translations. These AI capabilities have already benefited millions of people using &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/software/google-translate-gets-a-deep-learning-upgrade"&gt;Google Translate&lt;/a&gt; and other online services.&lt;/p&gt;
&lt;p&gt;But neural networks face huge challenges when they try to rely solely on pattern recognition without having the external memory to store and retrieve information. &lt;span&gt;To improve deep learning&lt;/span&gt;
&lt;span&gt;’s capabilities, Google DeepMind created a &lt;/span&gt;
&lt;span&gt;“differentiable neural computer&lt;/span&gt;
&lt;span&gt;” (DNC) that gives neural networks an external memory for storing information for later use.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“Neural networks are like the human brain; we humans cannot assimilate massive amounts of data and we must rely on external read-write memory all the time,” says &lt;/span&gt;
&lt;span&gt;Jay McClelland, director of the Center for Mind, Brain and Computation at &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://stanford.edu/~jlmcc/"&gt;Stanford University&lt;/a&gt;
&lt;span&gt;. “We once relied on our physical address books and Rolodexes; now of course we rely on the read-write storage capabilities of regular computers.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;McClelland is a cognitive scientist who served as one of several independent peer reviewers for the Google DeepMind paper that describes development of this improved deep learning system. The full paper is &lt;span&gt;presented in the 12 Oct 2016 issue of the &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://nature.com/articles/doi:10.1038/nature20101"&gt;journal &lt;em&gt;Nature&lt;/em&gt;
&lt;/a&gt;
&lt;span&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The DeepMind team found that the DNC system’s combination of the neural network and external memory did much better than a neural network alone in tackling the complex relationships between data points in so-called “graph tasks.” For example, they asked their system to either simply take any path between points A and B or to find the shortest travel routes based on a symbolic map of the London Underground subway.&lt;/p&gt;
&lt;p&gt;An unaided neural network could not even finish the first level of training, based on traveling between two subway stations without trying to find the shortest route. It achieved an average accuracy of just 37 percent after going through almost two million training examples. By comparison, the neural network with access to external memory in the DNC system successfully completed the entire training curriculum and reached an average of 98.8 percent accuracy on the final lesson.&lt;/p&gt;
&lt;p&gt;The external memory of the DNC system also proved critical to success in performing &lt;span&gt;logical planning tasks such as solving simple block puzzle challenges. Again, a neural network by itself could not even finish the first lesson of the training curriculum for the block puzzle challenge. The DNC system was able to use its memory to store information about the challenge’s goals and to effectively plan ahead by writing its decisions to memory before acting upon them.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;In 2014, DeepMind’s researchers developed another system, called the &lt;a shape="rect" href="https://arxiv.org/abs/1410.5401"&gt;neural Turing machine&lt;/a&gt;, that also combined neural networks with external memory. But the neural Turing machine was limited in the way it could access “memories” (information) because such memories were effectively stored and retrieved in fixed blocks or arrays. The latest DNC system can access memories in any arbitrary location, &lt;span&gt;McClelland explains.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The DNC system’s memory architecture even bears a certain resemblance to how the hippocampus region of the brain supports new brain cell growth and new connections in order to store new memories. Just as the DNC system uses the equivalent of time stamps to organize the storage and retrieval of memories, human “free recall” experiments have shown that people are more likely to recall certain items in the same order as first presented.&lt;/p&gt;
&lt;p&gt;Despite these similarities, the DNC’s design was driven by computational considerations rather than taking direct inspiration from biological brains, DeepMind’s researchers write in their paper. But McClelland says that he prefers not to think of the similarities as being purely coincidental.&lt;/p&gt;
&lt;p&gt;“&lt;span&gt;The design decisions that motivated the architects of the DNC were the same as those that structured the  human memory system, although the latter (in my opinion) was designed by a gradual evolutionary process, rather than by a group of brilliant AI researchers,” McClelland says.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Human brains still have significant advantages over any brain-inspired deep learning software. For example, human memory seems much better at storing information so that it is accessible by both context or content, McClelland says. He expressed hope that future deep learning and AI research could better capture the memory advantages of biological brains.&lt;/p&gt;
&lt;p&gt;DeepMind’s DNC system and similar neural learning systems may represent crucial steps for the ongoing development of AI. But the DNC system still falls well short of what McClelland considers the most important parts of human intelligence.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The DNC is a sophisticated form of external memory, but ultimately it is like the papyrus on which Euclid wrote the elements. The insights of mathematicians that Euclid codified relied (in my view) on a gradual learning process that structured the neural circuits in their brains so that they came to be able to see relationships that others had not seen, and that structured the neural circuits in Euclid’s brain so that he could formulate what to write.  We have a long way to go before we understand fully the algorithms the human brain uses to support these processes. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p/&gt;
&lt;p&gt;It’s unclear when or how Google might take advantage of the capabilities offered by the DNC system to boost its commercial products and services. The DeepMind team was &lt;span&gt;“heads down in research” or too busy with travel to entertain media questions at this time, according to a Google spokesperson.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;But Herbert Jaeger, professor for computational science at &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://minds.jacobs-university.de/herbert"&gt;Jacobs University Bremen&lt;/a&gt;
&lt;span&gt; in Germany, sees the DeepMind team’s work as a “passing snapshot in a fast evolution sequence of novel neural learning architectures.” In fact, he’s confident that the DeepMind team already has something better than the DNC system described in the &lt;em&gt;Nature&lt;/em&gt; paper. (Keep in mind that the paper was submitted back in January 2016.)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;DeepMind’s work is also part of a bigger trend in deep learning, Jaeger says. The leading deep learning teams at Google and other companies are racing to build new AI architectures with many different functional modules—among them, attentional control or working memory; they then train the systems through deep learning. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“The DNC is just one among dozens of novel, highly potent, and cleverly-thought-out neural learning systems that are popping up all over the place,” Jaeger says.&lt;/span&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 12 Oct 2016 17:00:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/software/googles-deep-mind-boosts-memory-to-navigate-london-underground</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxOTE3Mg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxOTE3MA.jpg" height="225" width="300"/>
</item>
<item>
<title>Nolan Bushnell Says His New Virtual Reality Startup Has the Keys to the Holodeck—and it’s Portable</title>
<link>http://spectrum.ieee.org/view-from-the-valley/computing/hardware/nolan-bushnell-says-his-new-virtual-reality-startup-has-the-keys-to-the-holodeckand-its-portable</link>
<description>Startup Modal VR says it has built the closest thing yet to the holodeck—and wants to sell it to the business world</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Startup Modal VR says it has built the closest thing yet to the holodeck—and wants to sell it to the business world&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxODc1OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Modal VR&lt;/figcaption&gt;
&lt;figcaption&gt;Modal VR's virtual reality system for commercial applications includes a wearable headset, a full-body motion tracking suit, and a computer peripheral it calls the VR Fabricator.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;I’ve been talking to a lot of academics, investors, and analysts lately to get a sense of what’s coming down the path in virtual and augmented reality.&lt;/p&gt;
&lt;p&gt;And at least for VR—expensive, immersive, full on virtual reality—they’ve been telling me to look to gear for the business world, not the consumer world, for the next big thing. That’s because VR hardware won’t be cheap enough, at least in an untethered form, for the average consumer anytime soon.&lt;/p&gt;
&lt;p&gt;So maybe &lt;a shape="rect" href="http://www.modalvr.com/"&gt;Modal VR&lt;/a&gt;, a startup company that came out of stealth today that is building VR hardware exclusively for business applications, will hit the sweet spot for the technology in 2017. Cofounder Nolan Bushnell, founder of Atari and Chuck E. Cheese, thinks so. “My past successes have always been by being at the right time, at the right place,” he said in a video statement released at the launch.&lt;/p&gt;
&lt;p&gt;The company says it will be shipping multi-user, wireless, VR systems to developers soon, but didn’t disclose a date.&lt;/p&gt;
&lt;p&gt;“For those of us who grew up on “&lt;a shape="rect" href="http://www.nasa.gov/topics/technology/features/star_trek.html"&gt;Star Trek&lt;/a&gt;,” the &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Holodeck"&gt;holodeck&lt;/a&gt; has always been the gold standard, Bushnell said in the launch video. “Modal VR is the first time that I believe we actually have the holodeck.”&lt;/p&gt;
&lt;p&gt;The portable system, the company says, consists of a computer peripheral it’s calling the VR Fabricator, a virtual reality visor, a full-body motion tracking suit, and software; users will have to bring their own Mac or PC. Each Fabricator can support up to 10 users wearing visors and tracking suits; the peripherals can also be networked to add users or expand the range behind the initial 83,000 square meters.&lt;/p&gt;
&lt;p&gt;You know this is a Bushnell effort because the featured app in the launch video is a gaming app, a virtual battle played out on what is, in the real world, a soccer field. It’s not a bad idea—I can easily see how a company makes a birthday party business out of bringing VR games to kids, competing with permanent laser tag venues, without requiring the purchase of real estate.&lt;/p&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://www.modalvr.com/software/"&gt;Other apps suggested&lt;/a&gt; on the website include real estate, allowing walkthroughs of multiple homes from a realtor’s office, perhaps; emergency response training; immersive exhibits in museums; and virtual field trips and simulations for students. “I want to have students be able to walk through the human body, to walk amongst the planets,” says Bushnell.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 12 Oct 2016 12:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/computing/hardware/nolan-bushnell-says-his-new-virtual-reality-startup-has-the-keys-to-the-holodeckand-its-portable</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxODc3MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxODc2OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Doctors 1, Diagnosis Apps 0</title>
<link>http://spectrum.ieee.org/the-human-os/biomedical/diagnostics/doc-1-apps-0</link>
<description>Doctors outperform online apps at diagnosing symptoms</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Doctors outperform online apps at diagnosing symptoms&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxODIxMA.jpeg"/&gt;
&lt;figcaption&gt;Illustration: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;In the battle of who can correctly diagnose disease better, it seems doctors still have online apps beat. A group of self-diagnosing apps got many less diagnoses correct than doctors in new research published today in &lt;em&gt;JAMA Internal Medicine&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;About one in three U.S. adults have visited an online site to check their symptoms, &lt;a shape="rect" href="https://www.benton.org/node/143320"&gt;according to 2013 research&lt;/a&gt; by the Benton Foundation. Apps and websites “are very commonly used by the average person on the street,” says Ateev Mehrotra, a physician at Harvard Medical School.&lt;/p&gt;
&lt;p&gt;In &lt;a shape="rect" href="http://www.bmj.com/content/351/bmj.h3480"&gt;2015 research&lt;/a&gt; published in &lt;em&gt;
&lt;a shape="rect" href="http://www.bmj.com/about-bmj"&gt;The BMJ&lt;/a&gt; (&lt;/em&gt;formerly the&lt;em&gt; British Medical Journal)&lt;/em&gt;, Mehrotra and his team fed 23 symptom-checkers with symptoms from 45 standard patient cases inlcuding those later diagnosed with asthma and malaria. The team found that the checkers listed the correct diagnosis about a third of the time.&lt;/p&gt;
&lt;p&gt;In the new experiment, the researchers compared the checkers’ accuracy to the accuracy of 234 medical physicians, fellows, and residents. For each case, at least 20 doctors provided an online platform with their top three diagnoses.&lt;/p&gt;
&lt;p&gt;The physicians listed the correct diagnosis first about 72 percent of the time, compared to the apps, which listed the correct diagnosis first 34 percent of the time.&lt;/p&gt;
&lt;p&gt;“Physicians are by no means perfect,” Mehrotra says. They can still get diagnoses wrong about 10 to 15 percent of the time.&lt;/p&gt;
&lt;p&gt;However, he isn’t surprised by the performance of the self-diagnosing tools. “I wasn’t expecting them to surpass,” he says.&lt;/p&gt;
&lt;p&gt;Others raise questions.&lt;/p&gt;
&lt;p&gt;“There are major methodologic problems with the approach that was used,” &lt;span&gt;Mark Graber, a fellow at Research Triangle Institute International who researches health care quality and diagnostics and was not involved in the study, writes in an email. “The physicians in the study had essentially as much time as they wanted to evaluate and research each case; this is hardly comparable to actual practice. Its likely that the specific methods used overestimated the physician performance and underestimated ‘symptom checker’ performance.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Mehrotra says, “We tried to make it as realistic as possible but I think those criticisms are fair.” &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;He says timing isn’t an issue for computer programs, but people should be careful interpreting the results for real life because the doctors were not making decisions under pressure. They were also not able to examine patients.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;He also admits that there may have been some tweaks in the online sites since when the study was run and today, but he doesn’t believe that these minor site updates would have had a large effect on the outcome. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;He believes it’s more realistic for apps to help doctors diagnose, instead of simply replacing them. That’s one goal of systems such as &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/diagnostics/ibms-dr-watson-will-see-yousomeday"&gt;IBM’s Watson AI&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“I like the idea, we’re just not there yet,” he says.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This blog post&lt;/em&gt;
&lt;em&gt; was updated on 11 October 2016.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 10 Oct 2016 17:06:00 GMT</pubDate>
<dc:creator>Andrew Silver</dc:creator>
<guid>http://spectrum.ieee.org/the-human-os/biomedical/diagnostics/doc-1-apps-0</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxODIyMg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxODIyMA.jpg" height="225" width="300"/>
</item>
<item>
<title>Is Your Big Data Project a “Weapon of Math Destruction”?</title>
<link>http://spectrum.ieee.org/tech-talk/computing/software/are-you-making-a-weapon-of-math-destruction</link>
<description>Six key takeaways from a book about using your big data superpowers for good</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Six key takeaways from a book about using your big data superpowers for good&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNzU3Ng.jpeg"/&gt;
&lt;figcaption&gt;Photo: Crown Publishers&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;For those of us who make a living solving problems, the current deluge of big data might seem like a wonderland. Data scientists and programmers can now draw on reams of human data—and apply them—in ways that would have been unthinkable only a decade ago. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;
&lt;span&gt;But amid all the excitement, we’re beginning to &lt;/span&gt;
&lt;a style="text-decoration:none;" shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/software/computer-scientists-find-bias-in-algorithms"&gt;
&lt;span&gt;see hints&lt;/span&gt;
&lt;/a&gt;
&lt;span&gt; that our nice, tidy algorithms and predictive models might be prone to the same shortcomings that the humans who create them are. Take, for example, the revelation that Google &lt;a shape="rect" href="http://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study"&gt;disproportionately served ads&lt;/a&gt; for high-paying jobs to men rather than women. And there’s the troubling recent discovery that a &lt;/span&gt;
&lt;a style="text-decoration:none;" shape="rect" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"&gt;
&lt;span&gt;criminal risk assessment score&lt;/span&gt;
&lt;/a&gt;
&lt;span&gt; disproportionately flagged many African Americans as higher risk, sometimes resulting in longer prison sentences.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;aside class="inlay pullquote lt med-lrg"&gt;One of the things that makes big data so attractive is the assumption that it’s eliminating human subjectivity and bias. After all, you’re basing everything on hard numbers from the real world, right? Wrong. Predictive models and algorithms, says author Cathy O’Neil, are really just “opinions embedded in math.” &lt;/aside&gt;
&lt;p/&gt;
&lt;span&gt;Mathematician and data scientist Cathy O’Neil has a name for these wide-reaching and discriminatory models: Weapons of Math Destruction. In her &lt;/span&gt;
&lt;a shape="rect" href="http://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815"&gt;new book by the same name&lt;/a&gt;
&lt;span&gt;, she details the ways that algorithms often perpetuate or even worsen inequality and injustice.&lt;/span&gt;
&lt;br clear="none"/&gt;
 
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;We spoke to O’Neil last week during a Facebook Live session to find out how programmers and data scientists can ensure that their models do more good than harm. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;iframe frameborder="0" height="400" scrolling="no" allowfullscreen="true" width="400" allowtransparency="true" src="https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2FIEEE.Spectrum%2Fvideos%2F10154556908936182%2F&amp;amp;show_text=0&amp;amp;width=400"/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span&gt;
&lt;span&gt;Here are a few key takeaways:&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;strong&gt;1. Recognize the Signs of a “WMD”&lt;/strong&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;They’re Important &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;A signature of a Weapon of Math Destruction is that it’s used to determine some critical element in the lives of many people. We’re already using algorithms to sort resumes for job openings, automatically schedule shifts for service industry workers, decide the price of insurance or interest rates on a loan, or even to help determine how long a person will spend in jail when convicted of a crime. Because these algorithms affect crucial outcomes for millions of people, they have the potential to do widespread damage.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;They’re Secret or Unaccountable &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;The people most affected by WMD’s often don’t understand the rubric by which they’re being scored, or even that they’re being scored in the first place. The methodology behind them is often a “&lt;/span&gt;
&lt;a style="text-decoration:none;" shape="rect" href="http://www.chicagoip.com/pasquale.pdf"&gt;
&lt;span&gt;trade secret&lt;/span&gt;
&lt;/a&gt;
&lt;span&gt;,” protecting it from public scrutiny. While many companies argue that this keeps people from learning the rules and gaming the system, the lack of transparency also means there’s no way to check whether the score is actually fair. Machine learning algorithms take this one step further; while they’re powerful tools for finding correlations, they’re also often black boxes, even to the people who create them.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;They’re Destructive&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;Weapons of Math Destruction have a way of creating their own reality and then using that reality to justify their model, says O’Neil. An algorithm that, say, targets financially vulnerable people for predatory loans creates a feedback loop, making it even harder for them to get out of debt. Similarly, a model that labels a first-time drug offender as higher-risk because he comes from a high-crime neighborhood potentially makes that problem even worse. If his high risk score results in a longer jail sentence, he’ll have fewer connections to his community and fewer job prospects once he’s released. His score becomes a self-fulfilling prophecy, actually putting him at a greater risk of reoffending.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;strong&gt;2. Realize There Is No Such Thing as an “Objective Algorithm”&lt;/strong&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;One of the things that makes big data so attractive is the assumption that it’s eliminating human subjectivity and bias. After all, you’re basing everything on hard numbers from the real world, right? Wrong. Predictive models and algorithms, says O’Neil, are really just “opinions embedded in math.” Algorithms are written by human beings with an agenda. The very act of defining what a successful algorithm looks like is a value judgement; and what counts as success for the builders of the algorithm (frequently profit, savings, or efficiency) is not always good for society at large. Because of this, O’Neil says, it’s important for data scientists to look at the bigger picture. Who are the winners in my algorithm—and even more importantly—&lt;/span&gt;
&lt;em&gt;what happens to the losers&lt;/em&gt;
&lt;span&gt;?&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;strong&gt;3. Pay Attention to the Data You’re Using&lt;/strong&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;There’s another reason that algorithms aren’t as trustworthy as we might think: The data they draw on often comes from a world that’s deeply prejudiced and unequal. Crime statistics might seem objective—that is, until you realize that, for example, the mechanisms of the U.S. criminal justice system have been applied unfairly to target minorities throughout its entire history. That bias shows up in crime data. Researchers know that black and white people use marijuana at almost identical rates, but black teenagers are much more likely to be arrested for marijuana possession. The disparity in the numbers has much more to do with systemic racial profiling and a ramped up police presence in historically black neighborhoods than it does with actual levels of criminality. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;We’ve made the decision as a society to stamp out discrimination based on race, gender, sexual orientation, or disability status—and fortunately, most data scientists know to be very careful when using these attributes to categorize people or model behavior. But data from the real world is often fraught with less-obvious proxy variables that are essentially stand-ins for those characteristics. Zip codes, for example, are an easy proxy for race, thanks to decades of the discriminatory housing practice called &lt;a shape="rect" href="http://www.theatlantic.com/business/archive/2014/05/the-racist-housing-policy-that-made-your-neighborhood/371439/"&gt;redlining&lt;/a&gt;.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;strong&gt;4. Get Honest About What You’re Really Modeling&lt;/strong&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;Human behavior is messy, which often means that direct measurements of the attributes we’re trying to model (like criminality, trustworthiness, or fitness for a job) don’t actually exist. Because of this, data scientists often rely on other variables they believe might correlate with what they’re trying to measure.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;Car insurance companies, for example, use credit scores as a way to determine how reliable a driver is. At first glance it sounds reasonable to assume that a person who regularly pays her bills on time might be more conscientious or responsible. But strangely, &lt;a shape="rect" href="http://www.consumerreports.org/cro/car-insurance/auto-insurance-special-report/index.htm"&gt;Consumer Reports&lt;/a&gt; recently discovered that people with low credit scores and clean driving records were being charged much more for car insurance that people with high credit scores &lt;/span&gt;
&lt;em&gt;and DUIs on their driving records&lt;/em&gt;
&lt;span&gt;. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;This, of course, is nonsense. Having a previous DUI is a much better indicator of a driver’s likelihood of getting into an accident. But O’Neil asserts that there might be a hidden reason the insurance companies continue to incorporate credit score into their models: it’s a direct measurement of financial vulnerability. Drivers with low credit scores don’t have as much leverage to shop around for lower rates, and a person who’s desperate for insurance is often willing to pay much more to get it. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;strong&gt;5. Examine and Systematically Test Your Assumptions&lt;/strong&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;Even well-intentioned algorithms can have flawed assumptions built in. For example, &lt;/span&gt;
&lt;/span&gt;
&lt;span&gt;the recidivism risk score mentioned earlier is an attempt to make communities safer &lt;span&gt;by locking up potentially violent repeat offenders and releasing those who are deemed a lower risk. Other intended benefits would be reducing the prison population and making the justice system more fair. But once we lock people away, says O’Neil, we treat prisons as a black box and stop asking questions.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;Online giants like Amazon.com take the opposite approach; learning and experimentation are built into their business model. Amazon has a dedicated data laboratory where researchers constantly reexamine every aspect of the consumer experience, finding places along the pipeline where customers get confused or frustrated, or can’t find what they need. This feedback allows Amazon to continuously learn and tweak its online environment to maximize profit.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;If we truly wanted to optimize our criminal justice system for community safety, says O’Neil, we’d continuously be running controlled experiments: Does putting someone behind bars with other criminals make them more or less likely to commit a crime upon release? How beneficial are general-equivalency (alternative high school) diploma programs? What is the effect of solitary confinement? Of sexual abuse? How much does it cost to treat someone for a mental disorder, versus repeatedly locking him away? &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;br clear="none"/&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;strong&gt;6. Take The Modelers’ Hippocratic Oath:&lt;/strong&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;Eventually we’ll need laws and industry standards that can keep pace with this technology and require a level of transparency from companies about how they’re using data. It might even require mandatory fairness audits of important algorithms. But in the meantime, a disproportionate amount of the responsibility falls to programmers. Awareness of the issue is a crucial first step. A good way to start is by taking this pledge, &lt;a shape="rect" href="http://www.wilmott.com/archives/707"&gt;originally written by Emanuel Derman and Paul Wilmott&lt;/a&gt; in the wake of the 2008 financial crisis:&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
 

&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;∼ I will remember that I didn’t make the world, and it doesn’t satisfy my equations. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;∼ Though I will use models boldly to estimate value, I will not be overly impressed by mathematics. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;∼ I will never sacrifice reality for elegance without explaining why I have done so. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;∼ Nor will I give the people who use my model false comfort about its accuracy. Instead, I will make explicit its assumptions and oversights. &lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-ad4f3cb0-9621-df2e-d3c2-2b6a2de9a7cd"&gt;
&lt;span&gt;∼&lt;/span&gt;
&lt;span&gt; I understand that my work may have enormous effects on society and the economy, many of them beyond my comprehension.&lt;/span&gt;
&lt;/span&gt;
&lt;br clear="none"/&gt;
 &lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 5 Oct 2016 19:30:00 GMT</pubDate>
<dc:creator>Kristen Clark</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/software/are-you-making-a-weapon-of-math-destruction</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNzU2NQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNzU2Mw.jpg" height="225" width="300"/>
</item>
<item>
<title>Machine Learning Tools Help Google Science Fair Finalists Find Lost Objects, Predict Breast Cancer Risk</title>
<link>http://spectrum.ieee.org/view-from-the-valley/computing/software/google-science-fair-finalists-turn-to-machine-learning-to-predict-breast-cancer-risk-find-lost-objects</link>
<description>Can a digital mammogram reveal the future? Can a wearable retrace your past on demand?</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Can a digital mammogram reveal the future? Can a wearable retrace your past on demand?&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNTI3Mg.jpeg"/&gt;
&lt;figcaption&gt;Photo: Tekla Perry&lt;/figcaption&gt;
&lt;figcaption&gt;Anika Cheerla's submission to the Google Science Fair used machine learning to improve the accuracy of breast cancer risk prediction&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;This week, 16 teams of teens from around the world assembled in Mountain View to demonstrate the results of research projects at the Google Science Fair. You can view summaries of all the projects &lt;a shape="rect" href="https://www.googlesciencefair.com/en/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve been attending these finals for several years now and am always impressed with how creatively the teens use the technologies of today. And this year was no exception: &lt;a shape="rect" href="http://www.forbes.com/sites/toddhixon/2016/08/15/how-to-get-a-piece-of-the-boom-in-machine-learning-start-ups/#4296f48f432a"&gt;machine learning is hot&lt;/a&gt; in the tech world, and the teens are embracing it.&lt;/p&gt;
&lt;p&gt;Consider 14-year-old &lt;a shape="rect" href="https://www.googlesciencefair.com/projects/en/2016/ca4846a39226eb1c957776a7002f69b90cdaf8635c3346bd06c7d1b4247fa78b"&gt;Anika Cheerla’s submission&lt;/a&gt;. A Silicon Valley girl from Cupertino, Calif., Cheerla was curious about the current state of breast cancer prediction, and discovered that prediction methods using digital mammograms are just 64 percent effective, typically simply considering the percentage of dense tissue in a breast. She developed software that considers a broader range of features, including dense and non-dense regions, and, using a database of digital mammograms from Stanford University, built and began training classifiers to use in predicting risk. She discovered that the area closest to the nipple has the highest predictive power, and her system can take that into account. Right now her system is about 84 percent effective. She is hoping to improve her system by training it on more images and adding additional machine learning capabilities.&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxNTI3MQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Credit&lt;/figcaption&gt;
&lt;figcaption&gt;Google Science Fair finalist Shriank Kanapurti went through several iterations of his system designed to keep track of objects around you—and help you find your lost keys. He tested the system on his grandfather.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Shriank Kanapurti, a 16- year-old from Bangalore, India, turned to machine learning to help the forgetful find misplaced objects. His approach, called &lt;a shape="rect" href="https://www.googlesciencefair.com/projects/en/2016/f15b3ff87cbfe5e117f6d15ae7276ec7b0fea3cd7f27bbadb1dc57abb8bc77d8"&gt;KeepTab&lt;/a&gt;, involves a wearable camera constantly recording images of what’s in front of you, and designed software that extracts the objects from the images and figures out what they are in relation to other objects in your environment. To date he says he has trained the software on 600,000 images. He uses Google-Now’s natural language software to communicate with his system—you can say “Locate my keys,” and it will respond, “Your keys are on the television”. He’d like to eventually see his software run with less obtrusive wearables, like a future version of Google Glass.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 5 Oct 2016 13:37:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/computing/software/google-science-fair-finalists-turn-to-machine-learning-to-predict-breast-cancer-risk-find-lost-objects</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNTI4NA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNTI4Mg.jpg" height="225" width="300"/>
</item>
<item>
<title>Fujitsu Memory Tech Speeds Up Deep-Learning AI</title>
<link>http://spectrum.ieee.org/tech-talk/computing/software/fujitsu-memory-tech-speeds-up-deep-learning-ai</link>
<description>Fujitsu's memory tech squeezes more efficiency from deep learning algorithms running on GPUs</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Fujitsu's memory tech squeezes more efficiency from deep learning algorithms running on GPUs&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNTkxNA.jpeg"/&gt;
&lt;figcaption&gt;Photo: Nvidia&lt;/figcaption&gt;
&lt;figcaption&gt;The Nvidia Titan X is one of the latest examples of GPU chips used in deep learning.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Artificial intelligence driven by deep learning often runs on many computer chips working together in parallel. But the deep-learning algorithms, called neural networks, can run only so fast in this parallel computing setup because of the limited speed with which data flows between the different chips. The Japan-based multinational Fujitsu has come up with a novel solution that sidesteps this limitation by enabling larger neural networks to exist on a single chip.&lt;/p&gt;
&lt;p&gt;The neural networks used in deep learning typically run on graphics processing units (GPUs) that originated as components for generating and displaying images. By creating an efficiency shortcut in the calculations performed by neural networks, Fujitsu researchers reduced the amount of internal GPU memory used by 40 percent. Their solution allows for a larger and potentially more capable neural network to run on a single GPU.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“To the best of our knowledge, we are the first to propose this type of solution,” says &lt;/span&gt;Yasumoto Tomita, research manager of the Next-Generation Computer Systems Project at Fujitsu Laboratories Ltd.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Fujitsu’s memory efficiency technology was announced at the &lt;a shape="rect" href="https://signalprocessingsociety.org/blog/2016-ieee-international-workshop-machine-learning-signal-processing-mlsp-2016"&gt;2016 IEEE International Workshop on Machine Learning for Signal Processing&lt;/a&gt;, an international conference held in Salerno, Italy, from 13 to 16 September.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Understanding Fujitsu’s solution requires a bit of a deep dive into deep learning. The neural networks involved in deep learning train on huge amounts of data in order to perform tasks such as distinguishing between faces or translating between languages. Such networks consist of many layers of artificial neurons that are connected to each other and can influence one another.&lt;/p&gt;
&lt;p&gt;As training data flows through a neural network, each layer of neurons performs calculations that influence the next layer of neurons and, ultimately, the final result. “Weighted data” represents the connection strength between input and output neurons. The neurons themselves handle “intermediate data” that represent the data input to and output from each layer. A typical neural network must calculate error data for both the weighted data and intermediate data.&lt;/p&gt;
&lt;p&gt;All those calculations need to be stored by the internal memory of the GPU chips running the neural networks. Fujitsu figured out how to reuse certain parts of the GPU’s limited memory by calculating intermediate error data from weighted data and also generating weighted error data from intermediate data—all done independently but at the same time.&lt;/p&gt;
&lt;p&gt;The 40 percent reduction in memory usage effectively allows a larger neural network with “roughly two times more layers or neurons” to operate on a single GPU, Tomita says. The greater neural network capability on a single GPU helps avoid some of the performance bottleneck that comes up when neural networks spread across many GPUs must exchange data during training. (The traditional method of spreading neural networks across many GPUs is an example of what is called “model parallelization.”)&lt;/p&gt;
&lt;p&gt;Fujitsu has also been &lt;a shape="rect" href="http://www.fujitsu.com/global/about/resources/news/press-releases/2016/0809-01.html"&gt;developing software technology&lt;/a&gt; that can speed up the sharing of data across multiple GPUs. Those software innovations may combine with this memory-efficiency tech to provide a significant boost to the company’s deep-learning ambitions.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“By combining the memory-efficiency technology developed by Fujitsu with GPU parallelization technology, fast learning on large-scale networks becomes possible, without model parallelization,” Tomita says.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;By March 2017, Fujitsu aims to commercialize this memory-efficient deep learning within its consulting service, which goes by the name &lt;a shape="rect" href="http://www.fujitsu.com/global/about/resources/news/press-releases/2015/1102-01.html"&gt;Human Centric AI Zinrai&lt;/a&gt;. That would allow Fujitsu to join the many other tech giants and startups trying to &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/now-you-too-can-buy-cloudbased-deep-learning"&gt;translate deep learning into online services&lt;/a&gt; for businesses that lack the expertise.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 4 Oct 2016 14:30:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/software/fujitsu-memory-tech-speeds-up-deep-learning-ai</guid>
</item>
<item>
<title>Autonomous Vehicles Need In-Cabin Cameras to Monitor Drivers</title>
<link>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/autonomous-vehicles-need-incabin-cameras-to-monitor-drivers</link>
<description>Self-driving cars require driver-monitoring capability to know when it is safe to hand over control</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Self-driving cars require driver-monitoring capability to know when it is safe to hand over control&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNjI1MA.jpeg"/&gt;
&lt;figcaption&gt;Image: EDGE3 Technologies&lt;/figcaption&gt;
&lt;figcaption&gt;Self-driving cars should adopt cameras and other technologies to monitor drivers' attention, behaviors, and cognitive load to know when it is safe to hand over control.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;When Elon Musk unveiled his “&lt;a shape="rect" href="https://www.tesla.com/blog/master-plan-part-deux"&gt;Master Plan&lt;/a&gt;” for Tesla on the company’s blog, he argued for the electric car’s &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/tesla-autopilot-crash-why-we-should-worry-about-a-single-death"&gt;controversial&lt;/a&gt; Autopilot mode in stark ethical terms. It would be “morally reprehensible,” he said, to scale back or disable Tesla’s partially autonomous driving feature because, on balance, Autopilot still &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/teslas-autopilot-may-have-saved-a-life-cars-that-think"&gt;saves lives&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;There is no doubt that Autopilot and other similar driver-assistance technologies improve safety. But as CEO of &lt;a shape="rect" href="http://www.edge3technologies.com"&gt;EDGE3 Technologies&lt;/a&gt;, a vision company developing driver-monitoring systems for vehicles, and as a former professor and head of the Machine Vision Lab at &lt;a shape="rect" href="https://erau.edu/"&gt;Embry-Riddle Aeronautical University&lt;/a&gt;, my experience suggests something else too. Namely, in the rush to achieve fully &lt;a shape="rect" href="http://spectrum.ieee.org/transportation/self-driving"&gt;autonomous driving&lt;/a&gt;, we may be side-stepping the proper technology development path and overlooking essential technologies needed to help us get there.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Tesla’s &lt;a shape="rect" href="http://spectrum.ieee.org/tag/Autopilot"&gt;Autopilot&lt;/a&gt;, although a great pioneering effort, is in fact a driver-assist feature, and not quite the fully autonomous capability we all dream of. In technical terms, it is a &lt;a shape="rect" href="http://www.nhtsa.gov/About+NHTSA/Press+Releases/U.S.+Department+of+Transportation+Releases+Policy+on+Automated+Vehicle+Development"&gt;NHTSA-Level 2&lt;/a&gt; autopilot system, defined as “automation of at least two primary control functions.” Such systems require you to keep your hands on the steering wheel &lt;em&gt;at all times&lt;/em&gt;. In August, Tesla removed the Chinese words “autopilot” and “self-driving” from its China website—on the heels of an accident in Beijing in which the driver alleged Tesla misrepresented its cars’ capabilities.&lt;/p&gt;
&lt;p&gt;There is a clear disconnect between drivers’ expectations and what the reality of Autopilot today looks like. The updated &lt;a shape="rect" href="http://abcnews.go.com/Business/wireStory/tesla-update-halts-automatic-steering-driver-inattentive-42280290"&gt;“hands on the wheel”&lt;/a&gt; requirement and notification that Tesla recently released does not equate to eyes on the road. Many Tesla and other car owners may wander off visually or mentally, even with their hands on the wheel. They instead hope or believe that their Autopilot enables limited self-driving (Level 3), or even full autonomous driving (Level 4), when in reality they are driving a Level 2 system.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Vehicle owners can surely be forgiven for not knowing the level of automation that their vehicles are equipped with. Since it is possible to let a vehicle drive itself on a highway for hundreds of miles, it is easy to understand how a driver may be lulled into a false sense of security believing the car is fully autonomous. This is problematic. We are starting to see the results with &lt;a shape="rect" href="http://www.reuters.com/article/us-tesla-germany-crash-idUSKCN11Z2IW"&gt;fender benders&lt;/a&gt;, and at least one &lt;a shape="rect" href="https://www.tesla.com/blog/tragic-loss"&gt;deadly crash&lt;/a&gt; earlier this year caused when the driver was not actively engaged in watching the road. &lt;span&gt;Joshua Brown’s Tesla was &lt;/span&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/fatal-tesla-autopilot-crash-reminds-us-that-robots-arent-perfect"&gt;traveling down the freeway&lt;/a&gt;
&lt;span&gt; in Autopilot mode with no way for him to be alerted as to how &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/what-next-for-teslas-autopilot"&gt;dangerous the situation was&lt;/a&gt;. It was arguably an avoidable tragedy, had the vehicle known that Brown was watching a movie on a DVD player, as some reports have suggested.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;So how do we get to the next stage of automation—that is, limited self-driving, or &lt;span&gt;Level 3 automation&lt;/span&gt;?&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;NHTSA instructs that, for Level 3, vehicles have to be intelligently aware of their surroundings, understand when a problem is going to occur, and know when/how to cede control of the vehicle to the driver. Cars are certainly getting better at seeing and understanding everything around them, but they are still blind to the one factor that a limited self-driving system needs the most in order to know when/how to cede control: the driver. &lt;/p&gt;
&lt;aside class="inlay pullquote xlrg"&gt;Cars are certainly getting better at seeing and understanding everything around them, but they are still blind to the one factor that a limited self-driving system needs the most in order to know when/how to cede control: the driver. &lt;/aside&gt;
&lt;p/&gt;
&lt;p&gt;What is missing is technology inside the cockpit that not only ensures a driver is available, but also that the vehicle is aware of the cognitive state of the driver and his or her ability to take control of the vehicle. The only way to enable such a feature is through the use of cameras, pointing inward at the driver.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Measuring the driver’s state of awareness can be very nuanced and often challenging. It is not just about monitoring where the driver is looking at a given point in time, but also about identifying and measuring the driver’s state of awareness, or cognitive load. Systems, currently under development by various automotive equipment manufacturers—Tier 1 and Tier 2 suppliers, including EDGE3—use a variety of hardware technologies and designs, but share one commonality: one or more in-cabin cameras that are monitoring drivers and where they are looking.&lt;/p&gt;
&lt;p&gt;Images are then processed by on-board embedded processors to extract critical information for a driver-handoff. A dominant design for driver monitoring, to be integrated to Level-3 autonomous driving, has yet to emerge, with a variety of techniques vying for production, including stereo cameras and time-of-flight cameras. Most solutions that will be first to market seem to focus solely on tracking the eyes, as &lt;a shape="rect" href="http://news.harman.com/releases/harman-demonstrates-industrys-first-pupil-based-driver-monitoring-system-at-ces-2016"&gt;Harman&lt;/a&gt; and Delphi showcased at CES earlier this year.&lt;/p&gt;
&lt;p&gt;Although monitoring the eyes is useful, zooming out to monitor the entire face, head position, and facial expressions, as well as layering in telematics data, would yield the most accurate assessment of a driver’s state of awareness and overall cognitive load. Driver monitoring has to combine visual input from the in-cabin camera(s) with input from the car’s telematics and advanced driver-assistance system (&lt;a shape="rect" href="https://en.wikipedia.org/wiki/Advanced_driver_assistance_systems"&gt;ADAS&lt;/a&gt;) to determine an overall cognitive load on the driver. Level 3 (limited self-driving) cars of the future will learn about an individual’s driving behaviors, patterns, and unique characteristics. With a baseline of knowledge, the vehicle can then identify abnormal behaviors and equate them to various dangerous events, stressors, or distractions. Driver monitoring isn’t simply about a vision system, but is rather an advanced multi-sensor learning system. &lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Monitoring the driver is not a trivial task. To date, it has been a crucial area of focus that has been overlooked by the industry. For instance, Google, in its effort to automate driving, has decided to remove the driver from the decision loop altogether, essentially jumping over &lt;span&gt;Level 3&lt;/span&gt; limited self-driving, believing it safer to do so. Setting aside the social implications of no longer having a steering wheel, skipping Level 3 automation might actually leave us in Level 2 limbo for a long time, because, as we are learning, error-free autonomous driving might well be somewhat of a red herring, or as one expert &lt;a shape="rect" href="http://www.wsj.com/articles/self-driving-hype-doesnt-reflect-reality-1474821801?tesla=y"&gt;argued&lt;/a&gt; in a recent &lt;em&gt;Wall Street Journal&lt;/em&gt; article, still another 15 to 20 years away.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Even the most sophisticated self-driving technologies have yet to prove themselves under complex circumstances. Looking at Google’s &lt;a shape="rect" href="https://www.dmv.ca.gov/portal/wcm/connect/dff67186-70dd-4042-bc8c-d7b2a9904665/GoogleDisengagementReport2014-15.pdf?MOD=AJPERES"&gt;filings with the California Motor Vehicle Division&lt;/a&gt; (PDF) reveals that their vehicles have failed hundreds of times—and human intervention was needed repeatedly. In fact, under one filing, it seems as if human drivers initiated taking over control of the vehicles thousands of times over a span of 400,000 miles. Self-driving vehicles will only improve and become more aware of their surroundings over time, but they will still get into many everyday circumstances that would perplex even the most advanced systems that are going to be deployed in the near future.&lt;/p&gt;
&lt;p&gt;This is the nature of technology. If the vehicle’s ADAS cannot resolve a potential hazard on the road ahead, or if the vehicle cannot recognize a police officer directing traffic, the driver must be expected to take over. Fully &lt;a shape="rect" href="http://spectrum.ieee.org/transportation/self-driving/can-you-program-ethics-into-a-selfdriving-car"&gt;autonomous vehicles raise other ethical dilemmas&lt;/a&gt;. For instance, who is to blame in the event of a crash? What decisions would an autonomous vehicle make when faced with the choice of saving a driver’s life or saving someone else’s life? These decisions do not just involve technology. They involve a more &lt;a shape="rect" href="http://spectrum.ieee.org/static/special-report-trusting-robots"&gt;ethical understanding&lt;/a&gt; and meaning of the impact that such technologies have at a societal level, and will surely involve policy makers, ethicists, and others.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;a shape="rect" href="https://newsroom.uber.com/pittsburgh-self-driving-uber/"&gt;Uber&lt;/a&gt; recently launched a similar effort to Google’s, with an “autonomous” fleet in &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/uber-will-start-driverless-service-in-pittsburghthis-month"&gt;Pittsburgh&lt;/a&gt;. Yet, in spite of Uber’s claims, they made sure that a professional driver is behind the wheel, and that the drivers take control of these vehicles any time a challenging circumstance arises, which is almost every ride.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;So, clearly, Uber knows that they are not at fully autonomous driving, and they’re mitigating risk by adding a driver. What happens, however, if the driver zones out (as studies show to happen quite often), or the driver’s intervention is too late? Unfortunately, Uber’s fleet is not actively monitoring the drivers to know how or when to hand off control of the vehicle. There is a deep flaw in Uber’s design—a missing link with the driver.&lt;/p&gt;
&lt;aside class="inlay pullquote xlrg"&gt;Uber knows that its cars are not at fully autonomous driving, and they’re mitigating risk by adding a driver. What happens, however, if the driver zones out, or the driver’s intervention is too late? . . . There is a deep flaw in Uber’s design—a missing link with the driver.&lt;/aside&gt;
&lt;p/&gt;
&lt;p&gt;What is essential to the outcome in all of these scenarios is the vehicle’s knowledge of the driver’s cognitive capacity, at any given moment, and their ability at handling driving responsibilities. If the driver is unable to take over some control of the vehicle, when necessary, other contingencies need to be pursued, especially if the window for any decision is in seconds or fractions of a second.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Having a deep understanding into where a driver’s attention is will be a crucial step in getting us to Level 3, the next level of vehicle autonomy. Enabling the vehicle to become intelligently aware of the degree of associated risk will allow it to make better decisions on when and how to hand off control to the driver. Level 3, in my opinion, is where we will be for quite some time before true and fully autonomous Level 4 driving is in place. If so, we need to put the technologies in place to embrace Level 3 driving as the reality of the road ahead.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
&lt;span&gt;Tarek El Dokor is the founder and CEO of &lt;/span&gt;
&lt;a shape="rect" href="http://www.edge3technologies.com"&gt;EDGE3 Technologies&lt;/a&gt;
&lt;span&gt; in Phoenix, Ariz.&lt;/span&gt;
&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 4 Oct 2016 13:00:00 GMT</pubDate>
<dc:creator>Tarek El Dokor</dc:creator>
<guid>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/autonomous-vehicles-need-incabin-cameras-to-monitor-drivers</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNjI2NQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNjI2Mw.jpg" height="225" width="300"/>
</item>
<item>
<title>No Test Driver? No Problem: California May Make Testing Self-Driving Cars Easier</title>
<link>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/california-may-be-making-testing-selfdriving-cars-easier</link>
<description>A revised draft of regulations proposes doing away with drivers during tests</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;A revised draft of regulations proposes doing away with drivers during tests&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNjM3Nw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Kim Kulish/Corbis/Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Self-driving cars without steering wheels or pedals might have gotten a little bit closer to reality late last week. On Friday, the California Department of Motor Vehicles &lt;a shape="rect" href="https://www.dmv.ca.gov/portal/wcm/connect/211897ae-c58a-4f28-a2b7-03cbe213e51d/avexpressterms_93016.pdf?MOD=AJPERES"&gt;released a revised draft of regulations&lt;/a&gt; that could give more flexibility to autonomous car manufacturers than they have today. The proposed regulations allow testing driverless cars that pass a federal safety inspection, even with no driver in the car.&lt;/p&gt;
&lt;p&gt;At the moment, states with self-driving car regulation normally require the presence of drivers. In California, 15 companies have permits to test vehicles as long as there is a licensed driver along for the ride, &lt;a shape="rect" href="http://www.reuters.com/article/us-california-selfdriving-idUSKCN1212W6"&gt;according to Reuters&lt;/a&gt;. On Thursday, &lt;a shape="rect" href="https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=201520160AB1592"&gt;Gov. Jerry Brown signed into law&lt;/a&gt; a bill for testing vehicles without steering wheels or pedals, but the pilot program only applies to vehicles traveling below 56 kph (35 mph) in a privately-owned business park.&lt;/p&gt;
&lt;p&gt;In December, &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/google-reported-to-be-setting-up-standalone-robocar-ridesharing-service"&gt;a year after a deadline&lt;/a&gt; set by state legislature, the California DMV &lt;a shape="rect" href="https://www.dmv.ca.gov/portal/dmv/detail/pubs/newsrel/newsrel15/2015_63"&gt;released a first draft of regulation&lt;/a&gt; that would always require a driver’s presence at the steering wheel. Friday’s revision is a shift—allowing autonomous cars that pass &lt;a shape="rect" href="https://www.transportation.gov/sites/dot.gov/files/docs/AV%20policy%20guidance%20PDF.pdf"&gt;a 15-point safety inspection&lt;/a&gt; by the National Highway Traffic Safety Administration to function without drivers.&lt;/p&gt;
&lt;p&gt;The safety assessment would take into account the vehicle’s ability to avoid objects and pedestrians, cybersecurity, ethics, and backup plans if a system fails, for example.&lt;/p&gt;
&lt;p&gt;
&lt;a shape="rect" href="https://www.dmv.ca.gov/portal/wcm/connect/e12bbed6-f168-4fa3-bc09-4ca244c56ad0/avworkshopnotice_10192016.pdf?MOD=AJPERES"&gt;The DMV is inviting interested members of the public&lt;/a&gt; to discuss the redrafted regulations during a workshop on 19 October in Sacramento.&lt;/p&gt;
&lt;p&gt;If passed as law, the rules could potentially have implications for Alphabet (Google) and other self-driving car hopefuls, as &lt;a shape="rect" href="http://bigstory.ap.org/article/725f335940614ae79cd6c5a918eeba29/california-opens-pathway-cars-lack-steering-wheel"&gt;the Associated Press reports&lt;/a&gt;. The &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/google-selfdriving-car-will-be-ready-soon-for-some-in-decades-for-others"&gt;Google Self-Driving Car Project&lt;/a&gt; has prototyped cars with &lt;a shape="rect" href="https://www.theguardian.com/technology/2014/may/28/google-self-driving-car-how-does-it-work"&gt;only start and emergency stop buttons.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;A spokesman for Google would not comment on the development.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 3 Oct 2016 20:00:00 GMT</pubDate>
<dc:creator>Andrew Silver</dc:creator>
<guid>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/california-may-be-making-testing-selfdriving-cars-easier</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNjM5MA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNjM4OA.jpg" height="225" width="300"/>
</item>
<item>
<title>Google Translate Gets a Deep-Learning Upgrade</title>
<link>http://spectrum.ieee.org/tech-talk/computing/software/google-translate-gets-a-deep-learning-upgrade</link>
<description>Google engineers balanced speed and accuracy to deploy deep learning in Chinese-to-English translations</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Google engineers balanced speed and accuracy to deploy deep learning in Chinese-to-English translations&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNjEwNA.jpeg"/&gt;
&lt;figcaption&gt;Photo: Yuki Mizuma&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Google Translate has become a quick-and-dirty translation solution for millions of people worldwide since it debuted a decade ago. But Google’s engineers have &lt;span&gt;been quietly tweaking their&lt;/span&gt;
&lt;span&gt; machine translation service’s algorithms behind the scenes. &lt;/span&gt;They recently delivered a huge Google Translate upgrade that harnesses the popular artificial intelligence technique known as &lt;span&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/software/nervana-systems-puts-deep-learning-ai-in-the-cloud"&gt;deep learning&lt;/a&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Machine translation services such as Google Translate have mostly used a “phrase-based” approach of breaking down sentences into words and phrases to be independently translated. But several years ago, Google began experimenting with a deep-learning technique, called neural machine translation, that can translate entire sentences without breaking them down into smaller components. That approach eventually reduced the number of Google Translate errors by at least 60 percent on many language pairs in comparison with the older, phrase-based approach.&lt;/p&gt;
&lt;p&gt;“&lt;span&gt;We believe we are the first using [neural machine translation] in a large-scale production environment,” says &lt;a shape="rect" href="http://research.google.com/pubs/MikeSchuster.html"&gt;Mike Schuster&lt;/a&gt;, research scientist at Google.&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;Many major tech companies have heavily invested in neural machine translation from a research standpoint, &lt;span&gt;says Kyunghyun Cho, a &lt;a shape="rect" href="http://www.kyunghyuncho.me/"&gt;deep-learning researcher at New York University&lt;/a&gt; with a focus on natural language processing. But he confirmed that &lt;/span&gt;Google seems to be the first to publicly announce its use of neural machine translation in a translation product.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Google Translate has already begun using neural machine translation for its 18 million daily translations between English and Chinese. In a &lt;a shape="rect" href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;blog post&lt;/a&gt;, Google researchers also promised to roll out the improved translations to many more language pairs in the coming months.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The deep-learning approach of Google’s neural machine translation relies on a type of software algorithm known as a recurrent neural network. The neural network consists of nodes, also called artificial neurons, arranged in a stack of layers consisting of 1,024 nodes per layer.&lt;/p&gt;
&lt;p&gt;A network of eight layers acts as the “encoder,” which takes the sentence targeted for translation—let’s say from Chinese to English—and transforms it into a list of “vectors.” Each vector in the list represents the meanings of all the words read so far in the sentence, so that a vector farther along the list will include more word meanings.&lt;/p&gt;
&lt;p&gt;Once the Chinese sentence has been read by the encoder, a network of eight layers acting as the “decoder” generates the English translation one word at a time in a series of steps. A separate “attention network” connects the encoder and decoder by directing the decoder to pay special attention to certain vectors (encoded words) when coming up with the translation. It’s not unlike a human translator constantly referring back to the original sentence during a translation.&lt;/p&gt;
&lt;p&gt;This represents an improved version of the original encoder-decoder method that would compress the starting sentence into a fixed-size vector, regardless of the original sentence’s length. The improved version was presented in a paper that includes Cho as coauthor. Cho, who is not affiliated with Google, explains the less accurate original encoder-decoder method as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If I made an analogy to a human translator, what this means is that the human translator is going to look at a source sentence once, memorize the whole thing and start writing down its translation without ever looking back at the source sentence. This is both unrealistic and extremely inefficient. Why wouldn't a translator look back at the source sentence over and over?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Google started working on neural machine translation several years ago, but the method still generally proved less accurate and required more computational resources than the old approach of phrase-based machine translation. Better accuracy often came at the expense of speed, which is problematic for Google Translate users, who expect almost instantaneous translations.&lt;/p&gt;
&lt;p&gt;Google researchers had to harness several clever work-around solutions for their deep-learning algorithms to get beyond the existing limitations of neural machine translation. For example, the team connected the attention network to the encoder and decoder networks in a way that sacrificed some accuracy but allowed for faster speed through parallelism—the method of using several processors to run certain parts of the deep-learning algorithm simultaneously. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“&lt;/span&gt;
&lt;span&gt;We believe some of our architectural choices are quite unique, mostly to allow maximum parallelism during computation while achieving good accuracy,” Schuster explains.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Another innovation helped neural machine translation handle certain rare words. Part of Google’s solution to this came from the previous work of Schuster and his colleagues on improving the Google Japanese and Korean speech recognition systems. They figured out how to break down rare words into a limited set of smaller, common subunits called “wordpieces,” which the neural machine translation could handle more easily.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;A third innovation came from using “quantized computation” to reduce the precision of the system’s calculations and therefore speed up the translation process. Google’s team trained their system to tolerate the resulting “quantization errors” that could arise as a result. “Quantized computation is generally faster than nonquantized computation because all normally 32-bit or 64-bit data can be compressed into 8 or 16 bits, which reduces the time accessing that data and generally makes it faster to do any computations on it,” Schuster says.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Google’s neural machine translation also benefits from running on better hardware than traditional CPUs. The tech giant is using a specialized chip designed for deep learning called the Tensor Processing Unit (TPU). The TPUs alone helped speed up translation by 3.5 times over ordinary chips.&lt;/p&gt;
&lt;p&gt;When combined with the new algorithm solutions, Google made its neural machine translation more than 30 times faster with almost no loss of translation accuracy. That huge speed boost made the difference in Google’s decision to finally begin using the deep-learning algorithms for Google Translate in Chinese-to-English translations. The results seem impressive enough to outside experts such as Cho.&lt;/p&gt;
&lt;p&gt;“I am extremely impressed by their effort and success in making the inference of neural machine translation fast enough for their production system by quantized inference and their TPU,” Cho says.&lt;/p&gt;
&lt;p&gt;Google Translate and other machine translation services still have room for improvement. For example, even the upgraded Google Translate still messes up rare words or simply leaves out certain parts of sentences without translating them. It also still has problems using context to improve its translations. But Schuster seems optimistic that machine translation services will continue to make future progress and creep ever closer to human capabilities.&lt;/p&gt;
&lt;p&gt;“If you look at the history of machine translation, you see a constant uptick of translation quality and speed, and we only see this [continuing] until the system is as good as a human in communicating information from one language to another,” Schuster says.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 3 Oct 2016 14:31:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/software/google-translate-gets-a-deep-learning-upgrade</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNjExNw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNjExNQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Working Out With Intel and Oakley’s Chatty Radar Pace Augmented-Reality Smart Glasses</title>
<link>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/portable-devices/inteloakleys-chatty-smart-glasses-take-us-one-step-closer-to-her</link>
<description>If your smart glasses contain a virtual companion who knows exactly what you are doing and has an opinion about it, are you living in an augmented reality?</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;If your smart glasses contain a virtual companion who knows exactly what you are doing and has an opinion about it, are you living in an augmented reality?&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNTc0MQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Tekla S. Perry&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p class="jwcode"&gt;
&lt;script class="jwembed" src="//content.jwplatform.com/players/GkvvtSH2-7pFgM9ap.js"/&gt;
&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;figcaption&gt;Spectrum’s Tekla S. Perry takes over the treadmill from Oakley’s Luiz Dias at a demo of the Radar Pace smart glasses developed by Oakley and Intel. The virtual coach talked her into turning what she had planned to be a light stroll into a brief run.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Intel’s Chris Croteau and Oakley’s Luiz Dias met with me this week to show off the &lt;a shape="rect" href="http://www.oakley.com/en/blog/radar-pace/"&gt;Radar Pace&lt;/a&gt; smart glasses that will hit stores this Saturday, but they were careful to downplay expectations for this new consumer wearable technology.&lt;/p&gt;
&lt;p&gt;The smart glasses, designed jointly by Intel and the Luxottica Group (Oakley is a subsidiary), are “really just for runners and cyclers,” they kept pointing out. “We spent a long time trying to understand what kind of wearables athletes want, and designed this for them,” they emphasized. And, they told me several times, “we’re not trying to be Google Glass.”&lt;/p&gt;
&lt;p&gt;“We’re concerned about avoiding social awkwardness,” chimed in Scott Smith, a vice president at Luxottica, Oakley’s parent company.&lt;/p&gt;
&lt;p&gt;Still, Radar Pace’s little voice in your ear—one that knows exactly what you’re doing and encourages you to do it a little better—has a bit of the flavor of the virtual companion in the movie &lt;em&gt;Her&lt;/em&gt;. It’s going to be very tempting to go beyond the questions it expects (Am I on my target pace? How’s my stride length?), to getting a little chattier (I think I know that guy who just ran past. Should I say hello?). In fact, Radar Pace strays into the territory of audio &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/bionics/augmented-reality-in-a-contact-lens/0"&gt;augmented reality&lt;/a&gt;, an area with huge potential that’s getting little attention as we obsess about visual AR like Pokémon Go.&lt;/p&gt;
&lt;p&gt;And Croteau, in spite of trying to focus on sports training, wouldn’t argue with that perception. “It is indeed AR,” he says. “We are augmenting the reality of an athlete through audio.”&lt;/p&gt;
&lt;p&gt;OK, so what is Radar Pace, really? It is a version of Oakley’s Radar sunglasses, &lt;a shape="rect" href="http://www.backpacker.com/gear/essentials/eyewear/gear-review-oakley-radar-path-sunglasses/#bp=0/img1"&gt;long popular with cyclists&lt;/a&gt; for their comfort, clarity, and durability. The standard Radar frame has been kitted out with a Bluetooth radio, an accelerometer, a gyroscope, a pressure sensor, a humidity sensor, a proximity sensor, three microphones, earbuds, and an embedded processor that acts as a &lt;a shape="rect" href="http://spectrum.ieee.org/consumer-electronics/gadgets/nextgen-sensors-make-golf-clubs-tennis-rackets-and-baseball-bats-smarter-than-ever"&gt;sensor fusion hub&lt;/a&gt;. According to Croteau, who leads Intel’s head-worn device projects in the company’s New Technology Group, all this gear adds just a tiny bit of weight to the standard Radar glasses; he didn’t say how much, but the total mass of the Radar Pace frames is 56 grams. On the software side, Intel is introducing its own natural voice package called Intel Real Speech with this product; it currently understands five languages.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxNTc1OA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Oakley&lt;/figcaption&gt;
&lt;figcaption&gt;The Oakley Radar Pace smart glasses include a Bluetooth radio, an accelerometer, a gyroscope, a pressure sensor, a humidity sensor, a proximity sensor, three microphones, earbuds, and an embedded processor.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The joint design project teaming Intel and Oakley started shortly after the January 2014 CES technology show, &lt;span&gt;says Dias, who is Oakley’s manager for wearable technology&lt;/span&gt;. Oakley had made a few forays into wearable electronics, like the Thump MP3 player in 2004, but didn’t have a big hit. Intel CEO &lt;a shape="rect" href="http://www.crn.com/slide-shows/components-peripherals/240165203/7-highlights-from-intels-ces-2014-keynote.htm"&gt;Brian Krzanich, giving a keynote&lt;/a&gt; at that CES, announced that Intel was going to go big in wearables. (Just about every major consumer electronics company made the same statement; when covering the conference that year, I called it &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/consumer-electronics/portable-devices/ces-2014-trends-everybodys-making-fitness-trackers-and-smart-watches-but-who-will-succeed"&gt;“the CES of the wearable gadget.”&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;“We came back from CES,” Dias says, “and started trying to understand what people running and cycling want from wearables. We saw people carrying phones for emergencies, people with earbuds listening to music, and people wearing lots of devices giving data, like wristbands and chest-strap monitors. We didn’t see any interpretation of the data.”&lt;/p&gt;
&lt;p&gt;Oakley and Intel began working together in March 2014. “At the time,” Dias says, “we knew we were going to do a coaching product, but we had a blank slate.”&lt;/p&gt;
&lt;p&gt;“We did know we wanted the Radar glasses as the form factor, says Croteau. “Radar was a successful product, and it was already out there.”&lt;/p&gt;
&lt;p&gt;Intel brought in its team of social scientists, and they held focus groups and mock coaching sessions with athletes and coaches to find out exactly what kind of information coaches would give athletes during their training.&lt;/p&gt;
&lt;p&gt;“We really focused on usability—we don’t need to put two pounds of stuff on your head, just what you will use,” Croteau says. That’s how they decided to go audio only.&lt;/p&gt;
&lt;p&gt;The final package—the tricked-out sunglasses with two swappable lenses along with the app—will retail for US $449. That’s about $150 more than comparable standard Radar sunglasses, Dias says.&lt;/p&gt;
&lt;p&gt;Besides using its own set of sensors to collect data, it connects to third-party fitness tracking products (a clip on your shoe, a heart rate monitor on your chest).&lt;/p&gt;
&lt;p&gt;The system gives you updates quantitatively (“Your heart rate is…”), says Dias.  But it will also answer more complicated questions, like “Why should I run faster up this hill?” It will also connect you to your music library or agents like Siri, for answers to key questions like “I’m almost done with my run. Where’s the nearest Jamba Juice?” An Intel executive is reportedly already using the noise-canceling microphone array to conduct conference calls while biking to work.&lt;/p&gt;
&lt;p&gt;I tested it out briefly in the demo room [see video]. The Radar Pace glasses were indeed hugely comfortable, and the voice interface responsive enough to be usable. My only beef is that there’s not a version that can accommodate those of us who need corrective lenses and don’t wear contacts.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 30 Sep 2016 14:43:00 GMT</pubDate>
<dc:creator>Tekla Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/portable-devices/inteloakleys-chatty-smart-glasses-take-us-one-step-closer-to-her</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNTc0Nw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNTc0NQ.jpg" height="225" width="300"/>
</item>
<item>
<title>New Memristor Circuit Mimics Synapses in the Brain</title>
<link>http://spectrum.ieee.org/the-human-os/semiconductors/memory/mimicking-the-synapses-of-the-brain</link>
<description>Engineers used memristors to accurately emulate a key part of learning and memory in the human brain</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Engineers used memristors to accurately emulate a key part of learning and memory in the human brain&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNTMyMA.jpeg"/&gt;
&lt;figcaption&gt;Illustration: University of Massachusetts, Amherst&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxNTM1NA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: University of Massachusetts, Amherst&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To a human brain, picking one particular image out of a thousand is an easy task. Billions of neurons and the synapses that connect them can quickly process information in parallel to make a decision. Seeking to make such processing that easy for machines, scientists and engineers have been working with devices called &lt;a shape="rect" href="http://spectrum.ieee.org/tag/memristors"&gt;memristors&lt;/a&gt;, which have some similar behaviors to neural synapses. Engineers at the University of Massachusetts report this week that they’ve invented a memristor circuit that matches a synapses’ behavior more closely than any before.&lt;/p&gt;
&lt;p&gt;First predicted in 1971 and &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/design/the-mysterious-memristor"&gt;invented in 2008&lt;/a&gt; by HP, memristors are so named because they remember how much voltage you applied across the device and how long you applied it, storing the information as a change in resistance. HP engineers noticed immediately that such a characteristic was similar to the way the synaptic connection between neurons strengthens with use to form a memory.&lt;/p&gt;
&lt;p&gt;Memristors are “kind of the ideal candidate in many aspect” says Jianhua Yang, an electrical engineer at the University of Massachusetts Amherst working on improving their behavior. Low-power, simple circuits of memristors could improve the ability of computers to solve power-intensive computer vision and machine learning tasks that human brains handle with little effort.&lt;/p&gt;
&lt;p&gt;In research published in &lt;a shape="rect" href="http://www.nature.com/nmat/journal/vaop/ncurrent/full/nmat4756.html"&gt;Nature Materials on 26 September&lt;/a&gt;, Yang and his team mimicked a crucial underlying component of how synaptic connections get stronger or weaker: the flow of calcium.&lt;/p&gt;
&lt;p&gt;The movement of calcium into or out of the neuronal membrane, neuroscientists have found, directly affects the connection. Chemical processes move the calcium in and out— triggering a long-term change in the synapses’ strength. 2015 research in &lt;a shape="rect" href="http://pubs.acs.org/doi/abs/10.1021/acs.nanolett.5b00697"&gt;ACS NanoLetters&lt;/a&gt; and &lt;a shape="rect" href="http://onlinelibrary.wiley.com/doi/10.1002/adfm.201501427/abstract"&gt;Advanced Functional Materials&lt;/a&gt; discovered that types of memristors can simulate some of the calcium behavior, but not all.&lt;/p&gt;
&lt;p&gt;In the new research, Yang combined two types of memristors in series to create an artificial synapse. The hybrid device more closely mimics biological synapse behavior—the calcium flow in particular, Yang says.&lt;/p&gt;
&lt;p&gt;The new memristor used--called a diffusive memristor because atoms in the resistive material move even without an applied voltage when the device is in the high resistance state—was a dielectic film sandwiched between Pt or Au electrodes. The film contained Ag nanoparticles, which would play the role of calcium in the experiments.&lt;/p&gt;
&lt;p&gt;By tracking the movement of the silver nanoparticles inside the diffusive memristor, the researchers noticed a striking similarity to how calcium functions in biological systems.&lt;/p&gt;
&lt;p&gt;A voltage pulse to the hybrid device drove silver into the gap between the diffusive memristor’s two electrodes–creating a filament bridge. After the pulse died away, the filament started to break and the silver moved back— resistance increased.&lt;/p&gt;
&lt;p&gt;Like the case with calcium, a force made silver go in and a force made silver go out.&lt;/p&gt;
&lt;p&gt;To complete the artificial synapse, the researchers connected the diffusive memristor in series to another type of memristor that had been studied before.&lt;/p&gt;
&lt;p&gt;When presented with a sequence of voltage pulses with particular timing, the artificial synapse showed the kind of long-term strengthening behavior a real synapse would, according to the researchers. “We think it is sort of a real emulation, rather than simulation because they have the physical similarity,” Yang says.&lt;/p&gt;
&lt;p&gt;He says the next step is to better understand the mechanisms. Then, his team plans to combine the artificial synapses into arrays and eventually build bio-inspired circuits.&lt;/p&gt;
&lt;p&gt;“We can have a more direct, more natural, and a more complete emulation to the synaptic system using the silver-based system,” he says.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 29 Sep 2016 18:34:00 GMT</pubDate>
<dc:creator>Andrew Silver</dc:creator>
<guid>http://spectrum.ieee.org/the-human-os/semiconductors/memory/mimicking-the-synapses-of-the-brain</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNTMzMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNTMyOA.jpg" height="225" width="300"/>
</item>
<item>
<title>IBM's Brain-Inspired Chip Tested for Deep Learning</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/ibms-braininspired-chip-tested-on-deep-learning</link>
<description>IBM's TrueNorth computer chip shows it can do deep learning despite not having been designed for it</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;IBM's TrueNorth computer chip shows it can do deep learning despite not having been designed for it&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxNDIwNQ.jpeg"/&gt;
&lt;figcaption&gt;Illustration: Shutterstock&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;The deep-learning software driving the modern artificial intelligence revolution has mostly run on fairly standard computer hardware. Some tech giants such as Google and Intel have focused some of their considerable resources on creating more specialized computer chips designed for deep learning. But IBM has taken a more unusual approach: It is testing its brain-inspired TrueNorth computer chip as a hardware platform for deep learning.&lt;/p&gt;
&lt;p&gt;Deep learning’s powerful capabilities rely on algorithms called convolutional neural networks that consist of layers of nodes (also known as neurons). Such neural networks can filter huge amounts of data through their “deep” layers to become better at, say, automatically recognizing individual human faces or understanding different languages. These are the types of capabilities that already empower online services offered by the likes of Google, Facebook, Amazon, and Microsoft.&lt;/p&gt;
&lt;p&gt;In recent research, IBM has shown that such deep-learning algorithms could run on &lt;a shape="rect" href="http://spectrum.ieee.org/computing/hardware/how-ibm-got-brainlike-efficiency-from-the-truenorth-chip"&gt;brain-inspired hardware&lt;/a&gt; that typically supports a very different type of neural network.&lt;/p&gt;
&lt;p&gt;IBM published a paper on its work in the 9 September 2016 issue of the journal &lt;em&gt;
&lt;a shape="rect" href="http://www.pnas.org/content/early/2016/09/19/1604850113.full"&gt;Proceedings of the National Academy of Sciences&lt;/a&gt;. &lt;/em&gt;The research was funded with &lt;a shape="rect" href="https://govtribe.com/contract/award/fa945315c0055"&gt;just under $1 million&lt;/a&gt; from the U.S. Defense Advanced Research Projects Agency (DARPA). Such funding formed part of DARPA’s &lt;a shape="rect" href="https://www.fbo.gov/index?s=opportunity&amp;amp;mode=form&amp;amp;id=91bc9e58d6fa024d55d7c0583d38fc21&amp;amp;tab=core&amp;amp;_cview=0"&gt;Cortical Processor program&lt;/a&gt; aimed at brain-inspired AI that can recognize complex patterns and adapt to changing environments.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“The new milestone provides a palpable proof of concept that the efficiency of brain-inspired computing can be merged with the effectiveness of deep learning, paving the path towards a new generation of chips and algorithms with even greater efficiency and effectiveness,” says Dharmendra Modha, &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://researcher.watson.ibm.com/researcher/view.php?person=us-dmodha"&gt;chief scientist for brain-inspired computing at IBM Research-Almaden&lt;/a&gt;,&lt;span&gt; in San Jose, Calif.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;IBM first laid down the specifications for TrueNorth and a prototype chip in 2011. So, TrueNorth predated—and was therefore never specifically designed to harness—the deep-learning revolution based on convolutional neural networks that took off starting in 2012. Instead, TrueNorth typically supports spiking neural networks that more closely mimic the way real neurons work in biological brains.&lt;/p&gt;
&lt;p&gt;Instead of firing every cycle, t&lt;span&gt;he neurons in spiking neural networks must gradually build up their potential before they fire&lt;/span&gt;. To achieve precision on deep-learning tasks, spiking neural networks typically have to go through multiple cycles to see how the results average out. That effectively slows down the overall computation on tasks such as image recognition or language processing.&lt;/p&gt;
&lt;p&gt;Deep-learning experts have generally viewed spiking neural networks as inefficient—at least, compared with convolutional neural networks—for the purposes of deep learning. &lt;span&gt;Yann LeCun, director of AI research at Facebook and a pioneer in deep learning, &lt;a shape="rect" href="https://www.facebook.com/yann.lecun/posts/10152184295832143"&gt;previously critiqued&lt;/a&gt; IBM’s TrueNorth chip because it primarily supports spiking neural networks. (See &lt;/span&gt;
&lt;span id="docs-internal-guid-6f6ef5c7-6c47-76fe-9bd9-e48722c4b97d"&gt;
&lt;span&gt;
&lt;em&gt;IEEE Spectrum&lt;/em&gt;’s &lt;a shape="rect" href="http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning#qaTopicTwo"&gt;previous interview&lt;/a&gt; with LeCun on deep learning.)&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The IBM TrueNorth design may better support the goals of neuromorphic computing that focus on closely mimicking and understanding biological brains, &lt;span&gt;says Zachary Chase Lipton, a deep-learning researcher in the &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px;" shape="rect" href="http://ai.ucsd.edu/"&gt;Artificial Intelligence Group at the University of California, San Diego&lt;/a&gt;
&lt;span&gt;. By comparison, deep-learning researchers are more interested in getting practical results for AI-powered services and products. He explains the difference as follows:&lt;/span&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To evoke the cliche metaphor about birds and airplanes, you might say the computational neuroscience/neuromorphic community is more concerned with studying birds, and the machine learning community more interested in understanding aerodynamics, with or without the help of biology. The deep learning community is generally bullish on the benefits of specialized hardware. [Therefore,] the neuromorphic chips don't inspire as much excitement because the spiking neural networks they focus on are not so popular in deep learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;To make the TrueNorth chip a good fit for deep learning, IBM had to develop a new algorithm that could enable convolutional neural networks to run well on its neuromorphic computing hardware. This combined approach achieved what IBM describes as “near state-of-the-art” classification accuracy on eight data sets involving vision and speech challenges. They saw between 65 percent and 97 percent accuracy in the best circumstances.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;When just one TrueNorth chip was being used, it surpassed &lt;a shape="rect" href="http://www.pnas.org/content/early/2016/09/19/1604850113/T3.expansion.html"&gt;state-of-the-art accuracy on just one out of eight data sets&lt;/a&gt;. But IBM researchers were able to boost the hardware’s accuracy on the deep-learning challenges by using up to eight chips. That enabled TrueNorth to match or surpass state-of-the-art accuracy on three of the data sets.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The TrueNorth testing also managed to process between&lt;span&gt; 1,200 and 2,600 video frames per second. That means a single TrueNorth chip could detect patterns in real time from between as many as 100 cameras at once, Modha says. This assumes each camera uses 1,024 color pixels (32 x 32) and streams information at a standard TV rate of 24 frames per second.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Such results may be impressive for TrueNorth’s first major foray into deep-learning testing, but they should be taken with a grain of salt, Lipton says. He points out that the vision data sets involved very minor problems with the 32 x 32 pixel images.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Still, IBM’s Modha seems enthusiastic about continuing to test TrueNorth for deep learning. He and his colleagues hope to test the chip on so-called unconstrained deep learning, which involves gradually introducing hardware constraints during the training of neural networks instead of constraining them from the very beginning.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Modha also points to TrueNorth’s general design as an advantage over those of more specialized deep-learning hardware designed to run only convolutional neural networks. It will likely allow the running of multiple types of AI networks on the same chip. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;“Not only is TrueNorth capable of implementing these convolutional networks, which it was not originally designed for, but it also supports a variety of connectivity patterns (feedback and lateral, as well as feed forward) and can simultaneously implement a wide range of other algorithms,” Modha says.&lt;/p&gt;
&lt;p&gt;Such biologically inspired chips would probably become popular only if they show that they can outperform other hardware approaches for deep learning, Lipton says. But he suggested that IBM could leverage its hardware expertise to join Google and Intel in creating new specialized chips designed specifically for deep learning.&lt;/p&gt;
&lt;p&gt;“I imagine that some of the neuromorphic chipmakers will use their expertise in hardware acceleration to develop chips more focused on practical deep-learning applications and less focused on biological simulation,” Lipton says.&lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 27 Sep 2016 20:00:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/ibms-braininspired-chip-tested-on-deep-learning</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxNDIxOA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxNDIxNg.jpg" height="225" width="300"/>
</item>
<item>
<title>The 2017 Acura NSX: A Hybrid Supercar</title>
<link>http://spectrum.ieee.org/transportation/advanced-cars/the-2017-acura-nsx-a-hybrid-supercar</link>
<description>Honda’s new Acura NSX is the first to marry a V-6 engine to three electric motors for high-speed steering</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Honda’s new Acura NSX is the first to marry a V-6 engine to three electric motors for high-speed steering&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxMDIyOQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Acura&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgxMDIwNQ.jpeg"&gt;
&lt;img alt="Acura NSX" src="http://spectrum.ieee.org/image/MjgxMDIwNA.jpeg"/&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Acura&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;One. Two. Whoosh. &lt;/strong&gt;By the time I count to three, the Acura NSX’s automated launch control leaps from a standstill to 60 miles per hour. But there’s not a trace of wheel spin and smoking rubber, the usual hallmarks of a neck-snapping drag-strip run here at the track in Thermal, Calif. Oh, there is drama, only it’s largely confined to what’s happening under the Acura’s swoopy skin.&lt;/p&gt;
&lt;p&gt;This Acura is a plug-in hybrid, part of an electron-pumping vanguard that’s changing the very definition of a performance car. From showrooms to race paddocks, the clock is ticking for fuel-slurping gasoline engines. Battery-boosted cars, whether hybrid or full electric, are rushing to fill the gap. In our highly regulated future, these may be the only kinds of sports cars you’ll be able to buy, and the trippy journey to such a world seems to be taking place at warp speed.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="The new Acura NSX has a twin-turbo." src="http://spectrum.ieee.org/image/MjgxMDIwNg.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Acura&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Hunkered Down: &lt;/strong&gt;The new Acura NSX has a twin-turbo, 500-horsepower V-6 engine, which is mounted longitudinally in the midsection for optimal handling. A 75-degree angle between the cylinder banks lets the engine hunker down, giving it the lowest center of gravity in its class. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;Back in 1990, &lt;/strong&gt;the original Acura NSX challenged every notion of what a supercar was supposed to be. Coming from Honda, the manufacturer of the Acura luxury brand and a company known for safe, affordable, and ultrareliable cars, the NSX wedded those practical virtues to a gorgeous lightweight body designed by Italy’s Pininfarina. Smack at its center rested a modest 3-liter V-6, capable of 200 kilowatts (270 horsepower). Packing more lightweight aluminum than anything from Ferrari, Lamborghini, or Porsche, the Acura defied expectations again with a shocking US $60,000 price, a fraction the cost of its highfalutin rivals. In a final coup, Brazilian Formula One superstar Ayrton Senna, then driving for McLaren-Honda, helped tune the NSX’s suspension and performance prior to its release.&lt;/p&gt;
&lt;p&gt;Ferrari and Co. were instantly forced out of their complacency on technology and quality alike. The NSX topped the Ferrari 348, and most every other competitor, in handling and daily drivability.&lt;/p&gt;
&lt;p&gt;So when Honda found itself developing a reborn NSX in 2011, the new car had massive shoes to fill. Oddly, Honda’s engineers originally planned to power their new roadster with a prosaic V-6 derived from an Odyssey minivan. No wonder that project was aborted midstream. To deserve the storied name, any Son of NSX would have to be an “everyday supercar” while again moving the needle on technology. Ted Klaus, chosen to head up the NSX’s global R&amp;amp;D team—which is now run out of Ohio, rather than Japan—knew that electricity was the answer, not just to power the car but to perform handling magic as well.&lt;/p&gt;
&lt;p&gt;“We had been working for years to come up with drive force that could help turn the car right and left,” Klaus recalls. “We asked ourselves: What if we could marry emerging hybrid e-drive technology with yaw-control tech [that is, steering]? Would it be possible?”&lt;/p&gt;
&lt;p&gt;The answer was yes. But the tight-knit NSX team was facing three more years—and an increasingly skeptical media and fan base—to create that ambitious design from scratch: a hybrid supercar that converts electricity into mechanical commands, not just for explosive, efficient propulsion and regenerative braking but also to steer and stabilize the car.&lt;/p&gt;
&lt;p&gt;As it happens, Porsche was developing an all-wheel-drive hybrid with similar characteristics, the &lt;a shape="rect" href="http://spectrum.ieee.org/transportation/advanced-cars/the-greening-of-the-supercar/porsche918-spyder"&gt;
&lt;span&gt;918 Spyder&lt;/span&gt;
&lt;/a&gt;; it would arrive priced at a mind-boggling $845,000. The 2017 NSX that I’m testing near Palm Springs costs $157,800.&lt;/p&gt;
&lt;figure class="stacked xlrg" role="img"&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgxMDI1NQ.jpeg"&gt;
&lt;img alt="Photos: Acura" src="http://spectrum.ieee.org/image/MjgxMDIwNw.jpeg"/&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgxMDI2Mg.jpeg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDIwOA.jpeg"/&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photos: Acura&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;The Energizer:&lt;/strong&gt; Behind the seats, a T-shaped lithium-ion battery feeds three electric motors, including a pair to power and steer the front wheels. A braking simulation system saves energy in a way that feels natural.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;And unlike the Porsche, which has just one electric motor to power both front wheels, the Acura has two electric tricks up its sleeves: the so-called Twin Motor Unit. This dizzyingly complex electric duo, mediated through a planetary gear set, cranks out up to 27 kW (36 hp) and 73 newton meters (54 foot-pounds) of torque to either wheel, divvying it up as needed. This is true torque vectoring, able to independently speed up or slow down either wheel, helping the Acura dive into turns and dig out the other side. Discreetly nestled behind pilot and passenger, the roughly 1-kilowatt-hour lithium-ion battery is designed to rapidly charge and discharge for generous squirts of performance. A larger battery might have contributed more all-electric driving range, but it would have come at the expense of weight and ultimate performance.&lt;/p&gt;
&lt;p&gt;The instant the driver rotates the steering wheel, sensors sample the car’s controls, and then software processes the data in just 10 to 20 milliseconds. The time it takes the electrical system to convert that input into steering response, Klaus says, is on the order of 50 ms. Yes, that’s fast. Front-axle motors deliver their full monty of instantaneous torque at every possible engine speed between 0 and 2,000 rpm; 2,000 rpm is the point at which the gas engine rouses itself to take over the majority of propulsion. The electric motors can still help propel the car at up to 200 kilometers per hour (124 miles per hour) and assist in turning right up to the car’s 300-km/h top speed.&lt;/p&gt;
&lt;p&gt;To provide shove at the rear wheels, a 3.5-L racing V-6 shares nary a bolt with any production Honda engine ever built before. A 75-degree angle between its cylinder banks lowers the center of gravity, in unique contrast to the industry’s typical 60-degree V-6s. And it churns up 373 kW (500 hp) and 550 N∙m (406 foot-pounds) of torque. A third electric traction motor, with 35 kW and 148 N∙m of torque, sandwiches between the engine’s crankshaft and nine-speed, dual-clutch automated gearbox. The motors even help smooth gear changes, adding power as engine speeds fall during shifts.&lt;/p&gt;
&lt;p&gt;Add it up and the NSX sends 427 kW (573 hp) and 633 N∙m (476 ft.-lbs.) to the wheels, on par with gas-only supercars like the Audi R8 V-10 that have more cylinders and consume much more fuel.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;So h&lt;/strong&gt;
&lt;strong&gt;ow does it driv&lt;/strong&gt;
&lt;strong&gt;e?&lt;/strong&gt; To find out, I chased Indy driver Graham Rahal around the &lt;a shape="rect" href="http://www.thethermalclub.com/uploads/1/3/9/2/13927410/tml2218_trackmap_8.30.14.pdf"&gt;Thermal Club’s snaking road course&lt;/a&gt; [PDF]. This was definitely the time to dial the NSX up from its Quiet setting, the Sunday-church mode that prioritizes electric propulsion and highlights those smooth Honda manners. I moved through Sport Plus to Track mode, which transformed the car into the howling, road-clawing beast you’d expect at this price. The car’s driver-selectable performance modes, four in all, can vary engine sound by as much as 25 decibels, with natural frequencies pumped into the cabin via tubes behind passengers’ heads.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDIwOQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Acura&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Right Stuff: &lt;/strong&gt;Ted Klaus, head of NSX development, takes a whirl in his baby.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This yin-yang quality pervades the Acura. You could argue that it tries too hard to be an everyday Honda, to disguise the seriously heroic stuff under its skin. Since when do supercar owners, the flamboyant types who drop six figures on a howling Lamborghini, prefer to fly below the radar?&lt;/p&gt;
&lt;p&gt;Indeed, my first experience here in the Honda has me questioning its supercar bona fides. There’s plenty of tire grip, but the steering feels a bit robotic. The turbos emit a discreet whoosh, but the V-6 itself sounds meek and wholly unthreatening. And while the speedometer says we’re making good time, the Acura feels sneaky fast, not freaky fast.&lt;/p&gt;
&lt;p&gt;Whether the subject is street cars or the pinnacle of motor sports in Formula One—and &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/transportation/advanced-cars/formula-e-series-ends-first-season-wheeltowheel"&gt;its offshoot&lt;/a&gt;, the all-electric Formula E series—the purists’ complaints have been as loud as the cars are quiet: Electricity will silence the shriek and dull the visceral sensations that have thrilled drivers and spectators for more than a century.&lt;/p&gt;
&lt;p&gt;“I would say that’s a hot topic, the emotional attributes of an electric car,” Klaus acknowledges. “I like the idea that electric propulsion can stir your soul, without constantly shaking you up.”&lt;/p&gt;
&lt;p&gt;And even professional racers, he argues, don’t like a car that’s always on the edge, requiring constant correction and mental and physical strain. “That kind of car uses so much of your human bandwidth that you can’t extract maximum performance,” he says. “We’ve electrified a supercar that gives you a wide range of emotional experiences, from quiet cruising to unleashing the beast at the track.”&lt;/p&gt;
&lt;p&gt;Cars like the Acura may never wail like a V-10 Lamborghini, at least not without the artificially synthesized sound of cars like the &lt;a shape="rect" href="http://spectrum.ieee.org/transportation/advanced-cars/2015s-top-ten-tech-cars-bmw-i8"&gt;BMW i8&lt;/a&gt;, the autosport equivalent to lip-synching. But nor does the Acura slurp premium unleaded as shamelessly as a ’60s muscle car. The U.S. Environmental Protection Agency credits the NSX with 11.2 liters per 100 kilometers (21 miles per gallon), commendable for a supercar that can hang with a &lt;a shape="rect" href="http://spectrum.ieee.org/transportation/advanced-cars/top-10-tech-cars-2011"&gt;Ferrari 458&lt;/a&gt;, &lt;a shape="rect" href="http://spectrum.ieee.org/transportation/advanced-cars/2015s-top-ten-tech-cars-lamborghini-huracn-lp-6104"&gt;Lamborghini Huracán&lt;/a&gt;, or Porsche 911 Turbo.&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="Honda’s Ohio Performance Manufacturing Center " src="http://spectrum.ieee.org/image/MjgxMDIyNg.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Acura&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Clean-sheet Design:&lt;/strong&gt; Honda’s Ohio Performance Manufacturing Center is all new, like the NSX it makes. Ablation casting uses water to blast sand from molds, quickly cooling the molten aluminum for a stronger microstructure and superior crash performance. Eight robots apply 860 welds to the aluminum space frame. Wireless torque wrenches record tension settings for every bolt.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This being Honda, the NSX team set itself another lofty goal, creating what they believe is the world’s safest supercar. Klaus and his team say the NSX is measurably more crashworthy than its rivals. Maybe that’s not sexy, but it might save a driver’s life in a car designed for pushing the envelope.&lt;/p&gt;
&lt;p&gt;The team also developed numerous fail-safes to ensure consistent, trustworthy performance.&lt;/p&gt;
&lt;p&gt;“It took a couple of years to harden the system to our high reliability standards, so that if someone puts in an unusual or unskilled input, the vehicle’s safety performance is in line with an Accord or Civic,” says Klaus. “At up to 300 km/h or pulling over 1 g (in handling force), we had to make it accelerate, brake, and turn according to the driver’s desires, but ensure that nothing uncontrolled would happen to a driver or those around them in case of an electrical or mechanical failure.”&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Despite all the&lt;/strong&gt;
&lt;strong/&gt;integrated systems and all the electronic oversight, the Acura must still feel like a natural, involving sports car. The bravura brake-force simulator is a shining example: All supercars go, but the NSX is special for how it stops.&lt;/p&gt;
&lt;p&gt;The problem is that any hybrid car that wants to save fuel must brake regeneratively: Those electric motors must also be able to function as generators during braking, turning kinetic energy into electricity that is shunted to the battery for reuse. But that tends to give brakes a nonlinear, mashed-potatoes feel.&lt;/p&gt;
&lt;p&gt;“We needed a breakthrough in man-machine engineering,” Klaus says. “How much force do drivers want to feel, and how do we link that to deceleration value?”&lt;/p&gt;
&lt;p&gt;The answer was a system that simulates natural brake-pedal feel via a separate hydraulic circuit. Pushing the brakes triggers an electric motor that varies the amount of resistance felt in the pedal, but none of that hydraulic fluid is actually stopping the car. Your foot’s commands are actually electronically translated to activate brakes on a separate hydraulic circuit. The result is perhaps the most natural, linear-feeling hybrid brakes yet, whether used around town or at the track.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDIyNw.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Acura&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Automated Automaker:&lt;/strong&gt; The low-volume factory requires only 100 technicians to assemble, paint, and monitor quality for the NSX.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;“Those brakes felt horrible at the beginning of development,” Klaus says. “There were a lot of sleepless nights on how that system didn’t feel natural.”&lt;/p&gt;
&lt;p&gt;Honda’s customary no-stone-unturned approach extended to manufacturing. In Ohio, where Honda built its first American Accord in 1982, the NSX has begun rolling off the line at a showpiece plant called the Performance Manufacturing Center, or PMC.&lt;/p&gt;
&lt;p&gt;Clement D’Souza, the engineer who led development of the PMC, said that Honda visited about two dozen other factories, including those at Ferrari, McLaren, and Porsche, to benchmark manufacturing processes, tailoring its approach to that of Formula One teams. On its meticulous, low-volume assembly line, NSX is treated to a dozen patent-pending processes, including the &lt;a shape="rect" href="http://foundrymag.com/meltpour/ablation-cast-parts-debut-new-acura-nsx"&gt;world’s first use&lt;/a&gt; of ablation casting in an automobile: Sand molds for parts of the aluminum space frame are ablated, using water to dissolve the binder and blast away sand, which has the virtue of quickly cooling the newly solidified metal. That creates a stronger microstructure, which allows &lt;a shape="rect" href="https://www.youtube.com/watch?v=YerFT-H1D9c"&gt;lighter, thinner castings&lt;/a&gt; with superior crash absorption. The largely aluminum space-frame chassis, with a carbon-fiber floor and mixed-materials body (mostly aluminum and sheet-molded composite) feels as indomitable as anything in the class. Acura claims the chassis is much more rigid than those of its rivals, with the lowest center of gravity—a boon to handling.&lt;/p&gt;
&lt;p&gt;In Anna, Ohio, at Honda’s largest engine plant, technicians hand-assemble the NSX’s novel V-6 and mate it to the gearbox and rear-drive electric motor. Honda will complete only about eight NSX engines per day, but its technology can now trickle down to the company’s other cars. Elements of the NSX’s all-wheel-drive hybrid technology, Klaus notes, are already found in affordable models including the MDX crossover and RLX sedan.&lt;/p&gt;
&lt;p&gt;For type A drivers, the effort all comes together with a few toggles of the performance dial. Cranked up to its Track mode on the road course at Thermal Club, the NSX palpably transforms from benign Jekyll to murderous Hyde, bounding into corners and wagging its exotic tail on the way out. That electric umbilical cord is severed and forgotten, and the Acura is suddenly an Italian-baiting supercar but undoubtedly more rock-solid reliable, just like the original NSX.&lt;/p&gt;
&lt;p&gt;I depart the track for a run to the town of Idyllwild, then a fantasy two-lane descent from the San Jacinto Mountains. The Acura slingshots from curve to fate-tempting curve. I’m driving as quickly as I dare on public roads, letting those front wheels electrify their way into and out of every corner. The threesome of electric motors fills every power gap in the turbocharged engines, with surges of juice making up for turbo lag and smoothing gear changes. The NSX actually gets to 100 km/h fastest in Sport Plus, the mode in which the computers let the motors suck more battery power than usual, knowing it’ll be instantly recovered via the frequent regeneration inherent to street driving. With such consistent power, and all the piped-in sounds, you’d almost swear a naturally aspirated V-8 was roaring behind your head.&lt;/p&gt;
&lt;p&gt;The Acura corners with near-Italian brio, its variable-ratio steering finely weighted, though the steering is not especially good at transmitting information on the road surface into your hands. That’s surely due to the filtering effect of its electric systems. Ultrawide-range magnetic dampers stiffen or soothe the car at all four corners, pancaking the NSX to the road with extra syrup. Maybe this Acura has a heart after all, beneath that sleek aluminum skin and layers of technology.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;So what do cars&lt;/strong&gt; like the NSX mean for the future of performance and, more specifically, for electric propulsion?&lt;/p&gt;
&lt;p&gt;Despite giveaway fuel prices in the United States, a sales explosion for SUVs around the world, and an ice-cold market for electric cars not named Tesla, automakers big and small continue to pour resources into hybrids. Companies as different as GM and McLaren are focused on fun-to-drive performance, not just efficiency.&lt;/p&gt;
&lt;p&gt;Tesla has something to do with that. In 2008, its Roadster overturned the stereotype of EVs as glorified golf carts. Just as important are new emissions and fuel-economy regulations, which are compelling carmakers to electrify their fleets.&lt;/p&gt;
&lt;p&gt;BMW may have blinked, abruptly changing the course of its high-profile i Division: Electric cars are out, autonomous cars are in. For its next all-new EV, the iNext, slated for 2021, BMW is suddenly touting autonomous function as much as its electric power train.&lt;/p&gt;
&lt;p&gt;Yet Ferrari has pledged that every new model will be a hybrid, turbocharged, or both. McLaren’s Track 22 project targets 2022 to electrify half its models, including a potential all-electric version of its fearsome P1 hypercar. Porsche has an electric sedan in the works, based on its stunning Mission E concept.&lt;/p&gt;
&lt;p&gt;The jury is still out on whether electricity will entirely displace old-school internal combustion in performance cars. But Klaus is convinced that electricity will soon be an unstoppable force. “Electric propulsion has a bright future,“ he says. “There’s no two ways about it.”&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the October 2016 print issue as “Supercar 2.0.”&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 26 Sep 2016 13:30:00 GMT</pubDate>
<dc:creator>Lawrence Ulrich</dc:creator>
<guid>http://spectrum.ieee.org/transportation/advanced-cars/the-2017-acura-nsx-a-hybrid-supercar</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxMDIzOQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxMDIzNw.jpg" height="225" width="300"/>
</item>
<item>
<title>The Megaprocessor Laughs at Your Puny Integrated Circuits</title>
<link>http://spectrum.ieee.org/geek-life/profiles/the-megaprocessor-laughs-at-your-puny-integrated-circuits</link>
<description>The wondrous insanity of a 42,300-transistor CPU the size of a room</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The wondrous insanity of a 42,300-transistor CPU the size of a room&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxMDkyMA.jpeg"/&gt;
&lt;figcaption&gt;Photo: James Newman&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDkxOQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: James Newman&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;In James Newman’s living room is a monument to our kind of crazy.&lt;/strong&gt; Seven 2-meter-tall panels are covered with circuit boards festooned with blinking lights. It is a complete working CPU, plus 256 bytes of RAM and an input/output interface, all built out of 42,300 hand-soldered discrete transistors.&lt;/p&gt;
&lt;p&gt;It took Newman some four years and roughly £40,000 sterling (about US $52,500) to build and design the so-called &lt;a shape="rect" href="http://www.megaprocessor.com/"&gt;Megaprocessor&lt;/a&gt; in his home in Cambridge, England. In some absinthe-tinged sense, the project is the logical end point of his software writing career. Newman explains that, over the years, he found himself doing increasingly low-level programming, and so became more and more interested in the underlying hardware. He says he began working on the Megaprocessor because he “wanted to build a processor to see how a real one worked.”&lt;/p&gt;
&lt;p&gt;The end result is a surprisingly full-featured machine. It’s 16-bit, with 8-bit opcodes, and basic &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Instruction_pipelining"&gt;pipelining&lt;/a&gt;, where the next instruction is fetched as the first is executed. All &lt;a shape="rect" href="http://www.megaprocessor.com/instruction_set.pdf"&gt;256 possible instructions&lt;/a&gt; are implemented, including ones for relatively advanced math functions. There are four general-purpose registers in addition to a program counter, stack pointer, and status register. “I could have built a minimal processor, but I don’t know what that would have showed,” says Newman. He admits though that he did “get carried away a little bit. I quite liked investigating how to do multiply, divide, and square root.”&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgxMDkzMQ.jpeg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDkzMQ.jpeg "/&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: James Newman&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Full Frame:&lt;/strong&gt; The Megaprocessor is built on seven frames.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Subsections of the processor—such as binary adders—are built from custom-made printed circuit boards. Each logic gate indicates the state of its inputs and outputs with red LEDs, and every memory cell similarly uses an LED to indicate its contents (driving the LEDs is actually responsible for most of the approximately 500 watts the Megaprocessor consumes). The boards are wired together on frames, with each frame making up a major system, such as the arithmetic logic unit or the instruction decoder. The CPU clock can be halted at any point or varied between about 1 hertz and 8 kilohertz (fast enough to play &lt;em&gt;Tetris&lt;/em&gt;), and an optional 32 kilobytes of external memory (implemented using boring old integrated circuits) is accessible via the input/output interface.&lt;/p&gt;
&lt;figure class="stacked lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDkzMw.jpeg"/&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMDkzMg.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photos: James Newman&lt;/figcaption&gt;
&lt;figcaption&gt;Each frame holds a major subsystem [top] such as the arithmetic logic unit or the instruction decoder. Subsystems are built from custom-printed circuit boards wired together. Transistors and LEDs are soldered into each board so that the inputs and outputs of each logic gate are indicated [bottom].&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In part, the relative complexity of the processor was a trade-off intended to reduce the amount of effort required for the construction of the internal memory. You “don’t have the whole picture unless you show what is going on in the memory,” says Newman, “but I quickly realized that building the memory was going to be as painful as building the processor. That kind of pushed toward increasing the capability of the processor, so I didn’t need as much program data.” (The memory accounts for 64 percent of the Megaprocessor’s transistor count.)&lt;/p&gt;
&lt;p&gt;The biggest setback in building the Megaprocessor came right at the end of the assembly. Newman ordered a last batch of about 4,000 transistors. “I ordered the right ones, and the packing list said they were the ones I wanted, and they looked like the ones I wanted. So I used them all.” But when Newman began testing the completed boards, nothing worked. “It took a while before I realized the full horror of what had happened,” he says: The final batch had been the wrong kind of transistor. “I wasted about a month of tedious soldering. It was bad enough having to do that again, but it was worse because I felt I’d &lt;em&gt;finished&lt;/em&gt; the soldering,” says Newman. “Yes, we can laugh &lt;em&gt;now&lt;/em&gt;, but it was a bad day.”&lt;/p&gt;
&lt;p&gt;Newman is currently in discussions about finding a permanent home for the Megaprocessor, where the general public will be able to view it without requiring an invitation to his house. In the meantime, if you want to try your hand at programming this masterpiece of transistorized glory, you can &lt;a shape="rect" href="http://www.megaprocessor.com/programming.html"&gt;download a simulator&lt;/a&gt; for the Windows operating system from &lt;a shape="rect" href="http://www.megaprocessor.com"&gt;http://www.megaprocessor.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the October 2016 print issue as “&lt;span&gt;The Megaprocessor&lt;/span&gt;.”&lt;/em&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 23 Sep 2016 19:00:00 GMT</pubDate>
<dc:creator>Stephen Cass</dc:creator>
<guid>http://spectrum.ieee.org/geek-life/profiles/the-megaprocessor-laughs-at-your-puny-integrated-circuits</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxMDkzMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxMDkyOA.jpg" height="225" width="300"/>
</item>
<item>
<title>Clinton Calls for Tech Firms to Fight Terrorism, but Students Could Be the Key</title>
<link>http://spectrum.ieee.org/view-from-the-valley/at-work/education/clinton-calls-for-tech-firms-to-fight-terrorism-but-hacking-for-defense-students-could-move-faster</link>
<description>Stanford’s “Hacking for Defense” class to be offered at 13 universities next year; U.S. Defense Department gets ready to send students classified problems</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Stanford’s “Hacking for Defense” class to be offered at 13 universities next year; U.S. Defense Department gets ready to send students classified problems&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxMzUyOA.jpeg"/&gt;
&lt;figcaption&gt;Photo: Steve Blank&lt;/figcaption&gt;
&lt;figcaption&gt;Stanford faculty member Steve Blank addresses educators who will roll the Hacking for Defense class out to 13 universities around the United States; students will tackle cyberterrorism and other real-world challenges.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;This week, U.S. presidential candidate Hillary Clinton called for “tech companies and experts online” to give the United States tools to intercept and prevent online recruiting efforts by terrorist organizations. &lt;a shape="rect" href="http://www.latimes.com/nation/politics/trailguide/la-na-trailguide-updates-clinton-response-to-terror-targets-1474299589-htmlstory.html"&gt;According to the &lt;em&gt;Los Angeles Times&lt;/em&gt;
&lt;/a&gt;, Clinton “made clear that she would put pressure on the tech firms to step up their efforts to root out terrorists.”&lt;/p&gt;
&lt;p&gt;Clinton sees tech firms as the place to go for the kind of technology the United States needs. But let me suggest what might be a better source: tech students.&lt;/p&gt;
&lt;p&gt;Last spring, I attended &lt;a shape="rect" href="http://spectrum.ieee.org/view-from-the-valley/at-work/education/hacking-for-defense-class-at-stanford-aims-to-begin-a-tech-rotc"&gt;the final presentations of the first-ever “Hacking for Defense” class&lt;/a&gt;, in which teams of Stanford University students were given real-world problems by the U.S. &lt;a shape="rect" href="https://www.nsa.gov/"&gt;National Security Agency&lt;/a&gt;, the &lt;a shape="rect" href="http://www.arcyber.army.mil/"&gt;U.S. Army Cyber Command&lt;/a&gt;, and other branches of government. The members of each team had 10 weeks to bring themselves up to speed on a real-world national security or defense related problem, and begin working towards a solution. The progress most teams made was impressive; many pledged to keep going in developing the technologies they had proposed.&lt;/p&gt;
&lt;p&gt;Since then, this little Stanford class of 40 people has started a movement. According to Steve Blank, one of the faculty members behind the effort, the organizers of the class held a three-day workshop for other educators looking to teach the class at their schools. Thirteen universities have decided to offer the course—with funding provided &lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://www.ndu.edu/"&gt;by the National Defense University&lt;/a&gt;—in the next year. The Stanford group has also obtained commitments from government sponsors to keep sending challenging problems to the students, and worked out a way for the Department of Defense to include classified problems in the mix.&lt;/p&gt;
&lt;p&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="https://steveblank.com/2016/09/19/the-innovation-insurgency-scales-hacking-for-defense-h4d/"&gt;Blank said in a blog post&lt;/a&gt;
&lt;span&gt; &lt;/span&gt;that, as a result of the meetings and workshops, and what they learned during the pilot class, the faculty has fine-tuned the methodology to be more specific about what students deliver to sponsors. This, of course, requires that sponsors actually define the metrics for success in addressing a problem early on. The course developers are also planning to make sure that the government sponsors give teams big-picture briefings about how the sponsoring organization operates, how it’s funded, its overall budget, and general issues the agency or group is facing.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 23 Sep 2016 14:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/at-work/education/clinton-calls-for-tech-firms-to-fight-terrorism-but-hacking-for-defense-students-could-move-faster</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxMzU0MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxMzUzOQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Tesla's Massive New Autopilot Update Is Released, Promising Safer Driving</title>
<link>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/teslas-massive-new-autopilot-update-is-released</link>
<description>Autopilot 8.0 makes more use of cars' existing radar and works harder to keep the driver focused on the road</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Autopilot 8.0 makes more use of cars' existing radar and works harder to keep the driver focused on the road&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxMjg3NA.jpeg"/&gt;
&lt;figcaption&gt;Photo: Tesla Motors&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;A long-heralded update to Tesla Motor’s Autopilot has just been made available for download. First reports suggest that it’s as big a change in the semiautonomous driving system as Tesla CEO &lt;a shape="rect" href="https://twitter.com/elonmusk"&gt;Elon Musk &lt;/a&gt;had promised.&lt;/p&gt;
&lt;p&gt;One key element of the upgrade is making more use of the car’s existing radar capabilities, both to perceive the road in real time and to map it so that subsequent Tesla cars can distinguish earlier fixed features from new, perhaps threatening ones. Another key element is saving drivers from over-dependence on the software.&lt;/p&gt;
&lt;p&gt;Either of those points might have saved the Tesla owner &lt;a shape="rect" href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/tesla-autopilot-crash-why-we-should-worry-about-a-single-death"&gt;who died last May&lt;/a&gt; when his Autopilot, apparently unsupervised by the driver, drove into the side of a tractor trailer. That is the first fatality known to have been caused by a modern robotic driving system.&lt;/p&gt;
&lt;p&gt;“We believe it would have seen a large metal object across the road,” &lt;a shape="rect" href="http://www.bloomberg.com/news/articles/2016-09-11/tesla-alters-autopilot-to-emphasize-radar-images-over-cameras"&gt;Musk said&lt;/a&gt; in a conference call earlier this month, referring to the trailer. “Knowing that there is no overhead road sign there, it would have braked.”&lt;/p&gt;
&lt;p&gt;(Another Tesla driver died in China in January, in a case &lt;a shape="rect" href="http://www.scmp.com/business/companies/article/2021356/father-chinese-man-killed-tesla-crash-files-lawsuit-over"&gt;now under litigation&lt;/a&gt; there, but it isn’t clear whether the Autopilot was operating at the time of the crash.)&lt;/p&gt;
&lt;p&gt;Tesla’s preference for radar over lidar, the laser-ranging equivalent, makes the company a little unusual in autoland. Lidar has far better resolution—unlike radar it can see road markings and make out the shapes of signs and other things even at a distance.&lt;/p&gt;
&lt;p&gt;Radar, however, is cheaper, more compact, and far better at seeing through rain and snow. And Tesla needs this immediate practicality because it’s incrementally raising the capability of its cars’ “advanced driver assistance systems,” or ADAS, to a fully self-driving level. By contrast, &lt;a shape="rect" href="http://spectrum.ieee.org/image/MjUwNTkyNA.jpeg"&gt;Google&lt;/a&gt;, &lt;a shape="rect" href="http://spectrum.ieee.org/image/MjgwOTUxMQ.jpg"&gt;Ford&lt;/a&gt; and &lt;a shape="rect" href="http://spectrum.ieee.org/image/MjgwMjI0NQ.jpg"&gt;Uber&lt;/a&gt; are aiming to produce a fully robotic car in one fell swoop. They now festoon their experimental cars with lidar in the expectation that it will become cheaper, smaller and more capable by the time that car is ready, five years (at least) from now.&lt;/p&gt;
&lt;p&gt;Tesla’s Autopilot 8.0 goes further than ever to keep the driver’s eyes on the road. For instance, it will sound the alarm if your hand’s off the wheel, then does it with increasing insistance until, after the third time, the Autopilot will disengage for the remainder of the trip. &lt;/p&gt;
&lt;p&gt;How far these changes will go to prevent accidents, small and large, remains to be seen. For now, though, the select reviewers who have been beta-testing the car say that it certainly drives in less machine-like way.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“It’s only human to want to give the truck a little more space and hug the outer edge of the lane,” &lt;a shape="rect" href="http://www.bloomberg.com/news/features/2016-09-22/tesla-drivers-wake-up-to-a-serious-upgrade"&gt;writes Tom Randall&lt;/a&gt; in &lt;em&gt;Bloomberg News&lt;/em&gt;. “With the upgrade, the car is beginning to act a little more human, adjusting its position in the lane to account for perceived threats from the sides.” &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;One ho-hum aspect of today’s upgrade would once have been the most striking thing of all: it’s all done through an over-the-air download. Tesla &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="https://forums.tesla.com/it_IT/forum/forums/first-overtheair-firmware-update"&gt;pioneered&lt;/a&gt;
&lt;span&gt; this trick, and now other automakers are &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://www.autonews.com/article/20160125/OEM06/301259980/over-the-air-updates-on-varied-paths"&gt;following suit&lt;/a&gt;
&lt;span&gt;. Here Tesla has a built-in advantage over other car makers: it sells cars direct to the public, so upgrades can go straight to the customer without alienating a dealer network.&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 22 Sep 2016 15:48:00 GMT</pubDate>
<dc:creator>Philip E. Ross</dc:creator>
<guid>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/teslas-massive-new-autopilot-update-is-released</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxMjg5OA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxMjg5Ng.jpg" height="225" width="300"/>
</item>
<item>
<title>3D-Printed Plastic Blocks Generate Complex Acoustic Holograms</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/3d-printed-plastic-blocks-generate-complex-acoustic-holograms</link>
<description>Instead of an ultrasonic transducer array, you can now make tractor beams with some plastic and a 3D printer</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Instead of an ultrasonic transducer array, you can now make tractor beams with some plastic and a 3D printer&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgxMjI3OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Kai Melde&lt;/figcaption&gt;
&lt;figcaption&gt;A underwater acoustic hologram projects a pair of rings onto the water's surface&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;The closest thing we have right now to a &lt;em&gt;Star Trek–&lt;/em&gt;style tractor beam is a technology based on moving small objects with sound. Last year, &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/hardware/acoustic-holograms-form-ultrasonic-tractor-beams-for-tiny-objects"&gt;researchers from the Public University of Navarre, in Spain, demonstrated how ultrasonic acoustic holograms can be used to manipulate things in midair&lt;/a&gt;, using arrays of ultrasonic transducers and some reasonably complicated modeling and programming. The overall complexity of the acoustic hologram—a &lt;span&gt;3D structure made of sound&lt;/span&gt;—that you can create in this way (and consequently what you can do with it) is limited primarily by the characteristics of your transducer array, and because transducers can only get so small, this is a significant limitation.&lt;/p&gt;
&lt;p&gt;An acoustic hologram manifests itself as variations in air pressure. You can make pressure structures like vortices and bottles, which can trap small, lightweight things in areas of low pressure inside areas of high pressure. Creating the structures involves the creation of a sound field where a bunch of different sound waves of varying amplitudes constructively and destructively interfere in just the right way to make exactly the structure that you want. One way to do this is with &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/hardware/acoustic-holograms-form-ultrasonic-tractor-beams-for-tiny-objects"&gt;an array of individual transducers&lt;/a&gt;, each one emitting a slightly different sound wave. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;But in &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://www.nature.com/articles/doi:10.1038/nature19755"&gt;a paper published in &lt;em&gt;Nature&lt;/em&gt; this week&lt;/a&gt;
&lt;span&gt;, a team from the &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://www.is.mpg.de/en"&gt;Max Planck Institute for Intelligent Systems&lt;/a&gt;,&lt;span&gt; in Germany, describe a new way of easily creating very high resolution acoustic holograms that work in air or water. Rather than relying on a whole bunch of small transducers, they use just one giant transducer instead. It sits&lt;/span&gt;
&lt;span&gt; underneath a special 3D-printed transmission hologram made out of finely contoured plastic.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://www.nature.com/articles/doi:10.1038/nature19755"&gt;In this new research&lt;/a&gt;, the single transducer emits one type of sound wave, which means that you can't use it to create a sound field that'll do all that much for you. What the researchers realized, however, is that the only important thing is finding a way to generate all of those different sound waves—which you can do without more than one transducer if you're clever and willing to make a few compromises.&lt;/p&gt;
&lt;p&gt;The trick is to use a 3D-printed hunk o’ plastic. Or, to get needlessly technical, a “3D-printed monolithic element.” Or, to get less needlessly technical, “a finely contoured solid plastic block.” The block is attached to the transducer, and when it transduces, the sound wave has to propagate through the block before it's emitted into space. Since sound moves slower through the block than it does through air or water, by carefully printing the top of the block in a very specific pattern to vary its thickness, you get the same effect as using a transducer array made up of individual transducers that are each as small as the resolution of your 3D printer. This allows the creation of acoustic holograms that are about 100 times as detailed as anyone has been able to make before. &lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgxMjI3OA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Images: Kai Melde&lt;/figcaption&gt;
&lt;figcaption&gt;The acoustic hologram at the upper right is able to generate the image of a bird.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In common use, the word “hologram” refers to a structured field of light or sound, but it can also refer to the spatial storage of a wave front in a way that allows the characteristics of that wave front to be reconstructed when you pump some energy back through the storage medium. In this particular case, the “hologram” is the plastic block, which converts a sound wave from a single transducer into a complex sound field. &lt;/p&gt;
&lt;p&gt;The obvious downside with this technique is that you can't dynamically change the acoustic field, since it's fixed by the 3D-printed plastic plate. You can set up acoustic fields that move objects, but they'll move along a fixed path. One potential workaround is 3D printing a single hologram that encodes multiple sound fields at different frequencies. That would allow you to dynamically select the hologram by twiddling the transducer.&lt;/p&gt;
&lt;p&gt;In terms of practical applications, the researchers mention super-resolution imaging, selective heating, and personalized medicine. But what's most relevant here is the fact that it's now very quick, easy, and cheap to create ultrahigh resolution acoustic hologram-based tractor beams for that spaceship that I'm totally not secretly building in my garage.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 21 Sep 2016 17:00:00 GMT</pubDate>
<dc:creator>Evan Ackerman</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/3d-printed-plastic-blocks-generate-complex-acoustic-holograms</guid>
<media:content url="http://spectrum.ieee.org/image/MjgxMjI5NQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgxMjI5Mw.jpg" height="225" width="300"/>
</item>
<item>
<title>Monkey Types 12 Words Per Minute With Brain-Computer Interface</title>
<link>http://spectrum.ieee.org/the-human-os/biomedical/bionics/monkeys-type-12-words-per-minute-with-braintokeyboard-communication</link>
<description>The brain-computer interface is already being tested in humans with Lou Gehrig's disease</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The brain-computer interface is already being tested in humans with Lou Gehrig's disease&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwODcyMg.jpeg"/&gt;
&lt;figcaption&gt;Image: Stanford University&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;“To be or not to be. That is the question.” That is also the text that Monkey J typed out using a brain implant to control a computer cursor. &lt;/p&gt;
&lt;p&gt;To be clear, the monkey didn’t know it was copying Shakespeare, and it had no deep thoughts about Hamlet’s &lt;a shape="rect" href="http://www.monologuearchive.com/s/shakespeare_001.html"&gt;famous monologue&lt;/a&gt;. Monkey J and its colleague, Monkey L, were both trained to use their neural implants to move a cursor over a computer screen, hitting circles as they turned green. Stanford University researchers placed letters on those targets to simulate the typing task. So to tap out the line from &lt;em&gt;Hamlet&lt;/em&gt;, first the “T” circle was illuminated, then the “O,” and so on.&lt;/p&gt;
&lt;p&gt;What was the point of this exercise? Was it simply an excuse to let journalists trot out the “&lt;a shape="rect" href="https://en.wikipedia.org/wiki/Infinite_monkey_theorem"&gt;infinite monkey theorem&lt;/a&gt;”? Because here we go: This probability theorem states that if you give a monkey a typewriter and infinite time, its random keystrokes will eventually produce the complete works of Shakespeare. (If you’d rather goof off than read about science, please enjoy &lt;a shape="rect" href="https://www.google.com/search?q=infinite+monkeys&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ved=0ahUKEwjioPbLhYrPAhXMPB4KHdFtBWEQ_AUICigD&amp;amp;biw=1304&amp;amp;bih=645#tbm=isch&amp;amp;q=infinite+monkeys+shakespeare+cartoon"&gt;these excellent cartoons&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;No, the bioengineers had a more practical motivation. By simulating this typing task, they demonstrated that their brain-computer interface could greatly benefit people who can’t communicate otherwise. That category includes people in the late stages of &lt;a shape="rect" href="http://www.alsa.org/about-als/what-is-als.html"&gt;amyotrophic lateral sclerosis&lt;/a&gt; (ALS), also known as Lou Gehrig’s disease, which leaves the mind intact but gradually paralyzes the body, including the mouth and other face muscles.&lt;/p&gt;
&lt;div style="page-break-after: always"/&gt;
&lt;p&gt;This experiment set a new record for typing-by-mind, with one monkey tapping out 12 words per minute. “To our knowledge, this is the highest communication level ever achieved,” says &lt;a shape="rect" href="https://npl.stanford.edu/~paul/"&gt;Paul Nuyujukian&lt;/a&gt;, a researcher at Stanford’s &lt;a shape="rect" href="http://med.stanford.edu/neurosurgery/research/NPTL.html"&gt;Neural Prosthetics Translational Lab&lt;/a&gt;. Nuyujukian is coauthor of &lt;a shape="rect" href="http://ieeexplore.ieee.org/document/7564474/"&gt;the paper&lt;/a&gt; describing this research, published today in &lt;em&gt;Proceedings of the IEEE&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;iframe frameborder="0" height="349" scrolling="auto" allowfullscreen="" width="620" src="https://www.youtube.com/embed/nxD2KDq18_E?rel=0"/&gt;
&lt;/p&gt;
&lt;p&gt;Here's how it works: The monkeys had tiny electrode arrays implanted in their brains, specifically in the part of the &lt;a shape="rect" href="http://thebrain.mcgill.ca/flash/d/d_06/d_06_cr/d_06_cr_mou/d_06_cr_mou.html"&gt;motor cortex&lt;/a&gt; that controls arm movements. Those electrodes measured the electrical activity of neurons while the monkeys were trained at the cursor-control task, first moving their actual arms while cameras carefully tracked the movements. Machine-learning algorithms found patterns in the stream of data, and translated those patterns into a monkey’s intent to move the cursor left, right, up, and down.&lt;/p&gt;
&lt;div class="imgWrapper rt med"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwODc3MQ.jpeg"/&gt;
&lt;/div&gt;
&lt;p&gt;Then the monkeys were set to the task of moving the cursor with their minds alone. (They could still make the movements in the air with their arms, but they weren’t being tracked.) The computer picked up the same patterns in the brain data, and the cursor moved smoothly from target to target.&lt;/p&gt;
&lt;p&gt;The prior record for brain typing, set by human patients with ALS last year, was &lt;a shape="rect" href="http://spectrum.ieee.org/the-human-os/biomedical/bionics/neural-implant-enables-paralyzed-als-patient-to-type-six-words-per-minute"&gt;6 words per minute&lt;/a&gt;. That experiment was done by a larger group of researchers, including Nuyujukian, who are part of the &lt;a shape="rect" href="http://www.braingate.org/"&gt;BrainGate&lt;/a&gt; consortium. The big improvement from that prior study came from software; the system used by the monkeys employed two smart algorithms in tandem, one to decode the &lt;a shape="rect" href="http://www.nature.com/neuro/journal/v15/n12/full/nn.3265.html"&gt;cursor’s movement&lt;/a&gt;, the other to decode the monkey’s &lt;a shape="rect" href="http://ieeexplore.ieee.org/document/7497003/?arnumber=7497003"&gt;intent to click&lt;/a&gt; on a letter. &lt;/p&gt;
&lt;p&gt;And there’s plenty more room for improvement, &lt;span&gt;Nuyujukian tells &lt;em&gt;IEEE Spectrum&lt;/em&gt; in an interview. The monkeys used an interface in which they picked one letter at a time, he says, but future interface could borrow some tricks from smartphones. “I c&lt;/span&gt;an imagine a smart interface that’s auto-completing the words,” he says. “Google and Apple have done a lot of work on how to maximize the input from our very inaccurate thumbs. We can leverage a lot of that.”&lt;/p&gt;
&lt;p&gt;Human trials of the technology are already underway. In one experiment, which the researchers &lt;a shape="rect" href="http://www.abstractsonline.com/plan/ViewAbstract.aspx?cKey=717decbc-7c8b-47c5-a024-170ada5a2847&amp;amp;mID=3744&amp;amp;mKey=d0ff4555-8574-4fbb-b9d4-04eec8ba0c84&amp;amp;sKey=c82a451a-073f-4250-aca1-e0d00b2f9192"&gt;presented at a conference&lt;/a&gt; last year, a woman with ALS used the cursor-control system with an Android tablet, using it to browse the web and write emails. Such human studies make it clear that the researchers aren’t just trying to generate a wave of jokes about Shakespearean monkeys. They’re trying to give paralyzed people their autonomy and the ability to speak their minds. &lt;/p&gt;
&lt;p&gt;But the researchers aren’t above having a little fun themselves. Nuyujukian divulged some of the early phrases they had the monkeys type out as they tested the system: “A banana, a banana, my kingdom for a banana!,” and “A banana by any other name would smell as sweet.” &lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 12 Sep 2016 22:00:00 GMT</pubDate>
<dc:creator>Eliza Strickland</dc:creator>
<guid>http://spectrum.ieee.org/the-human-os/biomedical/bionics/monkeys-type-12-words-per-minute-with-braintokeyboard-communication</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwODczOA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwODczNg.jpg" height="225" width="300"/>
</item>
<item>
<title>New System Could Break Bottleneck in Microprocessors</title>
<link>http://spectrum.ieee.org/tech-talk/semiconductors/processors/new-circuits-break-bottleneck-in-microprocessors</link>
<description>Turning burdensome software into simple hardware speeds core-to-core communication in microprocessors</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Turning burdensome software into simple hardware speeds core-to-core communication in microprocessors&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwODIwNg.jpeg"/&gt;
&lt;figcaption&gt;Photo: Intel&lt;/figcaption&gt;
&lt;figcaption&gt;Intel and other multicore processor makers want cores to be able to communicate with each other faster. More cores (the Haswell-EX Xeon E7-8890 V3 shown here has 18) typically means much more time coordinating communications.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Engineers at North Carolina State University and at Intel have come up with a solution to one of the modern microprocessor’s most persistent problems: communication between the processor’s many cores. Their answer is a dedicated set of logic circuits they call the Queue Management Device, or QMD. In simulations, integrating the QMD with the processor’s on-chip network, at a minimum, doubled core-to-core communication speed, and in some cases, boosted it much farther. Even better, as the number of cores was increased, the speed-up became more pronounced.&lt;/p&gt;
&lt;p&gt;In the last decade, microprocessor designers started putting &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/processors/multicore-cpu-processor-proliferation"&gt;multiple copies of processor cores on a single die&lt;/a&gt; as a way to continue the rate of performance improvement computer makers had enjoyed without chip-killing hot spots forming on the CPU. But that solution comes with &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/the-trouble-with-multicore"&gt;complications&lt;/a&gt;. For one, it meant that software programs had to be written so that work was divided among processor cores. The result: Sometimes different cores would need to work on the same data or have to coordinate the passing of data from one core to another.&lt;/p&gt;
&lt;aside class="inlay pullquote lt med"&gt; “We have to improve performance by improving energy efficiency. The only way to do that is to move some software to hardware”&lt;span class="pq-attrib"&gt;—Yan Solihin, North Carolina State University &lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;To prevent the cores from wantonly overwriting each other’s information, processing data out of order, or committing other errors, multicore processors use lock-protected software queues. These are data structures that &lt;span&gt;coordinate the movement of and access to information according to software-defined rules. &lt;/span&gt;But all that extra software comes with significant overhead, which only gets worse as the number of cores increases. “Communications between cores is becoming a bottleneck,” says&lt;a shape="rect" href="http://www.ece.ncsu.edu/people/solihin"&gt; Yan Solihin&lt;/a&gt;, a professor of electrical and computer engineering who led the work at NC State.&lt;/p&gt;
&lt;p&gt;The solution—born of a discussion with Intel engineers and executed by Solihin's student, Yipeng Wang, at NC State and at Intel—was to turn the software queue into hardware. This effectively turned three multistep software queue operations into three simple instructions—add data to the queue, take data from the queue, and put data near where it’s going to be needed next. Compared with just using the software solution, the QMD sped up a sample task such as packet processing—like network nodes do on the Internet—by a greater and greater amount the more cores were involved. For 16 cores, QMD worked 20 times as fast as the software could.&lt;/p&gt;
&lt;p&gt;Once they realized this result, the engineers reasoned that the QMD might be able to do a few other tricks—turning more software into hardware. They added more logic to the QMD and found that it could speed up several other core communications-dependent functions, including MapReduce, a technology Google pioneered for distributing work to different cores and collecting the results.&lt;/p&gt;
&lt;p&gt;They aren’t done yet. “The next step is to figure out other types of hardware accelerators that would be useful,” says Solihin. “We have to improve performance by improving energy efficiency. The only way to do that is to move some software to hardware. The challenge is to figure out which software is used frequently enough that we could justify implementing it in hardware. There is a sweet spot,” he says.&lt;/p&gt;
&lt;p&gt;Intel engineer Ren Wang is presenting the QMD speed-up results at the &lt;a shape="rect" href="http://pactconf.org/"&gt;25th Annual Conference on Parallel Architectures and Compilation Techniques&lt;/a&gt;, in Haifa, Israel this week.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 12 Sep 2016 12:38:00 GMT</pubDate>
<dc:creator>Samuel K. Moore</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/semiconductors/processors/new-circuits-break-bottleneck-in-microprocessors</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwODIxNw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwODIxNQ.jpg" height="225" width="300"/>
</item>
<item>
<title>A Chip-Scale Source for Quantum Random Number Generators</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/a-chip-scale-source-for-quantum-random-number-generators</link>
<description>Researchers build a photonic integrated circuit that could one day give smartphone security true random numbers</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Researchers build a photonic integrated circuit that could one day give smartphone security true random numbers&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNzQ1Mw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Daniel Bartolome and Ona Bombí/ICFO&lt;/figcaption&gt;
&lt;figcaption&gt;Two quantum random number sources were built on this 6 mm x 2 mm photonic integrated circuit, which is juxtaposed against a 1-cent euro coin.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Taking advantage of technology developed to manipulate light on chips, a team based in Spain and Italy has created an integrated circuit that can be used to generate true random numbers by taking advantage of the thoroughly unpredictable nature of quantum mechanics.&lt;/p&gt;
&lt;p&gt;The compact approach, which might one day find its way into smartphones and tablets, could be a boon for engineers hoping to keep &lt;span&gt;financial transactions and other communications secure&lt;/span&gt;. Random numbers are a vital ingredient in the encryption schemes we rely on to secure data, and they’re also a powerful tool in computational modeling. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Today’s conventional random number generation is done using computer algorithms or physical hardware. A chip-based random number generator can, for example, use analog or digital circuits that are &lt;a shape="rect" href="http://spectrum.ieee.org/computing/hardware/behind-intels-new-randomnumber-generator"&gt;sensitive to random thermal fluctuations&lt;/a&gt; to generate unpredictable strings. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;But even if these sources look quite random, it’s practically impossible to prove they are perfectly so, explains &lt;/span&gt;
&lt;a style="color: rgb(3, 166, 227); font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="https://www.icfo.eu/research/84-group-member-details.html?gid=28&amp;amp;people_id=168"&gt;Valerio Pruneri&lt;/a&gt;
&lt;span&gt; of the Institute of Photonic Sciences in Spain. If you wait long enough—perhaps far longer than you’d care to wait—you may ultimately find there are correlations between numbers, ones that would ultimately allow you to crack the random-number-generation scheme. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Systems that obey the rules of quantum mechanics, by contrast, could be impossible nuts to crack. &lt;span&gt;“Q&lt;/span&gt;
&lt;span id="docs-internal-guid-65aec9a1-0615-95f6-8fe0-ac19b4ec3db0"&gt;uantum physics, by definition, is fully unpredictable no matter what,” Pruneri&lt;/span&gt;
&lt;span&gt; says. “&lt;/span&gt;
&lt;span id="docs-internal-guid-65aec9a1-0615-95f6-8fe0-ac19b4ec3db0"&gt;There is no way that somebody can guess future numbers based on current information.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Quantum random number generators are nothing new; there are even &lt;a shape="rect" href="http://www.idquantique.com/random-number-generation/"&gt;commercial&lt;/a&gt;
&lt;a shape="rect" href="https://comscire.com/"&gt;systems&lt;/a&gt; available. &lt;/span&gt;But Pruneri and his colleagues decided to take aim at portability. They wanted to create something that could spit out random numbers at a high rate, but be small and &lt;span&gt;energy-efficient enough that it could ultimately be integrated with microelectronics—perhaps in a package small enough to fit in &lt;/span&gt;a smartphone or tablet.&lt;/p&gt;
&lt;p&gt;The chip they created takes advantage of standard fabrication techniques used to construct &lt;span&gt;photonic integrated circuits.&lt;/span&gt; A small, pulsed indium phosphide laser is responsible for infusing the system with randomness. Below a certain energy threshold, a laser emits a small number of photons through a process called spontaneous emission, which creates light with random phase. This randomness impacts the ultimate phase of the light the laser emits when it’s above that threshold, once stimulated emission starts to dominate, Pruneri explains. The result is that, pulse to pulse, the laser light will have a random phase.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;To transform these random phases into something usable, the pulsed light is mixed with light from a second indium phosphide laser on the chip. The phase of the first laser’s pulses will ultimate impact how light from the two laser sources interfere with one another, creating certain brightness differences that can be read out by a photodetector. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;This quantum “entropy source” can be used to produce random numbers at a good clip—in the realm of a gigabit per second. The work appears online today in the &lt;a shape="rect" href="http://dx.doi.org/10.1364/OPTICA.3.000989"&gt;journal &lt;/a&gt;
&lt;/span&gt;
&lt;em&gt;
&lt;a shape="rect" href="http://dx.doi.org/10.1364/OPTICA.3.000989"&gt;Optica&lt;/a&gt;. &lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span id="docs-internal-guid-65aec9a1-0621-ac45-f161-e57ad2c35d36"&gt;
&lt;span&gt;Pruneri says the next step is to integrate the chip with conventional CMOS electronics to turn the output of the system into random numbers that can be used by software. Here too, he expects the team will take advantage of photonics integrated circuit manufacturing techniques that have been built up over the years, in particular a way of pairing silicon and other materials, called &lt;/span&gt;
&lt;/span&gt;
&lt;span id="docs-internal-guid-65aec9a1-0621-ac45-f161-e57ad2c35d36"&gt;
&lt;span&gt;
&lt;a shape="rect" href="https://www.ll.mit.edu/about/integrated-photonics/hybrid-pics.html"&gt;hybrid integration&lt;/a&gt;.&lt;/span&gt;
&lt;/span&gt;
&lt;/p&gt;
&lt;div/&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 8 Sep 2016 14:30:00 GMT</pubDate>
<dc:creator>Rachel Courtland</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/a-chip-scale-source-for-quantum-random-number-generators</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNzQ3Mw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNzQ3MQ.jpg" height="225" width="300"/>
</item>
<item>
<title>The Two-Camera iPhone 7 Plus Is Just the Beginning of the Camera Count Competition</title>
<link>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/audiovideo/apples-2camera-iphone-7-plus-is-just-the-beginning-of-the-camera-count-competition</link>
<description>Apple dips into the multicamera waters, while startup Light gets ready to ship a 16-camera device</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Apple dips into the multicamera waters, while startup Light gets ready to ship a 16-camera device&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNzU4OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Apple&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;At today’s Apple launch event,  senior vice president Phil Schiller explained two gee-whiz features of Apple’s iPhone 7 Plus—a zoom that doesn’t sacrifice picture quality up to a factor of two, and an ability to create a depth of field effect in which foreground images are clear while the background is blurred. Both of these features are basic requirements for serious photographers. The secret to incorporating them into a small mobile device is stitching together the images captured by multiple camera modules with different focal lengths.&lt;/p&gt;
&lt;p&gt;Apple is using two camera modules in the iPhone 7 Plus: a standard 25-millimeter wide-angle equivalent along with a 66-mm telephoto equivalent.&lt;/p&gt;
&lt;p&gt;This means that images zoomed up to 66 mm don’t lose pixels, because the 66-mm lens uses the full 12 megapixels of its image sensor. (For zoom levels between 25 and 66 mm software interpolates the images.)&lt;/p&gt;
&lt;p&gt;Apple &lt;a shape="rect" href="http://www.theverge.com/2016/4/11/11406098/lg-g5-huawei-p9-two-camera-smartphone-trend-apple"&gt;isn’t the first&lt;/a&gt; smartphone maker to use multiple camera modules—Huawei introduced a phone with a monochrome and a color camera module, both wide angle, to improve image quality and do some rough simulation of depth of field; LG, like Apple, took on zoom.&lt;/p&gt;
&lt;p&gt;The iPhone 7 Plus will likely be the first mobile device out of the gate to do both. I say likely, because Schiller indicated that that feature will not be ready for the phones’ 16 September ship date, and will come as an update later this year, no specific date promised.&lt;/p&gt;
&lt;p&gt;Meanwhile, &lt;a shape="rect" href="https://www.light.co/camera"&gt;startup camera company Light&lt;/a&gt; concluded some time ago that multiple camera modules are the way to do zoom, improve image quality, and allow selective focusing in a digital device, but isn’t stopping at two. The company announced its plan almost a year ago and showed prototypes at CES last January. It will be shipping its 16-camera module Android-based gadget early in 2017, complete with optical zoom to 150 mm and blur at any depth of field, not just background.&lt;/p&gt;
&lt;p&gt;We’re done with one-upmanship on how big a camera screen can be (too big, it turns out, for many users). Battery life, a continuing issue, isn’t seeing any major breakthroughs anytime soon. Resolution is going beyond what the eye can distinguish. Phone makers clearly were ready for another quantifiable feature to distinguish their products, and make phone users willing to upgrade, and upgrade again.&lt;/p&gt;
&lt;p&gt;So get ready to start counting cameras.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 7 Sep 2016 21:30:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/audiovideo/apples-2camera-iphone-7-plus-is-just-the-beginning-of-the-camera-count-competition</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNzYwMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNzU5OA.jpg" height="225" width="300"/>
</item>
<item>
<title>Modern Airports Offer No Easy Way Out for Panicking Crowds</title>
<link>http://spectrum.ieee.org/tech-talk/computing/software/modern-airports-have-no-easy-exit-for-panicking-crowds</link>
<description>Simulations have highlighted the flaws of modern airport layouts when it comes to enabling emergency evacuations</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Simulations have highlighted the flaws of modern airport layouts when it comes to enabling emergency evacuations&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNzA0Mw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Wang Ying/Xinhua/Getty Images&lt;/figcaption&gt;
&lt;figcaption&gt;People wait for the reopening of the security checkpoint at the JFK International Airport in New York, on 15 August 2016. All flights from the airport were suspended due to reports of shooting inside a terminal.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;False reports of shooters at two of the busiest U.S. airports—one in New York City and one in Los Angeles—left many passengers swept along by surging crowds of panicked people searching for the closest exits. The chaos was perhaps all too predictable, because computer simulations suggest that many major airports are terribly designed for emergency evacuations.&lt;/p&gt;
&lt;p&gt;Many airports are designed as long, narrow “pier” buildings with aircraft parked at gates running along both sides. That’s relatively efficient for getting large numbers of people loaded onto as many planes as possible—even if delayed passengers sometimes have to sprint to catch their flights at the far end of a terminal building. But the pier design presents huge problems for people who are trying to get to traditional exits during emergencies. During a 15 August incident, crowds panicked by &lt;a shape="rect" href="http://nymag.com/daily/intelligencer/2016/08/the-terrifying-jfk-airport-shooting-that-wasnt.html"&gt;false reports of a shooter at John F. Kennedy International Airport&lt;/a&gt; in New York, ran out onto the tarmac where planes are typically parked.&lt;/p&gt;
&lt;p&gt;“Airports are built that way to get passengers through security and bag check and everything as quickly as possible, so that they have best customer service experience,” &lt;span&gt;says &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://mays.tamu.edu/field-trips/meet-the-professors-2/matthew-manley-information-technology/"&gt;Matthew Manley&lt;/a&gt;
&lt;span&gt;, clinical assistant professor in information and operations management at the Mays Business School of Texas A&amp;amp;M University. “&lt;/span&gt;But from an evacuation standpoint, it’s problematic because passengers might be at the end of a pier and would be required to evacuate across a very long distance and through hazards.”&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Busy airports usually cannot afford to conduct emergency drills with huge crowds of real paying passengers. That means real-life disasters or even false alarms can often leave airport crowds scrambling to figure out the safest course of action on their own. Still, computer simulations may provide some answers by showing how airport layouts and certain security measures impact crowd movements during emergency evacuations.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Manley worked with several colleagues at Utah State University to develop a crowd simulation, called Exitus, that showed how an especially vulnerable population—say, disabled individuals—would fare in emergency evacuations. They used Exitus to simulate several scenarios involving dirty bombs at Salt Lake City International Airport in Utah and published the results in the 20 November 2015 issue of the journal &lt;a shape="rect" href="http://ieeexplore.ieee.org/document/7332972/"&gt;
&lt;em&gt;IEEE Transactions on Systems, Man, and Cybernetics: Systems&lt;/em&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;The dozens of Exitus simulations, run with as many as six thousand simulated people, assumed a best-case situation with an orderly, controlled evacuation through the normal airport exits. But these “best-case” evacuation scenarios—&lt;span&gt;where the crowds, even without guidance, didn’t panic—still &lt;/span&gt;revealed big flaws in the traditional airport pier design found at Salt Lake City International Airport and most major airports.&lt;/p&gt;
&lt;p&gt;For example, the long corridors&lt;span&gt; inevitably&lt;/span&gt; led to pile-ups at escape routes such as stairways or narrow passageways leading back to the central airport hub and main exits. The evacuation became even slower when the researchers added a small group of disabled individuals to the simulated crowds. (&lt;span&gt;Exitus may be the first “agent-based&lt;/span&gt;
&lt;span&gt;” simulation to show how the presence of disabled individuals impacts emergency evacuations at an airport&lt;/span&gt;
&lt;span&gt;.)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Disabled people in wheelchairs, those with impaired eyesight, or elderly folks with lower stamina &lt;span&gt;tended to move the slowest and &lt;/span&gt;were at greatest risk. Just imagine your elderly grandparent trying to sprint down a long, crowded airport terminal, and you can get the idea. In addition, the slower progress of disabled individuals also slowed the overall crowd evacuation. &lt;/p&gt;
&lt;p&gt;One major impetus behind Manley and his colleagues’ decision to study this issue was the desire to find potential evacuation alternatives for the elderly and those with disabilities. The typical evacuation policy usually requires disabled individuals to “shelter in place” by staying in a designated refuge area until rescue personnel arrive.&lt;/p&gt;
&lt;p&gt;It may be a sign of how terrible airport designs are for mass evacuations that some airport security experts actually recommend the “shelter in place” strategy for everyone. Part of the advice to wait out the danger in a quiet restroom or similar area makes sense, at least from the standpoint of avoiding being swept up in panicking crowds running here and there. But it’s perhaps also symbolic of how modern airport design has failed passengers when it comes to providing escape routes from danger.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The researchers introduced another problem that set the airports’ design flaw in relief: a simulated a dirty bomb placed in locations that blocked some of the stairwells and passageways. That meant evacuating passengers had no choice but to cross through a potential danger zone as they exited on their way to the central airport terminal.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Such a scenario is not very different from a real-life incident that occurred at the Salt Lake City airport on 14 October 1989. A Boeing 727 parked at a gate near the intersection of a long concourse building and the central terminal caught fire. Passengers were forced to walk through smoke billowing from the fire as they exited toward the main airport terminal.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Again, keep in mind that these simulations represented best-case scenarios with no panicked crowds. The researchers talked with Salt Lake City International Airport officials to discover the established procedures for controlled evacuations, which do not usually call for passengers to run out onto busy airport tarmacs where planes and other potentially dangerous vehicles operate. But as the recent incidents at JFK Airport and &lt;a shape="rect" href="http://www.nbcnews.com/news/us-news/lax-scare-police-say-loud-noises-not-gunshots-caused-panic-n639191"&gt;LAX Airport&lt;/a&gt; have shown, people will flee perceived danger by any means necessary.&lt;/p&gt;
&lt;p&gt;“&lt;span&gt;Airports, unlike hotels, don’t have evacuation points, because they are supposed to be secure, so it’s not like you can easily get out,” Mike Ackerman, a travel security expert, told the &lt;a shape="rect" href="http://www.nytimes.com/2016/08/30/travel/how-to-stay-safe-at-the-airport-jfk-lax.html?_r=1"&gt;
&lt;em&gt;New York Times&lt;/em&gt;
&lt;/a&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;So what can be done to improve this sad state of affairs? Additional crowd simulation studies may point to better layouts for future airport designs. They could also help security officials map out better plans for dealing with crowd behavior during a crisis.&lt;/p&gt;
&lt;p&gt;For example, a &lt;a shape="rect" href="http://teamcore.usc.edu/papers/2011/Tsai-IVA.pdf"&gt;2011 study&lt;/a&gt; by researchers at the&lt;span&gt; University of Southern California in Los Angeles simulated how “emotional contagion” could spread fear or panic through a crowd. Their research was presented at the&lt;/span&gt; &lt;a shape="rect" href="http://link.springer.com/chapter/10.1007%2F978-3-642-23974-8_42#page-1"&gt;11th International Conference on &lt;/a&gt;
&lt;span&gt;
&lt;a shape="rect" href="http://link.springer.com/chapter/10.1007%2F978-3-642-23974-8_42#page-1"&gt;Intelligent Virtual Agents&lt;/a&gt; held in Reykjavik, Iceland from Sept. 15-17 in 2011. It did not specifically study airport environments, but such emotional contagion effects in a simulation could help better understand chaotic airport evacuations such as those that took place at JFK and LAX.&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The USC researchers&lt;/span&gt;
&lt;span&gt; compared the results from their ESCAPES simulation with video of real-life panicked crowds in order to tweak their models until they came up with the most realistic results. They found that their simulation best mimicked real life when it showed emotions spreading like temperature. An individual agent’s fear would go up or down based on the mood of the surrounding crowd. By comparison, a model that assumed fear spread like an infectious disease did not work as well.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The simulation also included other factors that helped it better mimic a chaotic situation, says &lt;a shape="rect" href="http://teamcore.usc.edu/tambe/"&gt;Milind Tambe&lt;/a&gt;, professor of computer science and industrial and systems engineering at the University of Southern California. Tambe is also co-founder and co-director of the Center on Artificial Intelligence for Social Solutions and a coauthor of the 2011 study.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“ESCAPES was the first to include families and children and first-time visitors,” Tambe says. “This meant evacuation became messy and agents in the simulation did not march out in perfect order when the evacuation started. Children would wander from their families and parents wouldn’t follow evacuation orders until they found their children; and children wouldn’t move fast, so the evacuation became messy.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Meanwhile, Manley at Texas A&amp;amp;M hopes to continue using crowd simulations to study how emergency evacuations from airports and other buildings could be made smoother and safer—especially for elderly and disabled people. He thinks that simulations could also begin directly illustrating the safety benefits of airport designs other than the traditional pier design.&lt;/p&gt;
&lt;p&gt;“The bottom line is the cost and safety concerns...at an airport are just too high to conduct a live safety exercise like a fire drill or anything,” Manley says. “Simulation is the only recourse for understanding what would happen during these situations.”&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 6 Sep 2016 17:00:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/software/modern-airports-have-no-easy-exit-for-panicking-crowds</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNzA1Mg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNzA1MA.jpg" height="225" width="300"/>
</item>
<item>
<title>The Little Hack That Could: The Story of Spotify’s “Discover Weekly” Recommendation Engine</title>
<link>http://spectrum.ieee.org/view-from-the-valley/computing/software/the-little-hack-that-could-the-story-of-spotifys-discover-weekly-recommendation-engine</link>
<description>How two software engineers pulled together machine learning tools to make one of Spotify's most popular offerings</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;How two software engineers pulled together machine learning tools to make one of Spotify's most popular offerings&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNjk5Ng.jpeg"/&gt;
&lt;figcaption&gt;Photo: Spotify&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;“Empower bottom-up innovation and amazing things will happen.” That’s what Spotify software engineer Edward Newett says. He was responsible for one of those amazing things: a way to help &lt;a shape="rect" href="https://www.spotify.com/"&gt;Spotify&lt;/a&gt; users discover new music called &lt;a shape="rect" href="https://www.spotify.com/us/discoverweekly/"&gt;Discover Weekly&lt;/a&gt;. This tool launched about a year ago; it now has 40 million users and is helping to build the careers of new artists.&lt;/p&gt;
&lt;figure class="lt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNjk5NA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Tekla Perry&lt;/figcaption&gt;
&lt;figcaption&gt;Spotify software engineer Edward Newett started building the Discover Weekly recommender system as a side project.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Newett joined Spotify in 2013, initially working on a team developing a web page with personalized information, news about artists, album releases, and local concerts, along with a recommender system that offered suggestions of albums a user might find appealing. The recommendation feature, Newett recalled, seemed like a good idea, but wasn’t heavily used. “My hunch was that navigating to this page and looking at albums was too much work,” he said, speaking at the third annual &lt;a shape="rect" href="https://atscaleconference.com/"&gt;@Scale conference&lt;/a&gt; held in San Jose last week. @Scale brings together engineers who build or maintain systems designed for vast numbers of users, including engineers from companies like Google, Airbnb, Dropbox, Netflix, and others.&lt;/p&gt;
&lt;p&gt;Newett thought he could come up with a better way, and in 2014 convinced a colleague to help him hack together a prototype of a tool they called Discover Weekly. Discover Weekly would give users a personalized playlist of music they’d never listened to, designed to fit their musical tastes.&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNjk5NQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Tekla Perry&lt;/figcaption&gt;
&lt;figcaption&gt;The Discover Weekly engineers didn’t want to commit a lot of time to the project until they were convinced it was worth doing. So they changed Spotify’s traditional engineering approach to include more frequent user tests.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To come up with the individualized lists, the two scrounged together a variety of machine learning tools used in other parts of the Spotify system. Their system looks at what the user is already listening to, and then find connections between those songs and artists and other songs and artists, crawling through user activity logs, playlists of other users, general news from around the web, and spectragrams of audio. It then filters the recommendations to eliminate music the user has already heard, and sends the individualized playlist to the user. To make it clear that this playlist was personalized, they decided to illustrate each with an image of the user, pulled from Facebook. And they determined that they would refresh the lists weekly, on Monday mornings.&lt;/p&gt;
&lt;p&gt;To test Discover Weekly in early 2015, the two engineers quietly pushed their new tool out to the Spotify accounts of all company employees. “Everyone freaked out in a good way,” Newett said, saying things like “‘It’s as if my secret music twin put it together.’”&lt;/p&gt;
&lt;p&gt;“We were feeling good, but we hadn’t tested it on [typical] users, so we then rolled it out to one percent of the user base,” he recalls, and again, he recalls, the reaction was awesome.&lt;/p&gt;
&lt;p&gt;Then came the mid-2015 rollout to the the rest of Spotify’s customers, some 100 million active users around the world. “We had to refresh 100 million playlists every Sunday night, with about a terabyte of new data.”&lt;/p&gt;
&lt;p&gt;Discover Weekly quickly became a habit for people, Newett reports, giving them something to look forward to on Monday mornings. Then, a few months after the mid-2015 launch, he says, the team had its “first production incident—it didn’t update.” Some users “went into blind rage or existential crisis.”&lt;/p&gt;
&lt;p&gt;For Newett, that reaction validated the tool’s popularity.&lt;/p&gt;
&lt;p&gt;Since then, the team moved the system from its own servers to Google’s Cloud &lt;a shape="rect" href="https://cloud.google.com/bigtable/"&gt;Bigtable&lt;/a&gt;, and now runs its recommendation algorithms several days ahead of time, rather than trying to crunch through recommendations for its entire user base on Sunday for Monday release.&lt;/p&gt;
&lt;p&gt;“This wasn’t a big company initiative,” Newett says, “just a team of passionate engineers who went about solving a problem we saw with the technology we had.”&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 6 Sep 2016 13:24:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/computing/software/the-little-hack-that-could-the-story-of-spotifys-discover-weekly-recommendation-engine</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNzAxMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNzAwOA.jpg" height="225" width="300"/>
</item>
<item>
<title>Facebook Engineers Crash Data Centers in Real-World Stress Test</title>
<link>http://spectrum.ieee.org/view-from-the-valley/computing/it/facebook-engineers-crash-data-centers-in-realworld-stress-test</link>
<description>After dodging disasters from Hurricane Sandy, Facebook instigates its own outages as part of Project Storm</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;After dodging disasters from Hurricane Sandy, Facebook instigates its own outages as part of Project Storm&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNjk3OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Facebook&lt;/figcaption&gt;
&lt;figcaption&gt;Facebook's Vice President of Engineering Jay Parikh addresses the engineers attending the third annual @Scale conference&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;“It’s easier to take a data center down than to put it back together,” says Facebook vice president of engineering Jay Parikh. But the company’s software engineers are getting better at the putting-it-back-together part, thanks to a series of regular stress tests conducted on Facebook’s operational network by the company’s disaster special weapons and tactics, or SWAT, team. Parikh described the effort, dubbed “Project Storm,” to the audience of invited engineers at the third annual &lt;a shape="rect" href="https://atscaleconference.com/"&gt;@Scale&lt;/a&gt; conference held in San Jose this week. @Scale brings together engineers who build or maintain systems designed for vast numbers of users, including companies like Google, Airbnb, Dropbox, Spotify, Netflix, and others.&lt;/p&gt;
&lt;p&gt;Facebook’s Project Storm originated in the wake of 2012’s Hurricane Sandy, Parikh reported. The superstorm threatened two of Facebook’s data centers, each carrying tens of terabits of traffic. Both got through Sandy unscathed, Parikh said, but watching the storm’s progress led the engineering team to consider what exactly would be the impact on Facebook’s global services if the company did indeed suddenly lose a data center or an entire region. The company assembled a SWAT team comprising the leaders of the various Facebook technology groups, who, in turn, marshaled the entire engineering workforce to figure out the answers.&lt;/p&gt;
&lt;p&gt;The group began running a number of tests and fine-tuning mechanisms for shifting traffic should a data center drop from the network, Parikh reported. They created tools and checklists of tasks both manual and automated; and they set time standards for completing each task. We wanted, Parikh said, “to run like a pit stop at a race; to get everything fixed on the car in the shortest period of time, realizing, however, that this is like taking apart an aircraft carrier and putting it back together in a few hours, not just taking apart a toy that I got for Christmas.”&lt;/p&gt;
&lt;p&gt;In 2014, Parikh decided Project Storm was ready for a real-world test: The team would take down an actual data center during a normal working day and see if they could orchestrate the traffic shift smoothly. &lt;/p&gt;
&lt;p&gt;Other Facebook leaders didn’t think he’d actually do it, Parikh recalls. “I was having coffee with a colleague just before the first drill. He said, ‘You’re not going to go through with it; you’ve done all the prep work, so you’re done, right?’ I told him, ‘There’s only one way to find out’” if it works.&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNjk3Nw.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Tekla Perry&lt;/figcaption&gt;
&lt;figcaption&gt;Traffic flow to Facebook’s various systems turns chaotic after the company’s first test of a data center suddenly going offline&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;That first takedown, which involved virtually the entire engineering team and a lot of people from the rest of the company, turned out to be a bit of a mess—at least from the inside. But users didn’t appear to notice. Parikh presented a chart tracking the traffic loads on various software systems—something that should have displayed smooth curves. “If you’re an engineer and see a graph like that, you’ve got bad data, your control system is not working right, or you have no idea what you’re doing.”&lt;/p&gt;
&lt;figure class="lt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNjk3OA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Tekla Perry&lt;/figcaption&gt;
&lt;figcaption&gt;Regular stress tests have helped Facebook improve the way its overall systems respond to a data center outage.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The Project Storm team forged ahead, continuing to hit Facebook’s networks with stress tests—although, Parikh recalls, there never seemed to be a good time to do them. “Something always ended up happening in the world or the company. One was during the World Cup final, another during a major product launch.” And the switchovers got smoother.&lt;/p&gt;
&lt;p&gt;The live takedowns continue today, with the Project Storm team members coming up with crazier and crazier ambitions for just what to take offline, Parikh says. “You need to push yourself to an uncomfortable place to get better.”&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 2 Sep 2016 17:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/computing/it/facebook-engineers-crash-data-centers-in-realworld-stress-test</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNjk5Mw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNjk5MQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Expert Questions Claim That St. Jude Pacemaker Was Hacked</title>
<link>http://spectrum.ieee.org/the-human-os/biomedical/devices/were-pacemakers-from-st-jude-medical-really-hacked</link>
<description>Evidence is not strong for harmful hack, says medical device security expert Kevin Fu</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Evidence is not strong for harmful hack, says medical device security expert Kevin Fu&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNjU1NA.jpeg"/&gt;
&lt;figcaption&gt;Image: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Last week, a &lt;a shape="rect" href="http://d.muddywatersresearch.com/wp-content/uploads/2016/08/MW_STJ_08252016_2.pdf"&gt;controversial report&lt;/a&gt; claimed that pacemakers and other implantable heart devices made by the manufacturer St. Jude Medical have massive security flaws that leave them vulnerable to hacking. Now, medical device security expert &lt;a shape="rect" href="https://web.eecs.umich.edu/~kevinfu/"&gt;Kevin Fu&lt;/a&gt;, an associate professor at University of Michigan, is &lt;a shape="rect" href="https://secure-medicine.blogspot.com/2016/08/study-on-st-jude-medical-device_30.html"&gt;questioning the accuracy&lt;/a&gt; of that report.&lt;/p&gt;
&lt;p&gt;The material presented in the report does not prove that hackers can cause a St. Jude device to crash, Fu told &lt;em&gt;IEEE Spectrum&lt;/em&gt; in an interview. “The onus is on the claimant,” Fu says. “We’re not saying the report is false, we’re saying the evidence is not strong.” Fu says a screenshot presented as evidence of the hack could have come from a benign situation that was misinterpreted. &lt;/p&gt;
&lt;div class="imgWrapper rt med"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNjU3OQ.jpeg"/&gt;
&lt;/div&gt;
&lt;p&gt;The investment firm &lt;a shape="rect" href="http://www.muddywatersresearch.com/"&gt;Muddy Waters&lt;/a&gt; issued the report based on an investigation by cybersecurity research company MedSec Holdings. The two firms had a financial incentive in releasing the report: Muddy Waters had “shorted” St. Jude stock (i.e., bet that it would decline in value), and MedSec had arranged to share any profits. St. Jude’s stock price has indeed taken a beating over the past week.&lt;/p&gt;
&lt;div style="page-break-after: always"/&gt;
&lt;p&gt;The report alleges that St. Jude’s cardiac devices can be caused to malfunction by hacking the at-home monitor patients use to send information to their doctors. It predicts that St. Jude will need to recall its devices, and estimates that class-action litigation could cost the company US. $6.4 billion. Here’s an excerpt from the report’s summary:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;span&gt;Key vulnerabilities can apparently be exploited by low level hackers. Incredibly, STJ has literally distributed hundreds of thousands of “keys to the castle” in the form of home monitoring units (called “Merlin@home”) that in our opinion, greatly open up the STJ ecosystem to attacks. These units are readily available on Ebay, usually for no more than $35. Merlin@homes generally lack even the most basic forms of security, and as this report shows, can be exploited to cause implanted devices to malfunction and harm users. We believe that courts will hold STJ’s lack of security in its Cardiac Device ecosystem is grossly negligent, unless STJ settles the litigation we see as inevitable. &lt;/span&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span&gt;It’s important to note that Fu has not yet tried to reproduce the hacks that MedSec claims to have pulled off: most notably, a “crash attack” causing an implanted device to beat at dangerously fast speed and another attack that allegedly drains a device’s battery. Fu says his team at the &lt;a shape="rect" href="https://www.secure-medicine.org/"&gt;Archimedes Center for Medical Device Security&lt;/a&gt; is still investigating the claims.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;However, he says he’s troubled by what he’s found so far. “I think these people are probably brilliant security experts. &lt;/span&gt;What we’re questioning is whether they’re able to correctly interpret clinical results,” Fu says. “It is possible to have vulnerabilities without having hazardous situations for patients.” &lt;/p&gt;
&lt;p&gt;Fu’s critique focuses on a screenshot from a St. Jude programming machine that’s presented on page 17 of the report, which is described as an indication that the associated cardiac device is malfunctioning. Here’s the screenshot in question:&lt;/p&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNjYyNQ.jpeg"/&gt;
&lt;/div&gt;
&lt;p&gt; Fu’s team conducted an experiment by connecting a St. Jude programming machine to their FDA-validated cardiac simulator. Using this setup, the researchers could create various conditions within the programmer and study the signals it sent to the simulated cardiac device.&lt;/p&gt;
&lt;p&gt;The researchers created the same screen and error messages, Fu says, while the simulated device continued to beat at the appropriate rhythm. “There was no change at all,” he says. The error messages on the screen are “benign alerts,” he says, that indicate that the device isn’t connected to cardiac tissue. “This is what you’d expect to see if it wasn’t connected to a patient.”&lt;/p&gt;
&lt;p&gt;His team’s brief demo is below, and they also describe the experiment in a &lt;a shape="rect" href="https://secure-medicine.blogspot.com/2016/08/correlation-is-not-causation-electrical.html"&gt;blog post&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;
&lt;iframe frameborder="0" height="349" scrolling="auto" allowfullscreen="" width="620" src="https://www.youtube.com/embed/ljDjyzh2C70?rel=0"/&gt;
&lt;/p&gt;
&lt;p&gt;Again, it’s worth noting Fu hasn’t proved anything conclusively. It’s possible that the same screenshot could also be created by the crash attack the MedSec team claims to have pulled off. &lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Muddy Waters responded to a request for comment with the following statement: “It’s no surprise the University of Michigan was inconclusive about our research given that we deliberately did not publish detailed information on the vulnerabilities, exploits or attacks on the devices in order to avoid giving the playbook to potential attackers. If anything, this proves that we were responsible with our disclosure.”&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;Fu says he can’t comment yet on the overall veracity of the report, or the likelihood of a massive recall of cardiac devices from St. Jude. He does note that such a move would be unprecedented, though. “T&lt;/span&gt;here has never been a [medical device] recall due to a security problem,” he says.&lt;/p&gt;
&lt;p&gt;Even if there is a security vulnerability in St. Jude’s technology, he says, regulators would have to determine that the vulnerability causes a clinical risk to patients. “Unfortunately, there’s very little clinical data in the report. And the one piece of clinical data we found appears to have been misinterpreted,” he says.  &lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 2 Sep 2016 13:00:00 GMT</pubDate>
<dc:creator>Eliza Strickland</dc:creator>
<guid>http://spectrum.ieee.org/the-human-os/biomedical/devices/were-pacemakers-from-st-jude-medical-really-hacked</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNjU3MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNjU2OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Researchers Map Locations of 4,669 Servers in Netflix’s Content Delivery Network</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/internet/researchers-map-locations-of-4669-servers-in-netflixs-content-delivery-network</link>
<description>The CDN of the popular video-streaming service lives in at least 243 locations</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The CDN of the popular video-streaming service lives in at least 243 locations&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNTI1Mw.jpeg"/&gt;
&lt;figcaption&gt;Illustration: Queen Mary University of London &lt;/figcaption&gt;
&lt;figcaption&gt;This map depicts the location of all Netflix servers recently found in a search led by Steve Uhlig and Timm Böttger from Queen Mary University of London.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;When you open Netflix and hit “play,” your computer sends a request to the video-streaming service to locate the movie you’d like to watch. The company responds with the name and location of the specific server that your device must access in order for you to view the film.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;For the first time, researchers have taken advantage of this naming system to map the location and total number of servers across &lt;a shape="rect" href="http://arxiv.org/pdf/1606.05519v1.pdf"&gt;Netflix’s entire content delivery network&lt;/a&gt;, providing a rare glimpse into the guts of the &lt;a shape="rect" href="http://www.geekwire.com/2016/study-amazon-video-now-third-largest-streaming-service-behind-netflix-youtube/"&gt;world’s largest video-streaming service&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;A group from &lt;a shape="rect" href="http://www.qmul.ac.uk/"&gt;Queen Mary University of London&lt;/a&gt; (QMUL) traced server names to identify 4,669 Netflix servers in 243 locations around the world. The majority of those servers still reside in the United States and Europe at a time when the company is eager to develop its international audience. The United States also leads the world in Netflix traffic, based on the group’s analysis of volumes handled by each server. Roughly eight times as many movies are watched there as in Mexico, which places second in Netflix traffic volume. The United Kingdom, Canada, and Brazil round out the top five.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The QMUL group presented its research to Netflix representatives earlier this year in a private symposium.&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“I think it's a very well-executed study,” says &lt;a shape="rect" href="https://www.doc.ic.ac.uk/~prp/"&gt;Peter Pietzuch&lt;/a&gt;, a specialist in large-scale distributed systems at Imperial College London who was not involved in the research. “Netflix would probably never be willing to share this level of detail about their infrastructure, because obviously it's commercially sensitive.”&lt;/p&gt;
&lt;p&gt;In March, Netflix did publish &lt;a shape="rect" href="https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience"&gt;a blog post&lt;/a&gt; outlining the overall structure of its content delivery network, but did not share the total number of servers or server counts for specific sites.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Last January, Netflix announced that it would expand its video-streaming service to 190 countries, and IHS Markit &lt;a shape="rect" href="http://fortune.com/2016/08/23/the-majority-of-netflix-subscribers-will-be-international-within-2-years/"&gt;recently predicted&lt;/a&gt; that the number of international Netflix subscribers could be greater than U.S. subscribers in as few as two years. Still, about &lt;a shape="rect" href="http://www.statista.com/statistics/324050/number-netflix-paying-streaming-subscribers/"&gt;72 percent of Netflix customers&lt;/a&gt; were based in the United States as of 2014.&lt;/p&gt;
&lt;div/&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://www.eecs.qmul.ac.uk/~steve/"&gt;Steve Uhlig&lt;/a&gt;, the networks expert at Queen Mary University of London &lt;a shape="rect" href="http://arxiv.org/pdf/1606.05519v1.pdf"&gt;who led the mapping project&lt;/a&gt;, says repeating the analysis over time could track shifts in the company’s server deployment and traffic volumes as its customer base changes.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“The evolution will reveal more about the actual strategy they are following,” he says.&lt;strong/&gt;“That's a bit of the frustrating part about having only the snapshot. You can make guesses about why they do things in a specific market, but it's just guesses.”&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Netflix launched streaming service in 2007 and began to design its own &lt;a shape="rect" href="https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience"&gt;content delivery network&lt;/a&gt; in 2011. Companies that push out huge amounts of online content have two options when it comes to building their delivery networks. They may choose to place tons of servers at Internet exchange points (IXPs), which are like regional highway intersections for online traffic. Or, they can forge agreements to deploy servers within the private networks of Internet service providers such as Time Warner, Verizon, AT&amp;amp;T, and Comcast so that they’re even closer to customers.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Traditionally, content delivery services have chosen one strategy or the other. &lt;a shape="rect" href="https://www.akamai.com/"&gt;Akamai&lt;/a&gt;, for example, hosts a lot of content with Internet service providers, while Google, Amazon, and &lt;a shape="rect" href="https://www.limelight.com/"&gt;Limelight&lt;/a&gt; prefer to store it at IXPs. However, Uhlig’s group found that Netflix uses both strategies, and varies the structure of its network significantly from country to country.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://www.eecs.qmul.ac.uk/people/view/45802/timm-bttger"&gt;Timm Böttger&lt;/a&gt;, a doctoral student at QMUL who is a member of the research team, says he was surprised to find two Netflix servers located within Verizon’s U.S. network. Verizon and other service providers have argued with Netflix over whether they would allow Netflix to directly connect servers to their networks for free. In 2014, &lt;a shape="rect" href="http://www.wsj.com/articles/SB10001424052702304834704579401071892041790"&gt;Comcast required Netflix to pay&lt;/a&gt; for access to its own network.&lt;/p&gt;
&lt;p&gt;Tellingly, the group did not find any Netflix servers in Comcast’s U.S. network. As for the mysterious Verizon servers? “We think it is quite likely that this is a trial to consider broader future deployment,” Böttger says. Netflix did not respond to a request for comment.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;To outline Netflix’s content delivery network, Uhlig and his group began by playing films from the Netflix library and studying the structure of server names that were returned from their requests. The researchers also used the &lt;a shape="rect" href="http://hola.org/"&gt;Hola browser extension&lt;/a&gt; to request films from 753 IP addresses in different parts of the world in order to find even more server names than would otherwise be accessible from their London lab.&lt;/p&gt;
&lt;p&gt;“We first tried to behave like the regular users, and just started watching random movies and took a look at the network packages that were exchanged,” says Böttger.&lt;/p&gt;
&lt;p&gt;Their search revealed that Netflix’s server names are written in a similar construction: a string of numbers and letters that include traditional airport codes such as lhr001 for London Heathrow to mark the server’s location and a “counter” such as c020 to indicate the number of servers at that location. A third element written as .isp or .ix shows whether the server is located within an Internet exchange point or with an Internet service provider.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Once they had figured out this naming structure, the group built a crawler that could search for domain names that shared the common nflxvideo.net address. The team supplied the crawler with a list of countries, airport codes, and Internet service providers compiled from publicly available information. After searching all possible combinations of those lists, the crawler returned 4,669 servers in 243 locations. (Though the study cites 233 locations, Böttger said in a follow-up email that 243 is the correct number.) &lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;To study traffic volumes, the researchers relied on a specific section of the IP header that keeps a running tally of data packets that a given server has handled. By issuing multiple requests to these servers and tracking how quickly the values rose, the team estimated how much traffic each server was processing at different times of the day. They tested the servers in 1-minute intervals over a period of 10 days.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Their results showed that the structure and volume of data requested from Netflix’s content delivery network varies widely from country to country. In the United States, Netflix is largely delivered through IXPs, which house 2,583 servers—far more than the 625 found at Internet service providers.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Meanwhile, there are no Netflix servers at IXPs in Canada or Mexico. Customers in those countries are served exclusively by servers within Internet service providers, as well as possibly through IXPs along the U.S. borders. South America also relies largely on servers embedded within ISP networks—with the exception of Brazil, which has Netflix servers stashed at several IXPs.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The U.K. has more Netflix servers than any other European country, and most of those servers are deployed within Internet service providers. All French customers get their films streamed through servers stationed at a single IXP called &lt;a shape="rect" href="https://www.franceix.net/en/"&gt;France-IX&lt;/a&gt;. Eastern Europe, meanwhile, has no Netflix servers because those countries were only just added to the company’s network in January.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;And the entire continent of Africa has only eight Netflix servers, all of which are deployed at IXPs in Johannesburg, South Africa. That’s only a few more than the four Netflix servers&lt;sup/&gt;the team found on the tiny Pacific Ocean island of Guam, which is home to the U.S.-operated &lt;a shape="rect" href="http://www.andersen.af.mil/"&gt;Andersen Air Force Base&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;span&gt;“It's kind of striking to see those differences across countries,” Pietzuch says. “[Netflix&lt;/span&gt;
&lt;span&gt;’s]&lt;/span&gt;
&lt;span&gt; recent announced expansion isn't really that visible when you only look at the evolution of their CDN structure.”&lt;/span&gt;
&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Before the group’s analysis, Uhlig expected to see servers deployed mostly through Internet service providers as a way to ease the traffic burden for service providers and get as close as possible to Netflix’s &lt;a shape="rect" href="http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html"&gt;83 million customers&lt;/a&gt;. He was surprised to see how heavily the company relies on IXPs, despite the company’s insistence that &lt;a shape="rect" href="https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience"&gt;90 percent of its traffic &lt;/a&gt;is delivered through ISPs.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“If you really want to say, ‘I really want to be close to the end users,’ you need to deploy more, and we didn't see that,” he says. “I think the centralized approach is convenient because you have more control and you can scale things up or down according to what the market tells you.”&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Uhlig didn’t expect to find Mexico and Brazil so high on the traffic list, even though Netflix has tried to expand its &lt;a shape="rect" href="http://variety.com/2016/film/global/netflix-acquires-alex-de-la-iglesia-skins-spain-1201816186/"&gt;Spanish&lt;/a&gt;- and Portuguese-language offerings.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;In March, the company said it delivers about &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience"&gt;125 million total hours of viewing to customers per day&lt;/a&gt;
&lt;span&gt;. &lt;/span&gt;The researchers learned that Netflix traffic seems to peak just before midnight local time, with a second peak for IXP servers occurring around 8 a.m., presumably as Netflix uploads new content to its servers. &lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 30 Aug 2016 21:00:00 GMT</pubDate>
<dc:creator>Amy Nordrum</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/internet/researchers-map-locations-of-4669-servers-in-netflixs-content-delivery-network</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNTMxMQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNTMwOQ.jpg" height="225" width="300"/>
</item>
<item>
<title>The Surprising Story of the First Microprocessors</title>
<link>http://spectrum.ieee.org/computing/hardware/the-surprising-story-of-the-first-microprocessors</link>
<description>You thought it started with the Intel 4004, but the tale is more complicated</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;You thought it started with the Intel 4004, but the tale is more complicated&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwMTcxMQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: INTEL&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="/image/MjgwMTQ2NQ.jpeg" src="http://spectrum.ieee.org/image/MjgwMTQ2NQ.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Intel&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;The Die is Cast:&lt;/strong&gt; Intel’s 4-bit 4004 chip is widely regarded as the world’s first microprocessor. But it was not without rivals for that title.    &lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;Transistors, &lt;/strong&gt;the electronic amplifiers and switches found at the heart of everything from pocket radios to warehouse-size supercomputers, were &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/devices/the-lost-history-of-the-transistor"&gt;invented in 1947.&lt;/a&gt; Early devices were of a type called bipolar transistors, which are still in use. By the 1960s, engineers had figured out how to combine multiple bipolar transistors into single integrated circuits. But because of the complex structure of these transistors, an integrated circuit could contain only a small number of them. So although a &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Minicomputer"&gt;minicomputer&lt;/a&gt; built from bipolar integrated circuits was much smaller than earlier computers, it still required multiple boards with hundreds of chips.&lt;/p&gt;
&lt;p&gt;In 1960, a new type of transistor was demonstrated: &lt;a shape="rect" href="http://users.ece.gatech.edu/~alan/ECE3040/Lectures/Lecture24-MOS Transistors.pdf"&gt;the metal-oxide-semiconductor (MOS) transistor&lt;/a&gt;. At first this technology wasn’t all that promising. These transistors were slower, less reliable, and more expensive than their bipolar counterparts. But by 1964, integrated circuits based on MOS transistors boasted higher densities and lower manufacturing costs than those of the bipolar competition. Integrated circuits continued to increase in complexity, as described by Moore’s Law, but now MOS technology took the lead.&lt;/p&gt;
&lt;p&gt;By the end of the 1960s, a single MOS integrated circuit could contain 100 or more logic gates, each containing multiple transistors, making the technology particularly attractive for building computers. These chips with their many components were given the label LSI, for &lt;a shape="rect" href="http://research.microsoft.com/en-us/um/people/gbell/cgb files/large scale integration 7209 c.pdf"&gt;large-scale integration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Engineers recognized that the increasing density of MOS transistors would eventually allow a complete computer processor to be put on a single chip. But because MOS transistors were slower than bipolar ones, a computer based on MOS chips made sense only when relatively low performance was required or when the apparatus had to be small and lightweight—such as for data terminals, calculators, or avionics. So those were the kinds of computing applications that ushered in the microprocessor revolution.&lt;/p&gt;
&lt;p&gt;Most engineers today are under the impression that the start of that revolution began in 1971 with Intel’s 4-bit 4004 and was immediately and logically followed by the company’s 8-bit 8008 chip. In fact, the story of the birth of the microprocessor is far richer and more surprising. In particular, some newly uncovered documents illuminate how a long-forgotten chip—Texas Instruments’ TMX 1795—beat the Intel 8008 to become the first 8-bit microprocessor, only to slip into obscurity.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;What opened the door&lt;/strong&gt; for the first microprocessors, then, was the application of MOS integrated circuits to computing. The first computer to be fashioned out of MOS-LSI chips was something called the D200, created in 1967 by &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Autonetics"&gt;Autonetics,&lt;/a&gt; a division of North American Aviation, located in Anaheim, Calif.&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="/image/MjgwMTQ4Mw.jpeg" src="http://spectrum.ieee.org/image/MjgwMTQ4Mw.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Paul Sakuma/AP Photos&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Three Proud Parents: &lt;/strong&gt;Posing during induction ceremonies for the National Inventors Hall of Fame in 1996, Federico Faggin, Marcian “Ted” Hoff Jr., and Stanley Mazor [from left] show off the pioneering microprocessor they created in the early 1970s, the Intel 4004.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;This compact, 24-bit general-purpose computer was designed for aviation and navigation. Its central processing unit was built from 24 MOS chips and benefitted from a design technique called four-phase logic, which used four separate clock signals, each with a different on-off pattern, or phase, to drive changes in the states of the transistors, allowing the circuitry to be substantially simplified. Weighing only a few kilograms, the computer was used for guidance on the Poseidon submarine-launched ballistic missile and for fuel management on the B-1 bomber. It was even considered for the space shuttle.&lt;/p&gt;
&lt;p&gt;The D200 was followed shortly by another avionics computer that contained three CPUs and used in total 28 chips: the &lt;a shape="rect" href="http://www.airspacemag.com/history-of-flight/the-road-to-the-future-is-paved-with-good-inventions-39041963/?no-ist"&gt;Central Air Data Computer,&lt;/a&gt; built by Garrett AiResearch (now part of Honeywell). The computer, a flight-control system designed for the &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Grumman_F-14_Tomcat"&gt;F-14 fighter&lt;/a&gt;, used the MP944 MOS-LSI chipset, which Garrett AiResearch developed between 1968 and 1970. The 20-bit computer processed information from sensors and generated outputs for instrumentation and aircraft control.&lt;/p&gt;
&lt;aside class="inlay lt med"&gt;
&lt;h3 class="sb-hed"&gt;
&lt;strong&gt;Some Assembly Required&lt;/strong&gt;
&lt;/h3&gt;
&lt;h4 class="sb-dek"&gt;
&lt;strong&gt;This online simulator lets you explore the workings of a simple microprocessor&lt;/strong&gt;
&lt;/h4&gt;
&lt;figure class="rt" role="img"&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/static/some-assembly-required"&gt;
&lt;img alt="/image/MjgwMTQ5Ng.jpeg" src="http://spectrum.ieee.org/image/MjgwMTQ5Ng.jpeg"/&gt;
&lt;/a&gt;
&lt;/figure&gt;
&lt;/aside&gt;
&lt;p&gt;The architecture of the F-14 computer was unusual. It had three functional units operating in parallel: one for multiplication, one for division, and one for special logic functions (which included clamping a value between upper and lower limits). Each functional unit was composed of several different kinds of MOS chips, such as a read-only memory (ROM) chip, which contained the data that determined how the unit would operate; a data-steering chip; various arithmetic chips; and a RAM chip for temporary storage.&lt;/p&gt;
&lt;p&gt;Because the F-14 computer was classified, few people ever knew about the MP944 chipset. But Autonetics widely publicized its D200, which then inspired an even more compact MOS-based computer: the System IV. That computer was the brainchild of &lt;a shape="rect" href="http://www.eecs.umich.edu/eecs/about/articles/2007/Boysel.html"&gt;Lee Boysel&lt;/a&gt;, who left Fairchild Semiconductor in 1968 to cofound &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Four-Phase_Systems"&gt;Four-Phase Systems,&lt;/a&gt; naming his new company after Autonetics’ four-phase logic.&lt;/p&gt;
&lt;p&gt;The CPU of the 24-bit System IV was constructed from as few as nine MOS chips: three arithmetic-logic-unit (ALU) chips of a design dubbed the AL1 (which performed arithmetic operations like adding and subtracting, along with logical operations like AND, OR, and NOT), three ROM chips, and three random-logic chips.&lt;/p&gt;
&lt;aside class="inlay rt med"&gt;
&lt;h3 class="sb-hed"&gt;Everything’s Bigger in Texas&lt;/h3&gt;
&lt;h4 class="sb-dek"&gt;Although Texas Instruments’ TMX 1795 and Intel’s 8008 had a similar number of transistors, the former required a much larger silicon die.  Indeed, the TMX 1795 was larger than the Intel 8008 and 4004 combined. Intel’s engineers believed that its large size made the TI chip impractical to produce in commercial quantities, but TI’s very successful TMS 0100 calculator chip, introduced at about the same time, had an even larger die. So the connection between die size and commercial viability must not have been straightforward. (The relative sizes of the dies are shown below.)&lt;/h4&gt;
&lt;figure class="rt" role="img"&gt;
&lt;img alt="/image/MjgwMTU3OQ.jpeg" src="http://spectrum.ieee.org/image/MjgwMTU3OQ.jpeg"/&gt;
&lt;figcaption&gt;
&lt;strong&gt;TMX 1795&lt;/strong&gt;
&lt;p&gt;3,078 transistors&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="rt" role="img"&gt;
&lt;img alt="/image/MjgwMTU4MA.jpeg" src="http://spectrum.ieee.org/image/MjgwMTU4MA.jpeg"/&gt;
&lt;figcaption&gt;
&lt;strong&gt;4004&lt;/strong&gt;
&lt;p&gt;2,300 transistors&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="rt" role="img"&gt;
&lt;img alt="/image/MjgwMTU4MQ.jpeg" src="http://spectrum.ieee.org/image/MjgwMTU4MQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Images: Computer History Museum&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;8008&lt;/strong&gt;
&lt;p&gt;3,098 transistors&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/aside&gt;
&lt;p&gt;Almost simultaneously, a Massachusetts-based startup called &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Viatron"&gt;Viatron Computer Systems&lt;/a&gt; got into the game. Just a year after its launch in November 1967, the company announced its System 21, a 16-bit minicomputer with various accessories, all built from custom MOS chips.&lt;/p&gt;
&lt;p&gt;We can thank someone at Viatron for coining the word “microprocessor.” The company first used it in an October 1968 announcement of a product it called the 2101. But this microprocessor wasn’t a chip. In Viatron’s lexicon, the word referred to part of a smart terminal, one that came complete with keyboard and tape drives and connected to a separate minicomputer. Viatron’s “microprocessor” controlled the terminal and consisted of 18 custom MOS chips on three separate boards.&lt;/p&gt;
&lt;p&gt;Amid these goings-on at the end of the 1960s, the Japanese calculator maker Business Computer Corp. (better known as &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Busicom"&gt;Busicom&lt;/a&gt;) contracted with &lt;a shape="rect" href="http://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt; for custom chips for a multiple-chip calculator. The final product was simplified to a single-chip CPU, the now-famous &lt;a shape="rect" href="http://www.intel.com/content/www/us/en/history/museum-story-of-intel-4004.html"&gt;Intel 4004&lt;/a&gt;, along with companion chips for storage and input/output (I/O). The 4-bit 4004 (meaning that it manipulated data words that were only 4 bits wide) is often considered the first microprocessor.&lt;/p&gt;
&lt;p&gt;The calculator containing the 4004 first came together at the start of 1971. By this time, it had plenty of competition. A semiconductor company called &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Mostek"&gt;Mostek&lt;/a&gt; had produced the first calculator-on-a chip, the&lt;a shape="rect" href="http://www.datamath.org/Mostek_IC.htm"&gt; MK6010.&lt;/a&gt; And Pico Electronics and General Instrument also had their G250 calculator-on-a-chip working. Within six months, the Texas Instruments &lt;a shape="rect" href="http://www.datamath.org/Story/Datamath.htm#The \"&gt;TMS 1802&lt;/a&gt; calculator-on-a-chip was also operational, it being the first chip in TI’s hugely successful 0100 line. While these circuits worked fine as calculators, they couldn’t do anything else, whereas the 4004 operated by carrying out instructions stored in external ROM. Thus it could serve in a general-purpose computer.&lt;/p&gt;
&lt;p&gt;This was a fast-moving time for the electronic-calculator business, and after running into financial difficulties, Busicom gave up its exclusive rights to the 4004 chip. In November 1971 Intel began marketing it and its associated support chips as a stand-alone product intended for general computing applications. Within a few months, the 4004 was eclipsed by more powerful microprocessors, however, so it found &lt;a shape="rect" href="http://www.technologizer.com/2011/11/15/intel-4004/"&gt;few commercial applications&lt;/a&gt;. They included a couple of pinball machines, a word processor, and a system for tallying votes.&lt;/p&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="/image/MjgwMTY1MA.jpeg" src="http://spectrum.ieee.org/image/MjgwMTY1MA.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: History-computer.com&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;CPU Flip-flops: &lt;/strong&gt;Makers of the Datapoint 2200 terminal sought a single-chip CPU for it from both Intel and Texas Instruments. Neither TI nor Intel’s CPU chips saw use in the Datapoint 2200, but they led a wave of 8-bit microprocessors that powered the microcomputer revolution.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;In this sense,&lt;/strong&gt; it was an electronic calculator that begot the first microprocessor, Intel’s 4-bit 4004. But the 8-bit microprocessors that quickly succeeded it had a very different genesis. That story starts in 1969 with the development of the &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Datapoint_2200"&gt;Datapoint 2200&lt;/a&gt; “programmable terminal,” by a company called Computer Terminal Corp. (CTC), based in San Antonio, Texas.&lt;/p&gt;
&lt;p&gt;The Datapoint 2200 was really a general-purpose computer, not just a terminal. Its 8-bit processor was initially built out of about 100 bipolar chips. Its designers were looking for ways to have the processor consume less power and generate less heat. So in early 1970, CTC arranged for Intel to build a single MOS chip to replace the Datapoint processor board, although it’s unclear whether the idea of using a single chip came from Intel or CTC.&lt;/p&gt;
&lt;figure class="lt sm" role="img"&gt;
&lt;img alt="/image/MjgwMTYxNQ.jpeg" src="http://spectrum.ieee.org/image/MjgwMTYxNQ.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Computer History Museum&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Guy From TI: &lt;/strong&gt;Gary Boone led the development of the TMX1795, along with other important digital chips.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;By June 1970, Intel had developed a functional specification for a chip based on the architecture of the Datapoint 2200 and then put the project on hold for six months. This is the design that would become the &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Intel_8008"&gt;Intel 8008&lt;/a&gt;. So whether you consider the calculator-inspired 4004 or the terminal-inspired 8008 to be the first truly useful single-chip, general-purpose microprocessor, you’d have to credit its creation to Intel, right? Not really.&lt;/p&gt;
&lt;p&gt;You see, in 1970, when Intel began working on the 8008, it was a startup with about 100 employees. After learning of Intel’s processor project, &lt;a shape="rect" href="http://www.ti.com/"&gt;Texas Instruments&lt;/a&gt;, or TI—a behemoth of a company, with 45,000 employees—asked CTC whether it, too, could build a processor for the Datapoint 2200. CTC gave engineers at TI the computer’s specifications and told them to go ahead. When they returned with a three-chip design, CTC pointedly asked whether TI could build it on one chip, as Intel was doing. TI then started working on a single-chip CPU for CTC around April 1970. That design, completed the next year, was first called the &lt;a shape="rect" href="http://en.wikichip.org/wiki/ti/tmx1795"&gt;TMX 1795&lt;/a&gt; (&lt;em&gt;X&lt;/em&gt; for “experimental”), a name that morphed into TMC 1795 when it was time for the chip to shed its prototype status.&lt;/p&gt;
&lt;p&gt;In June 1971, TI launched a media campaign for the TMC 1795 describing how this “central processor on a chip” would make the new Datapoint 2200 “a powerful computer with features the original one couldn’t offer.” That didn’t happen, though: After testing the TMC 1795, CTC rejected it, opting to continue building its processor using a board of bipolar chips. Intel’s chip wouldn’t be ready until the end of that year.&lt;/p&gt;
&lt;p&gt;Many historians of technology believe that the TMC 1795 died then and there. But newly surfaced documents from the late &lt;a shape="rect" href="http://ethw.org/Oral-History:Gary_Boone"&gt;Gary Boone&lt;/a&gt;, the chip’s lead developer, show that after CTC’s rejection, TI tried to sell the chip (which after some minor improvements became known as the TMC 1795A) to various companies. &lt;a shape="rect" href="http://corporate.ford.com/homepage.html"&gt;Ford Motor Co.&lt;/a&gt; showed interest in using the chip as an engine controller in 1971, causing Boone to write, “I think we have walked into the mass market our ‘CPU-on-a-chip’ desperately needs.” Alas, these efforts failed, and TI ceased marketing the TMC 1795, focusing on its more profitable calculator chips instead. Nevertheless, if you want to assign credit for the first 8-bit microprocessor, you should give that honor to TI, never mind that it fumbled the opportunity.&lt;/p&gt;
&lt;figure class="stacked lt med" role="img"&gt;
&lt;img alt="/image/MjgwMTY3Nw.jpeg" src="http://spectrum.ieee.org/image/MjgwMTY3Nw.jpeg"/&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgwMjA3MQ.jpeg"&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;img alt="/image/MjgwMTY3OA.jpeg" src="http://spectrum.ieee.org/image/MjgwMTY3OA.jpeg"/&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgwMjA3MA.jpeg"&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;img alt="/image/MjgwMTY3OQ.jpeg" src="http://spectrum.ieee.org/image/MjgwMTY3OQ.jpeg"/&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgwMjA2MQ.jpeg"&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Images: Steve Golson&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Engines of Change: &lt;/strong&gt;These memos reveal that Ford Motor Co. considered using TI's pioneering microprocessor as an engine controller.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;By the time Intel had the 8008 working, at the end of 1971, CTC had lost interest in single-chip CPUs and gave up its exclusive rights to the design. Intel went on to commercialize the 8008, announcing it in April 1972 and ultimately producing hundreds of thousands of them. Two years later, the 8008 spawned Intel’s 8080 microprocessor, which heavily influenced the 8086, which in turn opened the floodgates for Intel’s current line of &lt;em&gt;x&lt;/em&gt;86 chips. So if you’re sitting at a PC with an &lt;em&gt;x&lt;/em&gt;86 processor right now, you’re using a computer based on a design that dates all the way back to Datapoint’s 2200 programmable terminal of 1969.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;As this history makes clear,&lt;/strong&gt; the evolution of the microprocessor followed anything but a straight line. Much was the result of chance and the outcome of various business decisions that might easily have gone otherwise. Consider how the 8-bit processor architecture that CTC designed for the Datapoint 2200 was implemented in four distinct ways. CTC did it twice using a board stuffed with bipolar chips, first in an arrangement that communicated data serially and later using a parallel design that was much faster. Both TI and Intel met CTC’s requirements with single chips having almost identical instruction sets, but the packaging, control signals, instruction timing, and internal circuitry of the two chips were entirely different.&lt;/p&gt;
&lt;p&gt;Intel used more advanced technology than did TI, most notably self-aligned gates made of polysilicon, which made the transistors faster and improved yields. This approach also allowed the transistors to be packed more densely. As a result, the 4004 and 8008, even combined, were smaller than the TMC 1795. Indeed, Intel engineers considered the TI chip too big to be practical, but that really wasn’t the case: TI’s highly successful TMS 0100 calculator chip, introduced soon afterward, was even larger than the TMC 1795.&lt;/p&gt;
&lt;p&gt;Given all this, whom should we credit with the invention of the microprocessor? One answer is that the microprocessor wasn’t really an invention but rather something that everyone knew would happen. It was just a matter of waiting for the technology and market to line up. I find this perspective the most compelling.&lt;/p&gt;
&lt;p&gt;Another way to look at things is that “microprocessor” is basically a marketing term driven by the need of Intel, TI, and other chip companies to brand their new products. Boone, despite being the developer of the TMC 1795, later credited Intel for its commitment to turning the microprocessor into a viable product. In an undated letter, apparently part of a legal discussion over who should get credit for the microprocessor, he wrote: “The dominant theme in the development of the microprocessor is the corporate commitment made by Intel in the 1972–75 period…. Their innovations in design, software and marketing made possible this industry, or at least hurried it along.”&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="/image/MjgwMTcyNA.jpeg" src="http://spectrum.ieee.org/image/MjgwMTcyNA.jpeg"/&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href=""&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photos, left: Intel; right: Computer History Museum&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;The First Microprocessor: &lt;/strong&gt;Credit normally goes to the Intel 4004, a 4-bit chip designed to serve in a calculator [left]. But there are other possible firsts, depending on your definitions. One was the AL1 arithmetic-logic-unit chip from Four-Phase Systems [right], which predates the 4004 and was used to demonstrate a working computer in a dispute over an early patent for the microprocessor.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Honors for creating the first microprocessor also depend on how you define the word. Some define a microprocessor as a CPU on a chip. Others say all that’s required is an arithmetic logic unit on a chip. Still others would allow these functions to be packaged in a few chips, which would collectively make up the microprocessor.&lt;/p&gt;
&lt;p&gt;In my view, the key features of a microprocessor are that it provides a CPU on a single chip (including ALU, control functions, and registers such as a program counter) and that it is programmable. But a microprocessor isn’t a complete computer: Additional chips are typically needed for memory, I/O, and other support functions.&lt;/p&gt;
&lt;p&gt;Using such a definition, most people consider the Intel 4004 to be the first microprocessor because it contains all the components of the central processing unit on a single chip. Both Boone and &lt;a shape="rect" href="http://www.computerhistory.org/fellowawards/hall/federico-faggin"&gt;Federico Faggin&lt;/a&gt; (of Intel’s 4004 team) agree that the 4004 beat the earliest TMX 1795 prototypes by a month or two. The latter would then represent the first 8-bit microprocessor, and the Intel 8008 the first c&lt;em&gt;ommercially successful &lt;/em&gt;8-bit microprocessor.&lt;/p&gt;
&lt;p&gt;But if you adopt a less-restrictive definition of “microprocessor,” many systems could be considered the first. Those who consider an ALU-on-a-chip to be a microprocessor credit Boysel for making the first one at Fairchild in 1968, shortly before he left to cofound Four-Phase Systems. The AL1 from Four-Phase Systems is also a candidate because it combined registers and ALU on a single chip, while having the control circuitry external. If you allow that a microprocessor can consist of multiple LSI chips, the Autonetics D200 would qualify as first.&lt;/p&gt;
&lt;p&gt;Patents provide a different angle on the invention of the microprocessor. TI was quick to realize the profitability of patents. It obtained multiple patents on the TMX 1795 and TMS 0100 and made heavy use of these patents in litigation and licensing agreements.&lt;/p&gt;
&lt;p&gt;Based on its patents, TI could be considered the inventor of both the microprocessor and the microcontroller, a single-chip packaging of CPU, memory, and various support functions. Or maybe not. That’s because Gilbert Hyatt obtained a patent for the single-chip processor in 1990, based on a 16-bit serial computer he built in 1969 from boards of bipolar chips. This led to claims that Hyatt was the inventor of the microprocessor, until TI defeated Hyatt’s patent in 1996 after &lt;a shape="rect" href="http://www.nytimes.com/1996/06/20/business/for-texas-instruments-some-bragging-rights.html"&gt;a complex legal battle&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another possible inventor to credit would be Boysel. In 1995, during a legal proceeding that Gordon Bell later mockingly called “&lt;a shape="rect" href="http://www.ttivanguard.com/ttivanguard_cfmfiles/pdf/sanjose12/sanjose12session7111.pdf#page=28"&gt;TI versus Everybody&lt;/a&gt; [PDF],” Boysel countered TI’s single-chip processor patents by using a single AL1 ALU chip from 1969 to demonstrate a working computer to the court. His move effectively torpedoed TI’s case, although I don’t see his demo as particularly convincing, because he used some technical tricks to pull it off.&lt;/p&gt;
&lt;p&gt;Regardless of what you consider the first microprocessor, you have to agree that there was no lack of contenders for this title. It’s a shame, really, that most people seek to recognize just one winner in the race and that many fascinating runners-up are now almost entirely forgotten. But for those of us with an interest in the earliest days of microcomputing, this rich history will live on.&lt;/p&gt;
&lt;div id="biogrp"&gt;
&lt;h2&gt;About the Author&lt;/h2&gt;
&lt;p&gt;Ken Shirriff worked as a programmer for Google before retiring in June 2016. A computer history buff, he’s fascinated with the earliest CPU chips. At the time of publication of this article, he was helping to restore a 1973 Xerox Alto microcomputer, &lt;span style="display:inline !important"&gt;the computer that&lt;/span&gt; introduced the graphical user interface and the mouse. (For more on the restoration, see Shirriff’s blog, &lt;a shape="rect" href="http://www.righto.com/"&gt;www.righto.com&lt;/a&gt;.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 30 Aug 2016 15:00:00 GMT</pubDate>
<dc:creator>Ken Shirriff</dc:creator>
<guid>http://spectrum.ieee.org/computing/hardware/the-surprising-story-of-the-first-microprocessors</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwMTcyMw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwMTcyMQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Real-Time Crowd Simulator Could Help Prevent Deadly Stampedes</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/real-time-crowd-movement-simulator</link>
<description>System collects real-time crowd data to help authorities predict and head off tragic stampedes that have left hundreds dead</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;System collects real-time crowd data to help authorities predict and head off tragic stampedes that have left hundreds dead&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNDE0OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;As fatal crowd accidents in places like &lt;a shape="rect" href="https://www.theguardian.com/world/2014/dec/31/shanghai-35-people-killed-42-injured-new-year-crush"&gt;Shanghai&lt;/a&gt; and &lt;a shape="rect" href="http://www.cnn.com/2015/09/25/middleeast/hajj-pilgrimage-stampede/"&gt;Mecca&lt;/a&gt; have shown, improved means of controlling crowd behavior and predicting dangerous congestion spots would be a boon to authorities such as police forces that are tasked with managing major public events. To this end, Mitsubishi Electric and &lt;span&gt;the University of Tokyo’s&lt;/span&gt; Research Center for Advanced Science and Technology (RCAST) have jointly developed what they claim is the world’s first real-time crowd-congestion estimation system.&lt;/p&gt;
&lt;p&gt;According to Mitsubishi, conventional systems rely on previously collected data concerning crowd conditions for a given location or event. The data is retrieved from a database and fed to a crowd-movement simulator, which then produces a crowd density map.&lt;/p&gt;
&lt;p&gt;Rather than relying on historical data, the Mitsubishi system employs surveillance cameras positioned at strategic locations to capture the flow rate of crowds along major pathways in real time. To simulate pedestrian dynamics, the RCAST researchers used a mathematical tool known as a “floor field cellular automaton model,” which together with the flow-rate data can produce a crowd-movement simulation in real time.&lt;/p&gt;
&lt;p&gt;The combined results are converted into a color-coded map showing areas where crowd density could be a concern [see figure below].&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNDEzOQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Image: Mitsubishi Electric&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Mitsubishi says that whereas a conventional system bases its calculations on all the people estimated to be in a given location, the new system uses the real-time flow rate data to do a more accurate calculation, taking into account a more select group of people [figure, below].&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwNDE0MA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Image: Mitsubishi Electric&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Mitsubishi claims that &lt;span&gt;this optimized way of modeling pedestrian dynamics&lt;/span&gt; reduces computational complexity and raises the accuracy of calculated crowd-density maps to 80 percent, compared with the roughly 50 percent accuracy yielded by conventional systems.&lt;/p&gt;
&lt;aside class="inlay pullquote xlrg"&gt;Mitsubishi claims that its system can increase the accuracy of crowd-density maps to 80 percent, compared with the roughly 50 percent accuracy yielded by conventional crowd simulators.&lt;/aside&gt;
&lt;p&gt;Given these improvements, Shunichi Sekiguchi, general manager of Mitsubishi’s surveillance system technology division, believes the new system could help prevent the kinds of crowd tragedies that happened in Shanghai and Mecca.&lt;/p&gt;
&lt;p&gt;“Thanks to these more accurate predictions and visualizations made in real time, it makes early regulation of crowd traffic possible,” he says. “This would help authorities and security companies take preventive countermeasures when necessary.”&lt;/p&gt;
&lt;p&gt;Mitsubishi tested the system at a popular firework festival held on the Tamagawa River in Tokyo last week. The company worked with local Setagaya Ward authorities and used eight cameras to measure crowd flow.&lt;/p&gt;
&lt;p&gt;“We are currently reviewing the results from the field demonstration,” says Sekiguchi. “But what I can say is that it rained sometimes during the evening, and so we were able to verify the precision of the people count and congestion predictions in bad weather conditions.”&lt;/p&gt;
&lt;p&gt;No word yet when the system will be ready for commercialization, though Sekiguchi said they are “speeding up research and development for early practical application.”&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 25 Aug 2016 18:00:00 GMT</pubDate>
<dc:creator>John Boyd</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/real-time-crowd-movement-simulator</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNDE2Mw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNDE2MQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Tackling Air Quality Prediction in South Africa With Machine Learning</title>
<link>http://spectrum.ieee.org/energywise/energy/environment/tackling-air-quality-prediction-in-south-africa-with-machine-learning</link>
<description>IBM is expanding its machine learning technology to predicting air quality from China to South Africa</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;IBM is expanding its machine learning technology to predicting air quality from China to South Africa&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwNDA0Ng.jpeg"/&gt;
&lt;figcaption&gt;Photo: IBM Research&lt;/figcaption&gt;
&lt;figcaption&gt;Tapiwa M. Chiwewe is a research scientist at IBM Research in Johannesburg, South Africa, where he and colleagues are expanding the company's machine learning technology to predicting air quality.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Machine learning is nipping at the heels of conventional physical modeling of air quality predictions in more and more places. The latest is Johannesburg, South Africa, where computer engineer &lt;a shape="rect" href="http://researcher.ibm.com/researcher/view.php?person=za-TChiwewe"&gt;Tapiwa M. Chiwewe&lt;/a&gt; at the &lt;a shape="rect" href="http://researchweb.watson.ibm.com/labs/africa/index.shtml"&gt;newly opened IBM Research lab&lt;/a&gt; is adapting IBM’s air quality prediction software to local needs and adding new capabilities. The work is an expansion of the so-called &lt;a shape="rect" href="http://researchweb.watson.ibm.com/green-horizons/#fbid=sGnsXvOqrwR"&gt;Green Horizons&lt;/a&gt; initiative, in which IBM researchers partnered with Chinese government researchers and officials, starting two years ago.&lt;/p&gt;
&lt;p&gt;Last month, Chiwewe presented some of the Johannesburg lab’s first results, involving ground-level ozone level predictions, at the &lt;a shape="rect" href="https://ieee-indin2016.sciencesconf.org/"&gt;14th International Conference on Industrial Informatics&lt;/a&gt; in Poitiers, France. “You can do a lot of physics to understand how ozone is found in different places,” he says, “but what we did is we just collected a lot of data and trained these machines on it and they were able to predict [local ozone levels] without any knowledge of how ozone works in the atmosphere.”&lt;/p&gt;
&lt;p&gt;Like China, South Africa relies on coal power for a large part of its energy and suffers the consequences in the form of toxic, particulate-laden air. While Chiwewe says that he and his South African colleagues were able to re-use some of the air quality forecasting tool developed by their colleagues in China, they must also adapt it to local particularities. Johannesburg, for example, has a long mining history. The mining industry has left many exposed tailings piles and strong winds regularly pick up the finer particles, reducing air quality in neighborhoods downwind. Chiwewe says he hopes to develop a tool that might pick up on signs of rising winds and alert nearby residents.&lt;/p&gt;
&lt;aside class="inlay pullquote xlrg"&gt;“You can do a lot of physics to understand how ozone is found in different places, but what we did is we just collected a lot of data and trained these machines on it and they were able to predict [local ozone levels] without any knowledge of how ozone works in the atmosphere”&lt;span class="pq-attrib"&gt;—Tapiwa M. Chiwewe, IBM Research&lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;Johannesburg also lacks Beijing’s dense network of air quality monitoring stations: It has just eight stations compared to Beijing, which has &lt;a shape="rect" href="http://www.reuters.com/article/us-china-pollution-idUSKCN0VV0Y1"&gt;has 35&lt;/a&gt;, according to one report. The IBM system is designed to incorporate data also from other lower-cost sensors that might include just one or two types of measurements (such as particulate matter) rather than the full set of gases and particles measured at the main stations—Beijing may have around 1,000 of the smaller ones, Chiwewe says. So his team must adapt the “teaching” stage of their machine learning system to work with much sparser data and make up for it in creative ways. Until they have more sources of ground data they are working on an intermediate fix: so-called “virtual stations” that might use data from such remote-sensing platforms as satellites.&lt;/p&gt;
&lt;p&gt;All this should help guide authorities, who for now are providing IBM Research with data from public monitoring stations in return for free access to the resulting forecasts. As the forecasts mature, officials there or elsewhere might use them to order heavily polluting power plants to reduce production during inversions or other smog-inducing weather patterns. They might also use the tools in reverse to identify the sources of pollution, helping to enforce existing laws. Longer-term forecasts could help officials plan road placement and zoning to try to reduce emissions, or at least their health consequences.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 25 Aug 2016 13:20:00 GMT</pubDate>
<dc:creator>Lucas Laursen</dc:creator>
<guid>http://spectrum.ieee.org/energywise/energy/environment/tackling-air-quality-prediction-in-south-africa-with-machine-learning</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwNDA2MA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwNDA1OA.jpg" height="225" width="300"/>
</item>
<item>
<title>HOPE Hacker Conference Shows Off New Tricks</title>
<link>http://spectrum.ieee.org/geek-life/reviews/hope-hacker-conference-shows-off-new-tricks</link>
<description>Reverse engineering the Iridium satellite network, a home pharma reactor, and a new censorship-resistant file-sharing system were all demoed in NYC</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Reverse engineering the Iridium satellite network, a home pharma reactor, and a new censorship-resistant file-sharing system were all demoed in NYC&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwMTI5OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Stephen Cass&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/MjgwMTI5OA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Stephen Cass&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;The Alpha Version: &lt;/strong&gt;This automated vessel for pharmaceutical reactions was made from a mason jar. A later prototype adds an Arduino-controlled syringe to introduce reagents and catalysts to in-progress reactions at timed intervals.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;In a steaming July in New York &lt;/strong&gt;City, hackers from around the world gathered for &lt;a shape="rect" href="https://hope.net/"&gt;The Eleventh HOPE,&lt;/a&gt; the latest installment of the biannual Hackers on Planet Earth conference organized by &lt;a shape="rect" href="https://www.2600.com/"&gt;2600 magazine&lt;/a&gt;. As in previous years, it was a gloriously grungy affair, with attendees wearing black T-shirts (adorned with geek references) crammed into the worn corridors and ballrooms of the Hotel Pennsylvania as they chugged on specially imported bottles of the caffeinated &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Club-Mate"&gt;Club-Mate&lt;/a&gt; drink. But there was also a new emphasis on inclusion for women and the LGBT community, appropriate for a conference that has always styled itself as politically conscious.&lt;/p&gt;
&lt;p&gt;This atmosphere was all the background to some eye-opening technical sessions. On the first day of the conference, Michael Laufer displayed a working prototype of an automated home-brew reactor for small-batch pharmaceutical production. The goal is to free patients from the kind of commercial price spikes made infamous when entrepreneur Martin Shkreli raised the price of the widely used antiparasitic drug pyrimethamine by over 5,500 percent in 2015. The prototype was essentially a mason jar with a modified lid. Temperature and pressure can be controlled as reagents, and catalysts are fed in via an Arduino-controlled syringe, but the real innovation is in the chemistry: Published drug-synthesis recipes are often intended for large-batch production by pharmaceutical companies, but Laufer and his colleagues have partnered with the company Chematica, which uses expert systems to find pharmaceutical recipes that are simple, have a large margin of error, and use cheap ingredients.&lt;/p&gt;
&lt;p&gt;Another session focused on reverse engineering the &lt;a shape="rect" href="https://www.iridium.com/"&gt;Iridium&lt;/a&gt; satellite communications network. Stefan Zehl and “Schneider” from the Munich Chaos Computer Club (CCC) used software-defined radio systems to look at and &lt;a shape="rect" href="https://github.com/muccc/gr-iridium"&gt;decode the signals&lt;/a&gt; streaming down from orbit. Each Iridium satellite uses beam antennas to illuminate roughly 400-kilometer-wide spots as it passes over the Earth, so a message intended for a recipient anywhere in that area is broadcast over the entire spot. When Iridium was originally designed in the 1990s, the difficulty of receiving signals without the network’s own hardware made amateur surveillance impossible, so much of the traffic on the network is not encrypted. But now the CCC hackers claim a modified GPS antenna and a &lt;a shape="rect" href="https://greatscottgadgets.com/hackrf/"&gt;software radio&lt;/a&gt; is all that’s required to pick up and demodulate signals. By studying packets on a byte-by-byte basis, they were able to identify and decode a number of the different types of messages transmitted by the satellite constellation—including pager messages, emails, and even voice calls, albeit not yet in real time—and presented several samples of each. (Iridium will soon begin launching a new generation of satellites, but they will be backward compatible with existing equipment, so a lot of unencrypted traffic is still likely to flow over the network.)&lt;/p&gt;
&lt;p&gt;Finally, a talk by &lt;a shape="rect" href="http://paulkernfeld.com/"&gt;Paul Kernfeld&lt;/a&gt; that may give movie and record executives pause marked the release of his &lt;a shape="rect" href="https://gist.github.com/paulkernfeld/4278533bf83887f6f0ee67765c66d54d"&gt;Exandria&lt;/a&gt; decentralized file-library software. Current peer-to-peer networks like &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/telecom/security/bittorrent-enters-chat-with-bleep"&gt;BitTorrent&lt;/a&gt; solve the problem of decentralizing file storage and exchange, but in order to find information, they still need centralized indexes. This has meant that those pirating movies and films—or exchanging information that repressive regimes don’t want shared—have relied on sites like The Pirate Bay, which are visible targets. But Kernfeld has created a system that stores the index in the &lt;a shape="rect" href="http://spectrum.ieee.org/video/computing/networks/video-the-bitcoin-blockchain-explained"&gt;Bitcoin blockchain&lt;/a&gt;, which is increasingly being used for &lt;a shape="rect" href="http://spectrum.ieee.org/computing/networks/the-future-of-the-web-looks-a-lot-like-bitcoin"&gt;applications beyond actual cryptocurrency.&lt;/a&gt; The system tries to guard against spam and fakes by requiring users to “burn” some bitcoins before adding content to the index, which means sending them to a specific Bitcoin address that is the financial equivalent of a black hole—you can verify that bitcoins have been sent to the address, but they can never be spent again by anyone.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the September 2016 print issue as “Hackers Show Off New Tricks.”&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 22 Aug 2016 19:00:00 GMT</pubDate>
<dc:creator>Stephen Cass</dc:creator>
<guid>http://spectrum.ieee.org/geek-life/reviews/hope-hacker-conference-shows-off-new-tricks</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwMTMxMg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwMTMxMA.jpg" height="225" width="300"/>
</item>
<item>
<title>Are Your Apps Sluggish? Blame Summer</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/wireless/are-your-apps-sluggish-blame-summer</link>
<description>Humidity and rain cause their own form of interference</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Humidity and rain cause their own form of interference&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwMjU1Mw.jpeg"/&gt;
&lt;figcaption&gt;Photo: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;During late August here in the United States, it can start to feel like everything is moving a little bit slower. In the case of your apps, that may actually be the case.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Earlier this year, a San Francisco company called &lt;a shape="rect" href="https://www.apteligent.com/"&gt;Apteligent&lt;/a&gt; released &lt;a shape="rect" href="https://data.apteligent.com/download-report?report=apteligent-data-report-april-2016.pdf"&gt;a report&lt;/a&gt; based on internal data that suggests app performance slows by 15 percent in the summer. The report identifies humidity as the culprit, though the company can’t say for sure based on their data why the extra delay occurs.&lt;/p&gt;
&lt;p&gt;However, it’s a reasonable guess that moisture in the air is the guilty party in light of &lt;a shape="rect" href="http://www.intechopen.com/books/electromagnetic-waves/atmospheric-attenuation-due-to-humidity"&gt;research&lt;/a&gt; showing that radio signals attenuate in humidity as well as in rain, sleet, and snow. As radio waves travel through humid air, water molecules absorb part of their energy and scatter another portion, weakening the signal or causing data packets to be lost altogether, says Milda Tamošiūnaitė of the &lt;a shape="rect" href="http://www.ftmc.lt/en"&gt;Center for Physical Sciences and Technology&lt;/a&gt; in Vilnius, Lithuania.&lt;/p&gt;
&lt;p&gt;This effect is particularly bad at frequencies above 1 gigahertz, which are used for LTE service, because those shorter wavelengths repeat more frequently and so have more opportunities to encounter obstacles as they travel. Those obstacles will also be larger relative to the size of the wave than obstacles encountered by waves at lower frequencies.&lt;/p&gt;
&lt;p&gt;Phone calls and data connections are both attenuated by rain, humidity, and other forms of airborne water—though phone calls are sometimes handled at lower frequencies, so they may be slightly less impacted than Web browsing. Previous research has often focused on the effect of rain on radio signals, but the specific role of humidity has been studied less.&lt;/p&gt;
&lt;p&gt;The informal study by Apteligent hints at humidity’s potential impact on app performance, though can’t be considered definitive. The company monitors tens of thousands of apps for clients including Hilton, Groupon, Netflix, and PokemonGo. Its clients embed a bit of special code into their apps, and the code allows Apteligent to track what users are doing, how much data they are sending and receiving, whether they experience any delays, and what might be causing those delays.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“If it's a smartphone in the U.S. that has apps, the odds are very high that we're embedded in one of those,” says Andrew Levy, Apteligent’s co-founder and chief strategy officer.  &lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;To examine the possible role of humidity in app performance, the company compared the average latency across its entire U.S. network of smartphone apps during the summer of 2015 with its performance the following winter. They found that service was about 15 percent slower in the summer than in the winter. Their theory is that humidity caused the bulk of this impact.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Ultimately, the average delay only worsened by about 60 milliseconds—a period of time that customers aren’t likely to notice. For comparison, Tamošiūnaitė says light rain could attenuate a 2 GHz signal by 15 percent at a distance of about 3000 kilometers, or at 128 kilometers during heavy rain (Note: this example assumes that rain is the only factor causing signal degradation, which is never the case in real life).  &lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;So what does all of this mean for developers? &lt;a shape="rect" href="http://arctouch.com/2016/02/app-maker-paulo-michels/"&gt;Paulo Michels&lt;/a&gt;, VP of engineering for app development company &lt;a shape="rect" href="http://arctouch.com/"&gt;ArcTouch&lt;/a&gt;, says it won’t change his approach very much. He and his team of 60 software engineers, who have developed roughly 300 apps, aren’t focused on weather more than any other factor when building a new app. They already use common strategies such as compressing JPEGs, pre-processing videos to allow them to stream at multiple potential qualities based on a user’s network, and caching content on phones in order to avoid delays.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“The network, of course, plays a big effect on overall app performance, but as mobile developers, we're used to considering the network as something unreliable and unpredictable,” he says.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;a shape="rect" href="https://www.linkedin.com/in/ericrichardson97"&gt;Eric Richardson&lt;/a&gt;, senior software engineer for &lt;a shape="rect" href="http://willowtreeapps.com/"&gt;WillowTree&lt;/a&gt;, who has worked on more than 35 Android apps, says 60 milliseconds is no more than “the blink of an eye” and designing to account for peculiar weather conditions is not a major priority beyond the measures that developers already take for poor network connections.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;But he also says the Apteligent report might mean that developers should start to make an effort to test their apps in humid conditions as well as on dry days. Right now, his company relies primarily on simulated 3G and 4G networks running on Wi-Fi to evaluate their apps, as well as some beta testing in the real world.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“Up until now, I don't think weather has ever been on our minds,” he says. “But now that it is, I guess it kind of brings in the perspective to do more realistic testing as opposed to just sitting in the office connected to Wi-Fi.”&lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 19 Aug 2016 14:30:00 GMT</pubDate>
<dc:creator>Amy Nordrum</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/wireless/are-your-apps-sluggish-blame-summer</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwMjUwNA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwMjUwMg.jpg" height="225" width="300"/>
</item>
<item>
<title>Fighting Poverty With Satellite Images and Machine-Learning Wizardry</title>
<link>http://spectrum.ieee.org/tech-talk/aerospace/satellites/fighting-poverty-with-satellite-data-and-machine-learning-wizardry</link>
<description>Computer models combine daytime satellite images, nighttime lighting, and survey data to estimate poverty levels</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Computer models combine daytime satellite images, nighttime lighting, and survey data to estimate poverty levels&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/MjgwMTk0Ng.jpeg"/&gt;
&lt;figcaption&gt;Image: Craig Mayhew and Robert Simmon/GSFC/NASA&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Governments and NGOs need economic data to decide how best to aid the poor. But reliable, up-to-date data on poverty levels is hard to come by in the world’s poorest countries.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Scientists have now devised an inexpensive technique that combines satellite images and machine learning to accurately predict poverty levels at village level. Such a fine-grained gauge of poverty could help aid programs target those with the greatest needs. It could also be a valuable tool for researchers and policymakers to gather national statistics and set development goals.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Governments typically conduct surveys of income and consumption to measure poverty levels. These surveys cost hundreds of millions of dollars and are impossible to conduct in areas of conflict. World Bank data show that between 2000 and 2010, 39 out of 59 African countries conducted fewer than two surveys that were extensive enough to measure poverty.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Researchers have recently tried to &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/consumer-electronics/portable-devices/mobile-phone-data-predicts-poverty-in-rwanda"&gt;estimate poverty levels by analyzing mobile phone usage data&lt;/a&gt; and satellite photos showing nighttime lighting. But mobile phone data are typically not publicly available. Nighttime lights, meanwhile, indicate wealthier regions, but they cannot differentiate among economic levels in the most impoverished regions. “In the poorest areas in Africa, the ones we care the most about, it’s almost uniformly dark at night,” says Neal Jean, an electrical engineering and computer science Ph.D. student at Stanford University.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Jean, earth system science professor Marshall Burke, and their colleagues came up with a clever machine-learning method that combines nighttime light intensity data with daytime satellite imagery. &lt;span&gt;The technique, reported in the journal &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://science.sciencemag.org/content/353/6301/790"&gt;
&lt;em&gt;Science&lt;/em&gt;
&lt;/a&gt;
&lt;span&gt;, is general and could be applied to any developing country, Jean says. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;In machine learning, a computer model is fed labeled data sets—say, thousands of images labeled “dog” or “cat.” Much like humans learn by inference after seeing enough examples, the model analyzes certain features in the images and figures out how to classify an animal in a picture as a dog or cat.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The researchers trained their machine-learning algorithm with millions of daytime satellite images, each labeled with a number that corresponded to how bright the area was at night. Daytime images, which contain features &lt;span&gt;that indicate livelihoods, &lt;/span&gt;such as paved roads, metal roofs, and farmland, can help distinguish poor regions from ultrapoor ones. “The model looks for visual cues and automatically learns to find features in daytime imagery that correspond to nighttime light values,” Jean says.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Next, the team trains a second computer model to use the subtle differences in the images that are picked out by the first model in cooperation with existing economic survey data. This second model learns to estimate a village’s relative level of poverty—measured by the consumption expenditures in 2011 U.S. dollars and an asset-based wealth index. “So you can take an image of any area and predict how poor that area is,” he says.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Both the daytime satellite images from Google Maps and the nighttime light data from the National Geophysical Data Center are available in high resolution; 1-square-kilometer images can be pulled up for just about any point on the globe. But for the purposes of this research, the Stanford team used satellite and survey data for five countries: Nigeria, Tanzania, Uganda, Malawi, and Rwanda.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;The new model more accurately estimated poverty levels than models that used only nighttime light data in areas where the average income was half or even one-third of the poverty level.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;New studies would just require training the model with satellite imagery from the regions being evaluated. The team is now trying to use images with different resolutions, which yield different information—say, building density at low-res or roofing material at high-res—to see how having that information affects the accuracy of poverty estimates.&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 18 Aug 2016 18:00:00 GMT</pubDate>
<dc:creator>Prachi Patel</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/aerospace/satellites/fighting-poverty-with-satellite-data-and-machine-learning-wizardry</guid>
<media:content url="http://spectrum.ieee.org/image/MjgwMTk2MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/MjgwMTk1OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Robot Octopus Points the Way to Soft Robotics With Eight Wiggly Arms</title>
<link>http://spectrum.ieee.org/robotics/robotics-hardware/robot-octopus-points-the-way-to-soft-robotics-with-eight-wiggly-arms</link>
<description>A squishy underwater robot with limbs that bend in every direction requires unusual control strategies</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;A squishy underwater robot with limbs that bend in every direction requires unusual control strategies&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MTA3Ng.jpeg"/&gt;
&lt;figcaption&gt;Photo: Jennie Hills/London Science Museum&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;style type="text/css"&gt;a.zoom .magnifier {
    margin: -390px 10px 0 0 !important;
&lt;/style&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="Cecilia Laschi with octopus" src="http://spectrum.ieee.org/image/Mjc5MTA3NQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Jennie Hills/London Science Museum&lt;/figcaption&gt;
&lt;figcaption&gt;The author exhibits one of her octo-bot creations.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="gif img" src="http://spectrum.ieee.org/image/Mjc5MzcyMw.gif"/&gt;
&lt;figcaption class="hi-cap"&gt;Gif: The BioRobotics Institute/Scuola Superiore Sant’Anna&lt;/figcaption&gt;
&lt;figcaption&gt;An octo-bot takes the plunge.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The sun was sparkling on the Mediterranean Sea on the afternoon when a graduate student from my lab tossed our prize robot into the water for the first time. I watched nervously as our electronic creation sank beneath the waves. But the bot didn’t falter: When we gave it the command to swim, it filled its expandable mantle with water, then jetted out the fluid to shoot forward. When we ordered it to crawl, it stiffened its eight floppy arms in sequence to push itself along the sandy bottom and over scattered rocks. And when we instructed it to explore a tight space beneath the dock, the robot inserted its soft body into the narrow gap without difficulty.&lt;/p&gt;
&lt;p&gt;As a &lt;a shape="rect" href="http://sssa.bioroboticsinstitute.it/user/57"&gt;professor&lt;/a&gt; at the &lt;a shape="rect" href="http://sssa.bioroboticsinstitute.it/"&gt;BioRobotics Institute&lt;/a&gt; at the Scuola Superiore Sant’Anna, in Pisa, Italy, I lead a team investigating soft robotics. This relatively new field of research has the potential to upend our ideas about what robots are capable of and where they can be useful. I chose to build robots that mimic the form of the octopus for two reasons. First, because they’re well suited to demonstrate the many advantages that come when a machine can flex and squish as needed. Also, it’s an excellent engineering challenge: An octopus with eight wiggly arms, which must work together in the face of complex hydrodynamic forces, is very difficult to design and control.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="gif img" src="http://spectrum.ieee.org/image/Mjc5MzcyNA.gif"/&gt;
&lt;figcaption class="hi-cap"&gt;Gif: The BioRobotics Institute/Scuola Superiore Sant’Anna&lt;/figcaption&gt;
&lt;figcaption&gt;Octo-bot arms can bend and grip.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In the course of our research, my team hoped to provoke a fundamental rethinking of robotic theories and techniques. We wanted to showcase materials that could be used in actuators that bend and stretch. More crucially, we sought to develop strategies for operating a robot that can curl its limbs in any direction, making it far more tricky to control than a rigid, articulated robot with limbs that have just a few degrees of freedom. To address these challenges, we drew inspiration from nature’s design of the remarkable flesh-and-blood octopus.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Up until recently, robots have mainly&lt;/strong&gt; been used in factories, where their rigid arms are well suited for the repetitive tasks at hand and the accuracy required. Now, however, roboticists want to put their creations to work in more unpredictable settings where conventional robots often run into trouble.&lt;/p&gt;
&lt;p&gt;Some researchers want to build flexible robots that can navigate irregular landscapes, like the ocean floor or the &lt;a shape="rect" href="http://www.esa.int/gsp/ACT/doc/AI/pub/ACT-RPR-AI-2015-GECCO-softrobots.pdf"&gt;surface of Mars&lt;/a&gt; [pdf]. These robots must move over rough terrain without getting stuck and need manipulators that can grab whatever strangely shaped objects they encounter. Other researchers are focusing on soft robots that can be trusted not to hurt the people they come into contact with. Such soft robots could, for example, work as aides for the disabled or the elderly, and miniature soft robots could even &lt;a shape="rect" href="http://www.kcl.ac.uk/newsevents/news/newsrecords/2015/December/Worlds-first-soft-robotic-surgery-on-a-human-body-.aspx"&gt;serve as surgical tools&lt;/a&gt;
&lt;em&gt;inside&lt;/em&gt; the body.&lt;/p&gt;
&lt;aside class="inlay rt sm"&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/video/robotics/robotics-hardware/octopusinspired-robots-can-grasp-crawl-and-swim"&gt;
&lt;figure role="img"&gt;
&lt;img alt="graphic link to full video page" src="http://spectrum.ieee.org/image/MjgwMDIwMg.jpeg"/&gt;
&lt;figcaption&gt;
&lt;strong&gt;Video: See how the octo-bot crawls, grasps, and swims.&lt;/strong&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;In pursuit of these goals, robotics researchers are increasingly studying animals. That makes sense because the bodies of animals are composed mostly of soft materials, with pliable joints and tissue that can change shape without damage. Because their soft tissues absorb shocks and can conform to varied surfaces, animals can use simple control strategies that don’t demand great precision.&lt;/p&gt;
&lt;p&gt;That, in a nutshell, is why I helped launch the &lt;a shape="rect" href="http://www.octopusproject.eu/index.html"&gt;Octopus Integrating Project&lt;/a&gt;. The effort brought together several labs from European and Israeli universities, which began working together in 2009 to build a robot replica of the fascinating animal. Some of the consortium members had worked on a previous effort that resulted in &lt;a shape="rect" href="https://www.youtube.com/watch?v=6fhpt6MWDDE"&gt;an “OctArm” attached to a tanklike robot&lt;/a&gt;, and they eagerly joined the new effort to copy the animal’s remarkable capabilities. We knew it wouldn’t be easy.&lt;/p&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;a class="zoom" shape="rect" rel="lightbox" href="http://spectrum.ieee.org/image/MjgwMTA0Ng.jpeg"&gt;
&lt;img alt="illustration of octopus" src="http://spectrum.ieee.org/image/Mjc5NzE2NA.png"/&gt;
&lt;span class="magnifier"&gt; &lt;/span&gt;
&lt;/a&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: Emily Cooper&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Octo-anatomy:&lt;/strong&gt; In a real octopus’s arm, a criss-cross arrangement of muscles provides all the movement. When the longitudinal muscles contract the arm gets shorter and fatter; when the transverse muscles contract the arm gets longer and skinnier.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The octopus has neither an internal nor external skeleton, and its eight arms can bend at any point, elongate and shorten, and stiffen to apply force. It can twist its arms around objects and manipulate them with great dexterity, as demonstrated in plenty of entertaining YouTube videos, including one where the animal &lt;a shape="rect" href="https://www.youtube.com/watch?v=x5DyBkYKqnM"&gt;steals a camera&lt;/a&gt; from an underwater photographer and another where it &lt;a shape="rect" href="https://www.youtube.com/watch?v=IvvjcQIJnLg"&gt;releases itself from a jar&lt;/a&gt; by unscrewing the lid from the inside. An octopus needs such dexterity to survive in the wild. When it crawls along the seafloor, for example, its arms must coordinate their movements in a complex rippling sequence to push and pull its body forward.&lt;/p&gt;
&lt;p&gt;We wanted to build a robot that could replicate those agile motions. We started by studying the octopus arm’s &lt;a shape="rect" href="https://www.researchgate.net/publication/227668358_Tongues_tentacles_and_trunks_the_biomechanics_of_movement_in_muscular-hydrostats_Zool_J_Linn_Soc"&gt;muscular hydrostat&lt;/a&gt; structure, which allows the overall volume of the arm to remain constant while individual muscles contract and change shape. So when the diameter of an arm decreases, its length increases, and vice versa. To translate biology into engineering, we worked with marine biologists to take measurements of octopus arms and make computer models that could inform our designs. Then we began experimenting with soft actuators that could mimic the animal’s muscles.&lt;/p&gt;
&lt;p&gt;One option was to make artificial muscles using materials known as &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Electroactive_polymers"&gt;electro-active polymers&lt;/a&gt; (EAPs). A layer of a soft material is sandwiched between two electrodes; when a voltage is applied, the EAP acts as a capacitor and the electrodes draw closer together, squeezing the soft material between them. Exploiting this phenomenon, researchers have created contractile units that can be arranged in stacks to generate significant forces. A &lt;a shape="rect" href="http://www.euroeap.eu/index.php"&gt;research network in Europe&lt;/a&gt; is actively pursuing EAPs for artificial muscles.&lt;/p&gt;
&lt;p&gt;Another possibility was to construct our robotic arms using fluidic actuators, in which liquids or gases fill soft chambers to change the shape of the larger structure. Clever design of the shapes and arrangement of the compartments allow a robotic arm to &lt;a shape="rect" href="https://www.youtube.com/watch?v=LCp1DK_FQZ0"&gt;bend in the desired directions&lt;/a&gt; and may eventually enable more complicated movements.&lt;/p&gt;
&lt;p&gt;Yet another interesting approach relies on filling a chamber with a granular material, such as sand or even &lt;a shape="rect" href="http://www.news.cornell.edu/stories/2010/10/researchers-develop-universal-robotic-gripper"&gt;ground coffee&lt;/a&gt;, instead of a fluid. With this technique, called &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Jamming_(physics)"&gt;jamming&lt;/a&gt;, the soft robot remains pliable until a vacuum is applied. Then the robot’s body stiffens into a hard shape—like a vacuum-packed brick of coffee on the grocery-store shelf. By applying vacuum to discrete sections in programmed sequences, researchers can &lt;a shape="rect" href="http://spectrum.ieee.org/automaton/robotics/robotics-software/irobot-soft-morphing-blob-chembot"&gt;make soft robots stiffen and move&lt;/a&gt; in specific ways.&lt;/p&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="gif img" src="http://spectrum.ieee.org/image/Mjc5MzcyNQ.gif"/&gt;
&lt;figcaption class="hi-cap"&gt;Gif: The BioRobotics Institute/Scuola Superiore Sant’Anna&lt;/figcaption&gt;
&lt;figcaption&gt;An octo-bot’s actuators mimic real octopus muscles.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;My team was most interested in creating artificial muscles using materials called shape-memory alloys (SMAs). When heated, SMAs deform to a predefined shape, which they “remember.” We fashioned SMA wires into springs and ran electric current through them to heat them, causing the springs to scrunch up in a way that imitates muscular contractions. For the Octopus project, my team constructed &lt;a shape="rect" href="http://www.mdpi.com/2076-0825/3/3/226"&gt;a prototype arm&lt;/a&gt; using SMA springs to stand in for the longitudinal and transverse muscles found in the limbs of a real octopus. By sending current through different sets of springs, we made the underwater arm bend at multiple points, shorten and elongate, even grasp things.&lt;/p&gt;
&lt;p&gt;Our work is primarily meant to demonstrate the potential of soft robotics, and much work remains before a robot octopus will be ready to crawl out of the lab. For example, a bot with sensors on its limbs could provide feedback about its position and the materials it encounters, which could lead to better control strategies. A team of researchers at the Worcester Polytechnic Institute, in Massachusetts, is addressing just that challenge by &lt;a shape="rect" href="http://softrobotics.wpi.edu/papers/2016/2016-Ozel-ICRA.pdf"&gt;embedding proprioceptive sensors in a robotic snake&lt;/a&gt; [pdf].&lt;/p&gt;
&lt;p&gt;It’s fun to imagine how an advanced robot octopus with eight dexterous arms could perform in the wild. Take the marine-energy industry, where there’s great interest in placing &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/green-tech/geothermal-and-tidal/tidal-energy-could-supply-half-of-scotlands-power"&gt;tidal turbines&lt;/a&gt; on the seabed to harvest power from the flowing water. But if the machinery breaks, repairs would be difficult and expensive: Workers would have to either haul turbines up to the surface or send human divers down. Maybe, one day, an octo-bot technician could be sent instead. With its agile limbs, it could manipulate tools and fix whatever is broken.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;We roboticists aren’t interested&lt;/strong&gt; in the octopus for its limbs and muscles alone—we also value its particular brand of intelligence. The octopus’s brain and peripheral nervous system are well developed compared with those of other mollusks, but they’re still fairly limited. It’s surprising, then, that they can control a huge range of movements in eight independent arms. So our next challenge under the Octopus project was to study how the animal controls its arms. We hoped the results would help us find ways to manage a flexible robot’s complex movements.&lt;/p&gt;
&lt;p&gt;Biologists have determined that the octopus’s brain doesn’t issue top-down commands for every small movement of its twisty limbs. In &lt;em&gt;Octopus vulgaris&lt;/em&gt;, the common octopus, the brain actually contains far fewer neurons than the peripheral nervous system. Biologists believe that the brain initiates motions, while lower motor centers control the precise neuromuscular activity. Experiments have shown that even if you sever the nerves descending from an octopus’s brain, its arms can &lt;a shape="rect" href="http://www.sciencedirect.com/science/article/pii/S0022098113000683"&gt;still recoil&lt;/a&gt; from unpleasant stimuli and &lt;a shape="rect" href="http://science.sciencemag.org/content/293/5536/1845"&gt;reach out&lt;/a&gt; as if to grab something.&lt;/p&gt;
&lt;p&gt;And here’s what we found even more interesting: The octopus’s limbs don’t need comprehensive directions to produce the desired movement. Thanks to millions of years of evolution, their bodies are designed to respond to their environment in certain automatic and useful ways. This concept is often called &lt;a shape="rect" href="http://www.igi.tu-graz.ac.at/maass/psfiles/209.pdf"&gt;morphological computation&lt;/a&gt; [pdf] by roboticists, while artificial intelligence researchers refer to it as embodied intelligence.&lt;/p&gt;
&lt;p&gt;When &lt;a shape="rect" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=C511B85E660267A0A6E650CFD5E71F70?doi=10.1.1.176.386&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;translated to the robot world&lt;/a&gt; [pdf], this principle means we should design our robots so that the physical properties of their bodies automatically produce the desired movements. With this strategy, extremely simple commands can cause a robot to efficiently carry out complex tasks.&lt;/p&gt;
&lt;p&gt;We kept this principle in mind when we first set out to make a robot octopus that could &lt;a shape="rect" href="http://octopus.huji.ac.il/site/articles/Calisti-2011.pdf"&gt;crawl along the seafloor&lt;/a&gt; [pdf]. We studied the locomotion strategy of the real animal, and determined that it uses a four-step procedure to crawl. First it attaches one of its rear limbs, which are covered in suckers, to the bottom. Then it elongates that limb to push the rest of its body forward. In the third stage it pulls up its suckers to detach the limb from the seafloor. And finally, it shortens the limb to bring it closer to its body and prepare for the next push. The animal ripples forward as two of its arms perform these actions in graceful sequence.&lt;/p&gt;
&lt;p class="jwcode"&gt;
&lt;script class="jwembed" src="//content.jwplatform.com/players/c517QlTk-7pFgM9ap.js"/&gt;
&lt;/p&gt;
&lt;p&gt;To mimic this form of locomotion, it might seem that our octo-bot would need to carry out a huge number of computations to control its eight arms, each of which can bend at any point. In a typical robot, each degree of freedom for a limb requires an actuator to drive movement in that direction and some sort of numerical controller to govern the actuator. But our robotic arms had too many degrees of freedom to apply the usual control strategy.&lt;/p&gt;
&lt;p&gt;We decided to follow the example of evolution and try to fashion arms that didn’t require complex control inputs. Before we built anything, we formulated mathematical models to test various aspects of the arm’s design, including the density of the material used, its stiffness, its shape, the internal placement of its artificial muscles, and so on. All of these parameters needed to be examined in the context of how the arms would perform underwater. How would these arms fare in water of different salinities and temperatures? How would increased depth and pressure affect them? How would currents and turbulence influence their movement? To answer those questions, we added hydrodynamic factors to our models. We also had to consider the texture and composition of the surfaces over which they’d be crawling. We decided not to complicate our design by adding suckers to the arms, instead using a material for the exterior that would produce plenty of friction.&lt;/p&gt;
&lt;p&gt;With all those parameters in the model, the task of finding the optimal combination became far too complex for anyone to compute by trial and error, so we used an &lt;a shape="rect" href="http://www.solver.com/genetic-evolutionary-introduction"&gt;evolutionary algorithm&lt;/a&gt; to explore the vast range of possibilities. This algorithm started by creating many hypothetical octopus body shapes, each with its own set of characteristics. Then it tested those octopuses’ arms to see which performed best in the models, and used the “fittest” limbs’ attributes to inspire a new batch of possibilities. In this way, we identified an arrangement that would generate the correct amount of propulsive force and produce the desired crawling movement.&lt;/p&gt;
&lt;p&gt;The results were very gratifying: In particular, it proved simple to control the octo-bot, and we could mimic the four-step crawling we’d observed in the real animal. For this prototype, which was intended to showcase control mechanisms rather than materials, we swapped out the SMAs and instead used cables as artificial muscles. Each silicone-rubber arm contained a steel cable that lengthened or shortened the arm and a carbon fiber cable that bent the arm for the attachment and detachment steps (and also enabled the robot to grasp objects). One simple servomotor in each arm provided power.&lt;/p&gt;
&lt;p class="jwcode"&gt;
&lt;script class="jwembed" src="//content.jwplatform.com/players/XjxZgmtC-7pFgM9ap.js"/&gt;
&lt;/p&gt;
&lt;p&gt;An enormous number of sophisticated computations had been done at the time of the design. But when the robot was in action, we said proudly that it was very dumb: It had only a simple “brain,” or microcontroller, which triggered the arms in the correct sequence to make the octo-bot crawl by virtue of the mechanical properties of its body. Our octo-bot demonstrated morphological computation at work.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;In 2012, my team at the BioRobotics Institute&lt;/strong&gt; began a related project called &lt;a shape="rect" href="http://sssa.bioroboticsinstitute.it/projects/PoseiDRONE"&gt;PoseiDrone&lt;/a&gt;. For that venture, we decided to apply everything we’d learned to create a soft underwater robot that could not only crawl and grasp objects but also swim.&lt;/p&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="gif img" src="http://spectrum.ieee.org/image/Mjc5MzcyNg.gif"/&gt;
&lt;figcaption class="hi-cap"&gt;Gif: The BioRobotics Institute/Scuola Superiore Sant’Anna&lt;/figcaption&gt;
&lt;figcaption&gt;We devised a jet-propulsion system for an octo-bot.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To give this robot swimming abilities we once more relied on morphological computing. The key was the design of the octo-bot’s mantle—the headlike part of an octopus that swells up when it fills with water, then contracts to eject the water and generate jet propulsion. Again, we used computer models to determine the size, shape, and material properties of the silicone mantle, paying careful attention to the ways &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?&amp;amp;arnumber=7139337"&gt;the structure would deform&lt;/a&gt; as it ejected the water. Our algorithm produced the optimal combination that would, with only one small motor and a few simple cables, send the octo-bot jetting through the water.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="gif img" src="http://spectrum.ieee.org/image/Mjc5MzcyNw.gif"/&gt;
&lt;figcaption class="hi-cap"&gt;Gif: The BioRobotics Institute/Scuola Superiore Sant’Anna&lt;/figcaption&gt;
&lt;figcaption&gt;An octo-bot wiggles through the water.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;With our &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6741155"&gt;PoseiDrone prototype&lt;/a&gt;, we studied its ability to perform various tasks, such as crawling while toting an object and &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?&amp;amp;arnumber=7139337"&gt;propelling itself&lt;/a&gt; with a series of bursts while swimming. That prototype was the bot we tossed into the Mediterranean to see how it would fare in the great outdoors. We were proud to watch our robot deal handily with the unpredictable surfaces, waves, and currents it encountered, and at the conclusion of our experiments we found a suitable retirement for this exemplary specimen. The PoseiDrone robot will soon reside in a tank in the Livorno, Italy, aquarium—right next to a real octopus.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/video/robotics/robotics-hardware/octopusinspired-robots-can-grasp-crawl-and-swim"&gt;Video: See how the octo-bot crawls, grasps, and swims&lt;/a&gt;. &lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 15 Aug 2016 19:00:00 GMT</pubDate>
<dc:creator>Cecilia Laschi</dc:creator>
<guid>http://spectrum.ieee.org/robotics/robotics-hardware/robot-octopus-points-the-way-to-soft-robotics-with-eight-wiggly-arms</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MTA4OQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MTA4Nw.jpg" height="225" width="300"/>
</item>
<item>
<title>The Nervana Systems Chip That Will Let Intel Advance Its Deep Learning</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/the-chip-that-intel-wants-from-nervana-systems</link>
<description>Intel's purchase of Nervana gives the tech titan ownership of a specialized chip designed for deep learning</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Intel's purchase of Nervana gives the tech titan ownership of a specialized chip designed for deep learning&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5OTUwNw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Intel&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Deep-learning artificial intelligence has mostly relied upon the general-purpose GPU hardware used in many other computing tasks. But Intel’s recent acquisition of the startup Nervana Systems will give the tech giant ownership of a specialized chip designed specifically for deep learning AI applications. That could give Intel a huge lead in the race to develop next-generation artificial intelligence capable of swiftly finding patterns in huge data sets and learning through imitation.&lt;/p&gt;
&lt;p&gt;Nervana has leaned heavily on GPU hardware to build its own portfolio of deep-learning AI services for both companies and independent developers. But the startup has also been developing its own specialized deep-learning hardware, called &lt;a shape="rect" href="https://www.nervanasys.com/nervana-engine-delivers-deep-learning-at-ludicrous-speed/"&gt;Nervana Engine&lt;/a&gt;, that includes only the components necessary for running deep-learning algorithms and eliminates the extra components used for general-purpose GPU tasks. &lt;span&gt;Nervana claims that w&lt;/span&gt;hen the Engine chip comes out in 2017,  it will deliver around 10 times as much computing power for deep learning as the best of today’s GPUs.&lt;/p&gt;
&lt;p&gt;“Nervana’s AI expertise combined with Intel’s capabilities and huge market reach will allow us to realize our vision and create something truly special,” said Naveen Rao, CEO and cofounder of Nervana, in a &lt;a shape="rect" href="https://www.nervanasys.com/intel-nervana/"&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Software algorithms known as artificial neural networks are the heart of deep-learning AI. &lt;span&gt;Such algorithms learn how to perform certain tasks through imitation and by observing correctly labeled examples as they sift through huge amounts of data.&lt;/span&gt; &lt;span&gt;To accommodate deep learning’s voracious appetite for data, Nervana’s Engine hardware design includes High Bandwidth Memory technology that has stacked memory and densely packed data channels to swiftly move around large amounts of data.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;The end result: 32 gigabytes of on-chip storage and up to 8 terabits per second of memory access speed. By comparison, the &lt;/span&gt;GDDR5 memory technology used in GPUs has memory access speeds of just 224 gigabits per second.&lt;/p&gt;
&lt;p&gt;Intel clearly saw value in acquiring both Nervana and its deep-learning chip. Investors familiar with the acquisition deal pegged the startup’s purchase price at somewhere in the range of $408 million, according to &lt;a shape="rect" href="http://www.recode.net/2016/8/9/12413600/intel-buys-nervana--350-million"&gt;
&lt;em&gt;Recode&lt;/em&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The purchase gives Intel a possible edge in the market for deep-learning hardware while sidestepping the general-purpose GPUs produced by rival tech giant Nvidia, said Karl Freund, senior analyst for deep learning and HPC at Moor Insights &amp;amp; Strategy, in an interview with &lt;a shape="rect" href="http://www.eetimes.com/document.asp?doc_id=1330281"&gt;
&lt;em&gt;EE Times&lt;/em&gt;
&lt;/a&gt;. Intel currently produces multicore Xeon and Xeon Phi processors and other hardware, but has had no equivalent to the GPUs that currently dominate deep learning.&lt;/p&gt;
&lt;p&gt;
&lt;span&gt;“[Nervana’s] IP and expertise in accelerating deep-learning algorithms will expand Intel’s capabilities in the field of AI,” said Diane Bryant, executive vice president and general manager of the Data Center Group at Intel, in a &lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="https://newsroom.intel.com/editorials/foundation-of-artificial-intelligence/"&gt;blog post&lt;/a&gt;
&lt;span&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The 48-person Nervana team will remain at its San Diego headquarters and maintain a “startup mentality” as part of the deal. Nervana will also continue developing the Engine deep-learning hardware alongside other existing products such as its &lt;a shape="rect" href="https://www.nervanasys.com/technology/neon/"&gt;Neon deep-learning framework&lt;/a&gt;, a programming language and set of libraries intended to help outsiders create deep-learning models.&lt;/p&gt;
&lt;p&gt;Intel’s big bet on Nervana signifies the growing importance of deep learning and the broader field of machine learning. The purchase is&lt;span&gt; the latest in a string of deep-learning startup acquisitions by major tech companies such as Google, IBM, and Amazon. To learn more about the race between startups and tech titans to develop deep learning services, see the &lt;/span&gt;
&lt;em&gt;IEEE Spectrum&lt;/em&gt;
&lt;span&gt; article “&lt;/span&gt;
&lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://spectrum.ieee.org/computing/software/now-you-too-can-buy-cloudbased-deep-learning"&gt;Now You Too Can Buy Cloud-Based Deep Learning&lt;/a&gt;
&lt;span&gt;.”&lt;/span&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 12 Aug 2016 19:30:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/the-chip-that-intel-wants-from-nervana-systems</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5OTUyMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5OTUxOA.jpg" height="225" width="300"/>
</item>
<item>
<title>Delta Airlines: On Second Thought, the Computer Crash Was Our Fault</title>
<link>http://spectrum.ieee.org/tech-talk/computing/it/on-second-thought-the-computer-crash-was-our-fault</link>
<description>Carrier admits that its original story, blaming a local utility for setting off the chain of events that led to more than 1000 flight cancellations on Monday and Tuesday, was a work of fiction</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Carrier admits that its original story, blaming a local utility for setting off the chain of events that led to more than 1000 flight cancellations on Monday and Tuesday, was a work of fiction&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5ODM5MQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Carolyn Kaster/AP Photo&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;After having been called out by Georgia Power, the utility that delivers electric power to its Atlanta hub, Delta Airlines finally came clean. It admitted that the crash of its computer network at 2:30 a.m. EDT on Monday, 8 August, had &lt;a shape="rect" href="https://www.washingtonpost.com/local/trafficandcommuting/delta-identifies-cause-of-computer-crash-that-crippled-flights-monday/2016/08/09/65876f92-5e66-11e6-8e45-477372e89d78_story.html"&gt;nothing whatsoever to do with the power company&lt;/a&gt;—after Georgia Power came forward and confirmed that none of its other customers in that area had experienced a power outage.&lt;/p&gt;
&lt;p&gt;With that narrative revealed to be merely a canard, Delta says it has discovered the real root of the computer crash: One of its own power control modules went bad, allowing a surge that tripped circuits feeding the computer network that handles critical data including reservations, boarding passes, the matching of planes with the appropriate gates, and the roster showing which crew members are staffing each flight. The network is supposed to instantly switch over to backup systems. But as tens of thousands of stranded passengers have learned over the past few days, results may vary.&lt;/p&gt;
&lt;p&gt;“Critical systems and network equipment didn’t switch over to backups,” Delta chief operating officer Gil West said in a statement. “Other systems did. And now we’re seeing instability in these systems.” So much so that roughly 800 flights were canceled on Tuesday.&lt;/p&gt;
&lt;p&gt;“This makes my point that that backup failure is a recurring pattern,” says Robert Charette. &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/it/delta-airlines-downed-by-it-failure"&gt;Earlier this week&lt;/a&gt;, he told &lt;em&gt;IEEE Spectrum&lt;/em&gt;, “What you’ll see in reviewing [these airline computer system failures] is recurring problems with infrastructure (i.e., power, networks, routers, servers, etc.) that seem to keep surprising the airlines. In every case I can recall, there were backup systems in place, but they failed—another recurring theme.” &lt;/p&gt;
&lt;p&gt;In a &lt;a shape="rect" href="http://news.delta.com/delta-ceo-bastian-isn-t-who-we-are"&gt;video message&lt;/a&gt; posted on the airline’s website on Tuesday, Delta CEO Ed Bastian said, “This isn’t who we are.” He added, “Over the past three years, we have invested hundreds of millions of dollars in technology infrastructure upgrades and systems, including backup systems to prevent what happened yesterday.” That’s an IT project that would likely fit with the theme of last October’s &lt;em&gt;IEEE Spectrum&lt;/em&gt; special report, “&lt;a shape="rect" href="http://spectrum.ieee.org/static/lessons-from-a-decade-of-it-failures"&gt;Lessons From a Decade of IT Failures&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;But according to Bastian, passengers can rest assured. “We’ll do everything we can to make certain that this never happens again,” Bastian said.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 10 Aug 2016 18:30:00 GMT</pubDate>
<dc:creator>Willie D. Jones</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/it/on-second-thought-the-computer-crash-was-our-fault</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5ODQwMQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5ODM5OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Simulation Accelerates Innovation</title>
<link>http://spectrum.ieee.org/computing/software/simulation-accelerates-innovation</link>
<description>Industry leaders from around the globe use multiphysics simulation to stay ahead of the curve</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Industry leaders from around the globe use multiphysics simulation to stay ahead of the curve&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc1MjU0NQ.png"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;h2&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc1MjU0NA"/&gt;
&lt;/h2&gt;
&lt;p&gt;The latest edition of Multiphysics Simulation is a window into the latest simulation and modeling done by design engineers in industries ranging from touchscreens to biosensors. Learn how leading tech companies are incorporating multiphysics simulation into their workflow to solve their design challenges, and accelerate research and development.&lt;/p&gt;
&lt;p&gt;Researchers in the Early Technologies unit of Medtronics Minimally Invasive Therapies Group (MITG) at Medtronic used COMSOL Multiphysics® software to improve ablation probes, enhancing predictability of the ablation procedures and overall physician control. They modeled heat transfer, mass transport, and microwave propagation from the ablation probe to the damaged tissue, and tested probes that incorporate radiometric sensing in the same device. The team also evaluated the temperature dependence of relevant reaction rates, modeled phase changes, and radiometrically characterized the energy absorbed by the tissue, in the process of developing probes with enhanced reliability and precision.&lt;/p&gt;
&lt;p&gt;Plasmonics researchers at the Birck Nanotechnology Center of Purdue University are simulating graphene layers for opto-electronic devices, fine-tuning their design and reducing their total number of nanofabricated prototypes. The researchers modeled tunable graphene-assisted damping of plasmon resonances in nanoantenna arrays in the mid-infrared range. Numerical simulation allowed the optimization of design without the cost and complication of nanofabrication. The engineers continue to use multiphysics simulation to further characterize the properties of graphene for use of the material in devices such as photovoltaics, optical modulators, and flexible touchscreens.&lt;/p&gt;
&lt;p&gt;Researchers and designers at Roche Diagnostics are using numerical simulation to design new sensing methods for glucose monitors to provide more accurate readings. A robust design is necessary so that the sensors continue to give proper results even when used in uncertain conditions. The team used COMSOL software to evaluate different electrode configurations and materials, as well as to predict the dependence of the electric potential on hematocrit level and other physical variables, such as capillary height and spacer placement, values that would be very difficult to measure experimentally.&lt;/p&gt;
&lt;p&gt;Engineers at FUJIFILM Dimatix used numerical simulation to gather compliance data for improving industrial printhead actuator performance. The primary concerns for actuator design are maximizing deflection, minimizing size, and matching the actuators impedance to the flow channels and the nozzle. The results allowed them to fit a new device to tight specifications and smaller actuator geometries and revealed new information about their actuator and jet. Read on to learn how other leading tech companies are using multiphysics simulation to solve their design challenges.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More&lt;/strong&gt;: &lt;a shape="rect" href="http://comsol.com/c/3wwj"&gt;http://comsol.com/c/3wwj&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 10 Aug 2016 15:23:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/simulation-accelerates-innovation</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc1MjU1Mw.png" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc1MjU1MQ.png" height="225" width="300"/>
</item>
<item>
<title>Touchscreen Design with Computational Apps</title>
<link>http://spectrum.ieee.org/computing/software/touchscreen-design-with-computational-apps</link>
<description>Applications streamline capacitive touchscreen design and optimize the workflow</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Applications streamline capacitive touchscreen design and optimize the workflow&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc1MTgxMg.png"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc1MTgxMQ"/&gt;
&lt;/div&gt;
&lt;p&gt;Capacitive touchscreens are seemingly ubiquitous, appearing in more and more products, from cars and kitchen appliances to kids' toys. Companies must meet or exceed the industry standard to stay competitive. In the touchscreen industry, this amounts to a capacitive touchscreen that is equally or more accurate and precise as touchscreens in competing devices. In practice, a full characterization of electrical and thermal effects is necessary to ensure proper functioning of a touchscreen device. Let's quickly review the physics behind a capacitive touchscreen.&lt;/p&gt;
&lt;p&gt;The underlying principle of capacitive touchscreens is capacitance, or the amount of energy that can be stored in the electric field between two conductive objects. A capacitor consists of two parallel metal plates with a dielectric material between them. Bringing a dielectric material between two conductive objects or changing the distance between the plates alters the capacitance between them. A capacitive touchscreen has a large number of electrode pairs, each with a capacitance-to-finger location curve that precisely maps the finger location.&lt;/p&gt;
&lt;p&gt;Design engineers consider factors ranging from number of electrodes necessary for accuracy, to finger moisture and how hard the screen is pressed. As there is more than one physical effect to consider and fast development is key, multiphysics simulation software is an indispensable tool in the design of these devices.&lt;/p&gt;
&lt;p&gt;With COMSOL Multiphysics® software and computational apps the full characterization of physical effects and complete optimization of performance is streamlined. First the engineer sets up the simulation within the Model Builder of COMSOL Multiphysics®. Then, with the Application Builder, it is straightforward to convert the mathematical model into an easy-to-use app. The app designer has full control over which parts of the simulation will be shared and modifiable by the app user.&lt;/p&gt;
&lt;p&gt;The computational app is much simpler to interact with than the underlying multiphysics model. As such, the crucial insight derived from the full simulation can be easily shared with a larger team by way of the app, speeding overall product development. Continue reading to learn how to build a custom app for a touchscreen device within COMSOL.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a shape="rect" href="http://comsol.com/c/3ww9"&gt;http://comsol.com/c/3ww9&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 8 Aug 2016 23:57:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/touchscreen-design-with-computational-apps</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NzA3OQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NzA3Nw.jpg" height="225" width="300"/>
</item>
<item>
<title>Delta Airlines Computer Failure Part of a Pattern</title>
<link>http://spectrum.ieee.org/tech-talk/computing/it/delta-airlines-downed-by-it-failure</link>
<description>Hundreds of flight cancellations today are just the latest in a string of airline industry IT problems</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Hundreds of flight cancellations today are just the latest in a string of airline industry IT problems&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NzQxNw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Giordano Ciampini/Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;If you had plans to travel on &lt;a shape="rect" href="http://news.delta.com/flights-resuming-delays-cancels-continue-after-power-outage"&gt;Delta Airlines&lt;/a&gt; today or tomorrow, you &lt;em&gt;had&lt;/em&gt; plans. At this point, you might want to consider another carrier, a rental car, a bus, or even a bicycle with a basket on the handlebars. That’s because, according to Delta, a power outage that wreaked havoc on its hub in Atlanta brought down the airline’s computer system. And one expert says it’s just the latest data point in a string of airline industry IT problems.&lt;/p&gt;
&lt;p&gt;The outage wasn’t just local—Delta flights all across the globe were affected by the computer system shutdown, which lasted for around six hours and ended just before 9 a.m. eastern time this morning. Flights already in progress were completed as scheduled, but the computer snafu meant that, in the words of an old commercial for roach motels, planes could check in, but they couldn’t check out.  &lt;/p&gt;
&lt;p&gt;Though Delta officials &lt;a shape="rect" href="http://money.cnn.com/2016/08/08/news/companies/delta-system-outage-flights/"&gt;told CNN&lt;/a&gt; that flight departures have resumed on a limited basis, “Customers heading to the airport should expect delays and cancellations.” Thus far, the world’s second-largest airline has canceled hundreds of its 15,000 daily flights. The reverberations from those flights are bound to last for days.&lt;/p&gt;
&lt;p&gt;The airline’s advice to check its website for up-to-the-minute information “was a something of a joke,” says &lt;a shape="rect" href="http://www.itabhi.com/bio.htm"&gt;Robert Charette&lt;/a&gt;, a self-described “risk ecologist” who is an internationally acknowledged authority and pioneer in risk management, information systems and technology, and systems engineering. Charette adds that, “As Minnesota Public Radio said this morning, the information on the website was incorrect because the computer system was down. I don’t know how long it will take—now that the computer system is back online—for all the worldwide flight data to be brought up to date. I definitely wouldn’t want to be booked on a Delta flight today or tomorrow.”&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Who, us? Hacked?&lt;/strong&gt;
&lt;/p&gt;
&lt;p&gt;Delta’s spokespeople came right out of the gate (pun intended) trying to get ahead of any concern that the computer outage resulted from the work of hackers. Their reasoning must have been: The damage to the carrier’s reputation—and its bottom line—that would occur if travelers suspected that there are security concerns would be far larger than the fallout from stranding hundreds of thousands of passengers because of what it considers an uncontrollable event.&lt;/p&gt;
&lt;p&gt;But the airline still doesn’t get a pass. Today’s incident is but the latest in a string of “isolated incidents” that together reveal a pattern that is cause for concern. Charette puts today’s outage in context:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are have been several reservation system outages that have hit worldwide airline ops with distressing regularity over the past few years. Southwest Airlines had one just a few weeks ago. (It had another big one June of 2013 and another in October 2015.) What you’ll see in reviewing them is recurring problems with infrastructure (i.e., power, networks, routers, servers, etc.) that seem to keep surprising the airlines. In every case I can recall, there were backup systems in place, but they failed—another recurring theme. The Southwest CEO claimed that the last outage—caused by a router—was equivalent to a 1000-year flood. Not only was that a comical overstatement, but it also shows the thinking that is probably [leading to the airlines] skimping on contingency management preparations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Charette knows of what he speaks. For years, he was the lead contributor to &lt;em&gt;IEEE Spectrum&lt;/em&gt;’s &lt;a shape="rect" href="http://spectrum.ieee.org/blog/riskfactor"&gt;Risk Factor blog&lt;/a&gt;, where he catalogued IT failures both big, small, foreseeable, and inexplicable. Last October, he authored the special report, “&lt;a shape="rect" href="http://spectrum.ieee.org/static/lessons-from-a-decade-of-it-failures"&gt;Lessons From a Decade of IT Failures&lt;/a&gt;,” which examined the takeaways from the blog’s tracking of the big IT debacles of the previous decade.&lt;/p&gt;
&lt;p&gt;Based on recent events, it’s clear that 1000-year floods occur far more frequently than they used to.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 8 Aug 2016 20:00:00 GMT</pubDate>
<dc:creator>Willie D. Jones</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/it/delta-airlines-downed-by-it-failure</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NzQyNw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NzQyNQ.jpg" height="225" width="300"/>
</item>
<item>
<title>$100,000 Prize If You Can Find This Secret Command in DOS</title>
<link>http://spectrum.ieee.org/view-from-the-valley/computing/software/100000-prize-if-you-can-find-the-secret-command-in-dos</link>
<description>Expert says system calls, but not source code, were copied from CP/M to MS-DOS. But there's $100,000 if you can prove he's wrong</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Expert says system calls, but not source code, were copied from CP/M to MS-DOS. But there's $100,000 if you can prove he's wrong&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NjA5NQ.jpeg"/&gt;
&lt;figcaption&gt;Photo-Illustration: Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;There have long been rumors that Microsoft copied CP/M to create MS-DOS for the IBM PC. Consultant Bob Zeidman in 2012 used forensic software tools to analyze the code for &lt;em&gt;IEEE Spectrum&lt;/em&gt; and found no evidence of copying, as he reported in “&lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/did-bill-gates-steal-the-heart-of-dos"&gt;Did Bill Gates Steal the Heart of DOS?&lt;/a&gt;”  Since he did that analysis, Microsoft donated previously unavailable source code for MS-DOS to the Computer History Museum. (Zeidman did his original analysis using QDOS.). And the museum also located and released a more complete version of the CP/M source code. Zeidman reran his analysis and presented the results 6 August at the &lt;a shape="rect" href="http://vcfed.org/wp/festivals/vintage-computer-festival-west-xi/"&gt;Vintage Computer Festival West.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The conclusion? Still no sign of copying of source code. And no evidence to support a long-running rumor that there is a secret command in MS-DOS that can be called to print out a copyright notice in &lt;a shape="rect" href="http://www.computerhistory.org/atchm/gary-kildall-40th-anniversary-of-the-birth-of-the-pc-operating-system/"&gt;Gary Kildall&lt;/a&gt;’s name.&lt;/p&gt;
&lt;p&gt;However, Zeidman did find that at least 22 system calls, the commands used to request an action, like sending text to a printer or reading from a hard disk, had the same function number and function. That, he says, might have meant that Kildall “might have had a copyright claim for the system calls that it could have litigated against Microsoft. On the other hand, there is a good chance Microsoft could have beaten such litigation by claiming it was a ‘fair use.’ ”&lt;/p&gt;
&lt;p&gt;Meanwhile, Zeidman has had a lot of pushback since his original article was published, in part because it turned out he had an open consulting contract with Microsoft at the time. (He reports that it was an old and inactive contract and he has since severed all ties.) So he’s putting up $200,000 in prize money, $100,000 for anyone who can use “accepted forensic techniques” to prove the copying, and another $100,000 for anyone who can find that secret Kildall copyright function.&lt;/p&gt;
&lt;p&gt;His &lt;a shape="rect" href="http://zeidmanconsulting.com/DOS_comparisons/"&gt;full analysis of the code&lt;/a&gt; and supporting documents are &lt;a shape="rect" href="http://zeidmanconsulting.com/DOS_comparisons/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Sat, 6 Aug 2016 19:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/computing/software/100000-prize-if-you-can-find-the-secret-command-in-dos</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NjEwNQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NjEwMw.jpg" height="225" width="300"/>
</item>
<item>
<title>Autonomous Security Bots Seek and Destroy Software Bugs in DARPA Cyber Grand Challenge</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/security/autonomous-supercomputers-seek-and-destroy-software-bugs-in-darpa-cyber-grand-challenge</link>
<description>The team behind the victorious Cyber Reasoning System will receive a US $2 million prize</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The team behind the victorious Cyber Reasoning System will receive a US $2 million prize&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NjkwMg.jpeg"/&gt;
&lt;figcaption&gt;Photo: Steven Puetzer/Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;The mission: to detect and patch as many software flaws as possible. The competitors: seven dueling supercomputers about the size of large vending machines, each emblazoned with a name like Jima or Crspy, and programmed by expert hacker teams to autonomously find and fix malicious bugs.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;These seven “Cyber Reasoning Systems” took the stage on Thursday for &lt;a shape="rect" href="https://www.cybergrandchallenge.com/"&gt;DARPA’s Cyber Grand Challenge&lt;/a&gt; at the Paris Hotel and Conference Center in Las Vegas, Nev. They were competing for a $2 million grand prize in the world’s first fully autonomous “Capture the Flag” tournament. After eight hours of grueling bot-on-bot competition, DARPA declared &lt;a shape="rect" href="http://www.darpa.mil/news-events/2016-08-04"&gt;a system named Mayhem&lt;/a&gt;, built by Pittsburgh, Pa.-based &lt;a shape="rect" href="https://www.cybergrandchallenge.com/finalists"&gt;ForAllSecure&lt;/a&gt; as the unofficial winner. The Mayhem team was led by &lt;a shape="rect" href="https://users.ece.cmu.edu/~dbrumley/"&gt;David Brumley&lt;/a&gt;. &lt;a shape="rect" href="https://www.cybergrandchallenge.com/finalists"&gt;Xandra&lt;/a&gt;, produced by TECHX from GammaTech and the University of Virginia, placed second to earn a $1 million prize; and &lt;a shape="rect" href="https://www.cybergrandchallenge.com/finalists"&gt;Mechanical Phish&lt;/a&gt; by Shellphish, a student-led team from Santa Barbara, Calif., took third place, worth $750,000.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;DARPA is verifying the results and will announce the official positions on Friday. The triumphant bot will then compete against human hackers in a “Capture the Flag” tournament at the annual &lt;a shape="rect" href="https://www.defcon.org/"&gt;DEF CON&lt;/a&gt; security conference. Though no one expects one of these reasoning systems to win that challenge, it could solve some types of bugs more quickly than human teams.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Darpa hopes the competition will pay off by bringing researchers closer to developing software repair bots that could constantly scan systems for flaws or bugs and patch them much faster and more effectively than human teams can. DARPA says quickly fixing such flaws across &lt;a shape="rect" href="http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/"&gt;billions of lines of code&lt;/a&gt; is critically important. It could help to harden infrastructure such as power lines and water treatment plants against cyberattacks, and to protect privacy as more personal devices come online.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;But no such system has even been available on the market. Instead, teams of security specialists constantly scan code for potential problems. On average, it takes specialists 312 days to discover a software vulnerability and often months or years to actually fix it, according to DARPA CGC host &lt;a shape="rect" href="http://www.fit.edu/faculty/profiles/profile.php?value=433"&gt;Hakeem Oluseyi&lt;/a&gt;.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;“A final goal of all this is scalability,” says Michael Stevenson, Mission Manager for the Deep Red team from Raytheon. “If [the bots] discover something in one part of the network, these are the technologies that can quickly reach out and patch that vulnerability throughout that network.” The original&lt;a shape="rect" href="http://www.pbs.org/wgbh/nova/darpa/"&gt; 2005 DARPA Grand Challenge&lt;/a&gt; jumpstarted corporate and academic interest in autonomous cars.&lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5NjkyMw.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Image: DARPA                                                             &lt;/figcaption&gt;
&lt;figcaption&gt;This visualization shows network traffic flows for the bot Rubeus as it receives verification of software bugs from competitors.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The teams were not told what types of defects their systems would encounter in the finale, so their bots had to reverse engineer DARPA’s challenge software, identify potential bugs, run tests to verify those bugs, and then apply patches that wouldn’t cause the software to run slowly or shut down altogether.&lt;/p&gt;
&lt;p&gt;To test the limits of these Cyber Reasoning Systems, DARPA planted software bugs that were simplified versions of famous malware such as &lt;a shape="rect" href="http://groups.csail.mit.edu/mac/classes/6.805/articles/morris-worm.html"&gt;the Morris worm&lt;/a&gt; and the &lt;a shape="rect" href="http://heartbleed.com/"&gt;Heartbleed bug&lt;/a&gt;. Scores were based on how quickly and effectively the bots deployed patches and verified competitors’ patches, and bots lost points if their patches slowed down the software. “If you fix the bug but it takes 10 hours to run something that should have taken 5 minutes, that's not really useful,” explains Corbin Souffrant, a Raytheon cyber engineer.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Members of the &lt;a shape="rect" href="https://www.cybergrandchallenge.com/finalists"&gt;Deep Red team&lt;/a&gt; described how their system accomplished this in five basic steps: First, their machine (named Rubeus) used a technique called fuzzing to overload the program with data and cause it to crash. Then, it scanned the crash results to identify potential flaws in the program’s code. Next, it verified these flaws and looked for potential patches in a database of known bugs and appropriate fixes. It chose a patch from this repository and applied it, and then analyzed the results to see if it helped. For each patch, the system used artificial intelligence to compare its solution with the results and determine how it should fix similar patches in the future.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;During the live competition, some bugs proved more difficult for the machines to handle than others. Several machines found and patched an SQL Slammer-like vulnerability within 5 minutes, garnering applause. But only two teams managed to repair an imitation crackaddr bug in SendMail. And one bot, &lt;a shape="rect" href="https://www.cybergrandchallenge.com/finalists"&gt;Xandra&lt;/a&gt; by the TECHx team, found a bug that the organizers hadn’t even intended to create.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Whether humans or machines, it’s always nice to see vanquished competitors exhibit good sportsmanship in the face of a loss. As the night wound down, &lt;a shape="rect" href="https://twitter.com/mechanicalphish"&gt;Mechanical Phish&lt;/a&gt; politely &lt;a shape="rect" href="https://twitter.com/mechanicalphish/status/761391505114140672"&gt;congratulated&lt;/a&gt;
&lt;a shape="rect" href="https://twitter.com/MayhemCRS"&gt;Mayhem&lt;/a&gt; on its first place finish over the bots’ Twitter accounts.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 5 Aug 2016 17:00:00 GMT</pubDate>
<dc:creator>Amy Nordrum</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/security/autonomous-supercomputers-seek-and-destroy-software-bugs-in-darpa-cyber-grand-challenge</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NjkxNQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NjkxMw.jpg" height="225" width="300"/>
</item>
<item>
<title>A Peek Inside Andy Rubin’s Playground</title>
<link>http://spectrum.ieee.org/view-from-the-valley/at-work/start-ups/a-peek-inside-andy-rubins-playground</link>
<description>Every tool a hardware engineer could want—plus swings and a slide</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Every tool a hardware engineer could want—plus swings and a slide&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NjUyMQ.jpeg"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div id="836670862" class="carousel slide"&gt;
&lt;div class="carousel-inner"&gt;
&lt;div class="item active"&gt;
&lt;img id="836670862_0" alt="Playground Global’s huge workspace fills the inside of an old cannery in Palo Alto almost hidden behind Frye’s Electronics. About a dozen startup companies are currently in residence." src="http://spectrum.ieee.org/image/Mjc5NjA1MQ.jpeg" data-original="/image/Mjc5NjA1MQ.jpeg"/&gt;
&lt;span class="item-num"&gt;1/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Playground Global’s huge workspace fills the inside of an old cannery in Palo Alto almost hidden behind Frye’s Electronics. About a dozen startup companies are currently in residence. &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_1" alt="Inside Playground is an actual playground" src="http://spectrum.ieee.org/image/Mjc5NjA1NA.jpeg" data-original="/image/Mjc5NjA1NA.jpeg"/&gt;
&lt;span class="item-num"&gt;2/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Inside Playground is an actual playground &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_2" alt="A slide connects a lofted lounge to the main floor" src="http://spectrum.ieee.org/image/Mjc5NjA1Nw.jpeg" data-original="/image/Mjc5NjA1Nw.jpeg"/&gt;
&lt;span class="item-num"&gt;3/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;A slide connects a lofted lounge to the main floor &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_3" alt="Two labs are available for hardware engineering." src="http://spectrum.ieee.org/image/Mjc5NjA2MA.jpeg" data-original="/image/Mjc5NjA2MA.jpeg"/&gt;
&lt;span class="item-num"&gt;4/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Two labs are available for hardware engineering. &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_4" alt="A testing lab includes equipment for drop testing and environmental testing." src="http://spectrum.ieee.org/image/Mjc5NjA2Mw.jpeg" data-original="/image/Mjc5NjA2Mw.jpeg"/&gt;
&lt;span class="item-num"&gt;5/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;A testing lab includes equipment for drop testing and environmental testing. &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_5" alt="Classic video games offer a different kind of play." src="http://spectrum.ieee.org/image/Mjc5NjA2Ng.jpeg" data-original="/image/Mjc5NjA2Ng.jpeg"/&gt;
&lt;span class="item-num"&gt;6/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Classic video games offer a different kind of play. &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_6" alt="Rows of rows of workstations are interspersed with booths for quiet conversation or reflection." src="http://spectrum.ieee.org/image/Mjc5NjA2OQ.jpeg" data-original="/image/Mjc5NjA2OQ.jpeg"/&gt;
&lt;span class="item-num"&gt;7/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Rows of rows of workstations are interspersed with booths for quiet conversation or reflection. &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="836670862_7" alt="Lego art decorates a lounge area." src="http://spectrum.ieee.org/image/Mjc5NjA3Mg.jpeg" data-original="/image/Mjc5NjA3Mg.jpeg"/&gt;
&lt;span class="item-num"&gt;8/8&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Lego art decorates a lounge area. &lt;em&gt;Photo: Tekla Perry&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;a class="left carousel-control" shape="rect" href="#836670862" data-slide="prev"&gt;
&lt;span class="glyphicon glyphicon-chevron-left"/&gt;
&lt;/a&gt;
&lt;a class="right carousel-control" shape="rect" href="#836670862" data-slide="next"&gt;
&lt;span class="glyphicon glyphicon-chevron-right"/&gt;
&lt;/a&gt;
&lt;ol class="carousel-indicators"&gt;
&lt;li data-target="#836670862" data-slide-to="0" class="active"/&gt;
&lt;li data-target="#836670862" data-slide-to="1" class=""/&gt;
&lt;li data-target="#836670862" data-slide-to="2" class=""/&gt;
&lt;li data-target="#836670862" data-slide-to="3" class=""/&gt;
&lt;li data-target="#836670862" data-slide-to="4" class=""/&gt;
&lt;li data-target="#836670862" data-slide-to="5" class=""/&gt;
&lt;li data-target="#836670862" data-slide-to="6" class=""/&gt;
&lt;li data-target="#836670862" data-slide-to="7" class=""/&gt;
&lt;/ol&gt;
&lt;script&gt;
                $(document).ready(function(){
                    $('#836670862').carousel({
                        pause: true,
                        interval: false
                    });
                });
&lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Playground Global, an effort to make it easier for Silicon Valley hardware startups to make their ideas real, &lt;a shape="rect" href="http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gadgets/a-playground-for-hardware-startups-from-android-creator-andy-rubin"&gt;came on the scene in 2015&lt;/a&gt;. A group of investors including Hewlett-Packard, Google, Hon Hai Precision Industry Co, Seagate, and others—and led by Andy Rubin of Danger and Android fame—&lt;a shape="rect" href="http://blog.playground.global/open-software-makes-hardware/"&gt;backed the effort with at least US $48 million&lt;/a&gt;. The mission: create a hardware “studio.” It’s something more than an incubator or an accelerator, in that it allows inventors to focus on their gadgets and takes away as many of the barriers to doing that as possible .&lt;/p&gt;
&lt;p&gt;I got my first peek inside Playground’s spacious headquarters last week. And it sure seems to have &lt;a shape="rect" href="http://playground.global/studio/shop-and-labs"&gt;everything a Silicon Valley engineer could want, and then some&lt;/a&gt;. The setup includes: two tricked out engineering labs, a prototype-building area with multiple 3-D printers, various cutters, a laser-sintering machine, a testing lab, an optical lab, lots and lots of bench space, and 50 engineers available to help the startups. (Though it has room for 30 startups, only about a dozen seemed to be in residence during my visit.)&lt;/p&gt;
&lt;p&gt;Playground has the ambience of a Google or Facebook, not a shoestring startup. That means free gourmet meals, a well-stocked snack bar, and an espresso machine that requires a training session to operate, along with the most stylish “quiet room” space I’ve seen (think giant modernized London phone booths). And, of course, Playground has a playground, in addition to a slide—which is a pretty common way to get from the second floor to the first at tech companies these days—there’s also a swingset.&lt;/p&gt;
&lt;p&gt;All this is spread out inside a former apricot cannery, a Palo Alto building that I must have passed thousands of times but never noticed. It’s tucked behind Fry’s Electronics, long a mecca for hardware engineers needing a random component immediately, if not sooner.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 5 Aug 2016 14:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/at-work/start-ups/a-peek-inside-andy-rubins-playground</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NjUzMw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NjUzMQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Twisted Light Could Dramatically Boost Data Rates</title>
<link>http://spectrum.ieee.org/telecom/wireless/twisted-light-could-dramatically-boost-data-rates</link>
<description>Orbital angular momentum could take optical and radio communication to new heights</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Orbital angular momentum could take optical and radio communication to new heights&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MTE5NA.jpeg"/&gt;
&lt;figcaption&gt;Illustration: Anatomy Blue&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="/image/Mjc5MTE5Mw.jpeg" src="http://spectrum.ieee.org/image/Mjc5NDI0MQ.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: Anatomy Blue&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;
&lt;strong&gt;It was just 24 years ago that researchers reported&lt;/strong&gt; the discovery of something remarkable about light: Certain beams, traveling through space with a spiraling pattern suggestive of a corkscrew, carry a form of momentum called orbital angular momentum.&lt;/p&gt;
&lt;p&gt;Big deal, you say? It most certainly was. The concept of orbital angular momentum (OAM) has done nothing less than inspire a reimagining of what we’re capable of doing with electromagnetic radiation. Beams that carry OAM can be used to move tiny objects, and they have been used to enhance the resolving power of microscopes.&lt;/p&gt;
&lt;p&gt;But the exploitation of OAM may have the biggest impact in my own field of communications. OAM waves with different “twists” don’t interfere with one another. That means they can be overlaid one on top of another to carry a theoretically unlimited number of different data streams at the same time. Hardware that can transmit and receive even a few such OAM beams could dramatically boost the capacity of optical and radio transmissions without placing any more demands on the crowded electromagnetic spectrum than we do today. Indeed, my team at the University of Southern California, in Los Angeles, and others have performed experiments to test this idea, and they worked just as the theory predicted.&lt;/p&gt;
&lt;p&gt;If OAM communications sounds familiar, you may have noticed the news a few years ago when one of the first OAM radio demonstrations was performed. At the time, some engineers argued that the approach wasn’t new and was instead just a version of another strategy for sending multiple waves at the same time. But since then, it’s become clear that OAM transmission really is a novel and powerful technology, one that could allow us to transmit much more information along wireless connections and dramatically speed up parts of the networks that underpin the Internet. The technological challenge is finding good ways to harness OAM. We are finally starting to do just that.&lt;/p&gt;
&lt;aside class="inlay xlrg"&gt;
&lt;h3 class="sb-hed"&gt;A Tale of Three Beams&lt;/h3&gt;
&lt;div id="439003131" class="carousel slide"&gt;
&lt;div class="carousel-inner"&gt;
&lt;div class="item active"&gt;
&lt;img id="439003131_0" alt="The oscillation of electric fields can create linearly [left] or circularly [right] polarized light. In both cases, if the beam is a plane wave, the phase—a measure of where the electric field is in its oscillation between peak and trough—will be uniform at every point in the beam’s cross section (a phase of 0º is shown here)." src="http://spectrum.ieee.org/image/Mjc5NTEyNQ.png" data-original="/image/Mjc5NTEyNQ.png"/&gt;
&lt;span class="item-num"&gt;1/2&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;The oscillation of electric fields can create linearly [left] or circularly [right] polarized light. In both cases, if the beam is a plane wave, the phase—a measure of where the electric field is in its oscillation between peak and trough—will be uniform at every point in the beam’s cross section (a phase of 0º is shown here). &lt;em&gt;Illustrations: James Provost&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="item"&gt;
&lt;img id="439003131_1" alt="Although the sketch of a basic OAM beam resembles that of a circularly polarized wave, it is the phase of the beam and not the electric field at each point that does the twisting. In cross section, the beam contains “miniwaves” with a spectrum of phase values. The most basic beam [above] has a phase profile that runs from 0º to 360º. The location of miniwaves with a particular phase (here, 0º) rotates as the beam moves, creating a helix pattern." src="http://spectrum.ieee.org/image/Mjc5MTIxMA.png" data-original="/image/Mjc5MTIxMA.png"/&gt;
&lt;span class="item-num"&gt;2/2&lt;/span&gt;
&lt;div class="carousel-caption"&gt;
&lt;p&gt;Although the sketch of a basic OAM beam resembles that of a circularly polarized wave, it is the phase of the beam and not the electric field at each point that does the twisting. In cross section, the beam contains “miniwaves” with a spectrum of phase values. The most basic beam [above] has a phase profile that runs from 0º to 360º. The location of miniwaves with a particular phase (here, 0º) rotates as the beam moves, creating a helix pattern. &lt;em&gt;Illustration: James Provost&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;a class="left carousel-control" shape="rect" href="#439003131" data-slide="prev"&gt;
&lt;span class="glyphicon glyphicon-chevron-left"/&gt;
&lt;/a&gt;
&lt;a class="right carousel-control" shape="rect" href="#439003131" data-slide="next"&gt;
&lt;span class="glyphicon glyphicon-chevron-right"/&gt;
&lt;/a&gt;
&lt;ol class="carousel-indicators"&gt;
&lt;li data-target="#439003131" data-slide-to="0" class="active"/&gt;
&lt;li data-target="#439003131" data-slide-to="1" class=""/&gt;
&lt;/ol&gt;
&lt;script&gt;
                $(document).ready(function(){
                    $('#439003131').carousel({
                        pause: true,
                        interval: false
                    });
                });
&lt;/script&gt;
&lt;/div&gt;
&lt;/aside&gt;
&lt;p&gt;
&lt;strong&gt;Any form of&lt;/strong&gt; electromagnetic radiation, whether in the radio, visible, or another part of the spectrum, can have OAM. But for the purpose of explaining what OAM looks like (and to avoid writing out “electromagnetic radiation” every time we do so) we’ll simply use the term “light.”&lt;/p&gt;
&lt;p&gt;One of the most curious aspects of light is that it has energy and momentum, just like an ordinary physical object moving through space, even though it doesn’t have mass. And just like an ordinary physical object, when light strikes something, it exerts a force. A solar sail, for example, takes advantage of this property as it accelerates through space, pushed by sunlight alone. Light carries that “pushing” capacity—its linear momentum—along the direction it’s moving. But light can also have angular momentum.&lt;/p&gt;
&lt;p&gt;For a long time, the only commonly discussed form of such momentum was spin angular momentum. To understand how it works requires a bit of background on polarization. A ray of light has an electric field associated with it that oscillates perpendicularly to the direction of the ray. In light that is linearly polarized, the electric field oscillates along a fixed line. Light that is &lt;a shape="rect" href="https://www.khanacademy.org/science/physics/light-waves/introduction-to-light-waves/v/polarization-of-light-linear-and-circular"&gt;
&lt;em&gt;circularly&lt;/em&gt;
&lt;em/&gt;polarized&lt;/a&gt; has an oscillating electric field that rotates around the direction the light moves in. Such circularly polarized waves have spin angular momentum, the electromagnetic equivalent of a spinning top or a planet rotating on its axis. Remarkably, this form of momentum can also impart a torque: Shine light with spin angular momentum on a microscopic object and you can make it rotate.&lt;/p&gt;
&lt;p&gt;In 1992, physicist Les Allen, working with &lt;a shape="rect" href="http://www.physics.leidenuniv.nl/component/content/article?id=240&amp;amp;PID=220"&gt;Han Woerdman&lt;/a&gt; and colleagues at Leiden University, in the Netherlands, pointed out that a certain spiraling beam carries another form of angular momentum—orbital angular momentum. If light with spin angular momentum is like a spinning planet, the physical analogue of OAM light could be a planet orbiting the sun. OAM light can &lt;a shape="rect" href="http://www.wiley-vch.de/books/sample/3527409076_c01.pdf"&gt;also impart a torque,&lt;/a&gt; a “twist” that, depending on where the beam hits, can cause a small object to rotate or move in an orbit around the center of the beam.&lt;/p&gt;
&lt;p&gt;Not all light has OAM. To have it, a beam must have a particular kind of phase front. Phase is the component of an electromagnetic wave that governs the arrival times of its peaks and troughs. To picture what a phase front looks like, consider a beam of light. If you look at its cross section, it’s easy to view the beam as a collection of a very large number of “miniwaves.” If all these miniwaves oscillate in unison as they propagate—as they do in a common laser beam—the beam is a plane wave, and it has a flat phase front. At any given point along the beam’s propagating direction, the entire cross section has one phase value—that is, all the minibeams are at the peak of a crest, the bottom of a trough, or more likely, somewhere in between.&lt;/p&gt;
&lt;p&gt;But there is no rule demanding that different parts of a light beam all have the same phase. In the case of a helical wave, the sort that carries OAM, the miniwaves in the cross section of the beam aren’t uniform. Instead, the phase of each miniwave depends on its angular location around the center of the beam. If you were to trace a circle around the center, the phase would either steadily increase or decrease as you go around.&lt;/p&gt;
&lt;aside class="inlay rt med"&gt;
&lt;h3 class="sb-hed"&gt;Phase in Motion&lt;/h3&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="/image/Mjc5MTIzMA.png" src="http://spectrum.ieee.org/image/Mjc5MTIzMA.png"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: James Provost&lt;/figcaption&gt;
&lt;figcaption&gt;A plane wave, such as an ordinary laser beam, has the same phase at each point in cross section; the value, which corresponds to where each miniwave in the beam is in its oscillation, changes as the beam propagates. In an OAM beam, the phase can take on a variety of values, and the resulting phase profile rotates around the center of the beam as it moves.       &lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;/aside&gt;
&lt;p&gt;To give a more intuitive feel for how this translates into a “twisting” beam, think of those miniwaves as they move through space. At first, all the miniwaves that are at their peak will lie along some angle, like the hand on a clock. A short time later, those miniwaves will no longer be at their peak; instead, the peak will have advanced, like the clock hand, to another set of miniwaves at another angle around the circle. This process continues, so that if you track the angular location of the miniwave peaks, the wave appears to twist as it moves. To get a better sense of how this phase behavior translates into what looks like movement, consider a neon sign with individual bulbs timed to turn on and off in sequence. With the right program, the neon light can look as if it’s moving in one direction, even though none of the bulbs move. The same is true for OAM. Each miniwave in a beam oscillates steadily, but the time sequence at which each miniwave peaks makes the phase front twist, describing a helix as the beam moves.&lt;/p&gt;
&lt;p&gt;Crucially, the more dramatic the phase shift as you move in a circle around the cross section of the beam, the bigger the twist and the higher the amount of OAM. Phase changes around the entire circle can come in integer multiples of 360 degrees.&lt;/p&gt;
&lt;p&gt;Such a twisting, helical wave is hard to visualize, but it does produce a clear visible effect. Unlike a conventional beam, which is brightest at the center, the cross section of a helical beam has a ringlike shape, with a dark center. This happens because the center of the beam is full of miniwaves with every possible phase, and a miniwave at its peak is very likely to overlap at least in part with a miniwave at its trough. The opposing pairs cancel one another out through destructive interference.&lt;/p&gt;
&lt;p&gt;Helical waves—also known as vortex beams—were known to exist before 1992. But the realization that they carry OAM, as reported in Allen’s paper, was a profound discovery about light, something physicists thought they knew pretty well. For about a decade after the Allen paper was published, a small group of people, mostly physicists, studied OAM and tried to understand its fundamental nature. But since then, the number of researchers involved and technical fields affected have grown dramatically; according to Google Scholar, more than 5,000 scientific papers that discuss OAM were published in 2015.&lt;/p&gt;
&lt;aside class="inlay lt med"&gt;
&lt;h3 class="sb-hed"&gt;Twists, Ascending&lt;/h3&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MTI0MA.png"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: James Provost&lt;/figcaption&gt;
&lt;figcaption&gt;An OAM beam with the lowest level of twist has a phase profile that spans only from 0º to 360º as you trace a circle around the center of the beam [1, far right column]. Beams with higher values of OAM have multiple cycles of phase change—that is, they go from 0º to 360º multiple times as you go around the center of the beam. OAM produces a clear visible effect in cross section [middle column]: a doughnut-shaped beam with a dark center. The higher the value of OAM, the larger the beam.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;/aside&gt;
&lt;p&gt;
&lt;strong&gt;One of those fields&lt;/strong&gt; is communications, and that’s because of a powerful property of OAM: Overlapping beams with different values of OAM essentially behave as if they can’t “see” each other. To use the parlance of communications engineers and physicists, those OAM beams are orthogonal to one another. This means that each beam is distinct. It can’t be constructed using beams with other values of OAM, and it isn’t intrinsically capable of interacting with those other beams.&lt;/p&gt;
&lt;p&gt;This orthogonality means that OAM beams can occupy the same space without interfering with one another. In fact it’s theoretically possible to use an infinite number of beams, each with a different value of OAM, to carry signals. In practice, there are limitations, as there always are. We’ll get to some of those shortly. But the upshot is that we should be able to overlay many beams of the same frequency on top of one another (multiple polarizations can be used as well). This is good news for communications in general, and especially good news for radio communications, where spectrum is a particularly precious commodity.&lt;/p&gt;
&lt;p&gt;A major step forward for OAM communications occurred in 2004, when physicist Miles Padgett and his colleagues &lt;a shape="rect" href="https://www.osapublishing.org/oe/abstract.cfm?uri=oe-12-22-5448"&gt;showed that OAM waves could be used to encode data,&lt;/a&gt; by using different values of OAM to represent information. Later, it became clear that a beam with a single fixed OAM value could act as a data channel—that is, it could be modulated in a variety of conventional ways to carry information. The most straightforward of such modulation methods is on-off keying, which uses the presence and absence of a beam to represent “1” and “0” data bits.&lt;/p&gt;
&lt;p&gt;The 2004 work kicked off a number of investigations into using OAM for communications. My group started research on the subject in 2009, when we received funding from the U.S. Defense Advanced Research Projects Agency’s &lt;a shape="rect" href="http://www.darpa.mil/program/information-in-a-photon"&gt;Information in a Photon&lt;/a&gt; program, which set out to explore the limits of optical photons for imaging, sensing, and communications. Our first investigation involved building free-space optical links, the sort that can be used to transmit data directly from one computer to another, from one building to another, or possibly between a satellite and the ground.&lt;/p&gt;
&lt;p&gt;There are a variety of ways to create and transmit helical beams. We chose a conventional approach, built with as many off-the-shelf components as possible. The transmitter generates regular laser beams, which are then passed through a spatial light modulator, based on liquid crystal, in order to impart a twist to the beam. At the receiving end, each OAM beam was converted back into a regular plane wave by passing it through a spatial modulator with the inverse pattern. The data could then be recovered by a conventional optical receiver.&lt;/p&gt;
&lt;aside class="inlay rt med"&gt;
&lt;h3 class="sb-hed"&gt;Making the Twist&lt;/h3&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="/image/Mjc5MTI1MQ.png" src="http://spectrum.ieee.org/image/Mjc5MTI1MQ.png"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: James Provost&lt;/figcaption&gt;
&lt;figcaption&gt;There are multiple ways to make an OAM beam. One of the most straightforward is to use a spiral phase plate [gray], a filter that varies in physical thickness in a spiral fashion. The bigger the difference between the thinnest and the thickest portion of the plate, the greater the value of OAM.  &lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;/aside&gt;
&lt;p&gt;In 2012, we &lt;a shape="rect" href="http://www.nature.com/nphoton/journal/v6/n7/full/nphoton.2012.138.html"&gt;published our first journal article&lt;/a&gt; on this approach. Our experiment sent 32 different optical beams of the same frequency, each carrying 80 gigabits per second of data, over a modest distance—just 1 meter—in the laboratory. But the total transmission rate, some 2.5 terabits per second, was actually quite high for free-space communications. And it held out the promise for longer-distance transmissions and, because we used only one frequency, much higher data rates.&lt;/p&gt;
&lt;p&gt;Our paper on the experiment came out at a curious time. Excitement was building over a public demonstration the previous year by a group of researchers from Italy and Sweden. Following in the footsteps of radio pioneer Guglielmo Marconi, the group sent OAM radio waves some 450 meters from a lighthouse on the island of San Giorgio Maggiore to another building in Venice one evening in June 2011.&lt;/p&gt;
&lt;p&gt;But after the team &lt;a shape="rect" href="http://iopscience.iop.org/article/10.1088/1367-2630/14/3/033001/"&gt;shared its results&lt;/a&gt;, a kerfuffle erupted over the true nature of the technology. The main question was whether OAM transmission really was something new, or if it was just another form of the traditional multiple-input-multiple-output (MIMO) transmission found in wireless radio systems today.&lt;/p&gt;
&lt;p&gt;Quick background: MIMO uses multiple antennas, each transmitting an independent data stream. Multiple receiver antennas are used to pick up those signals. The transmitters are separated by some distance from one another, as are the receivers. But because electromagnetic beams spread out as they propagate, the different beams usually overlap by the time they reach their destination. This “crosstalk” degrades signals, but digital signal processing that incorporates knowledge of the relative positions of the antennas can counteract the effect.&lt;/p&gt;
&lt;p&gt;The disagreement over the Venice demonstration arose because MIMO and OAM both transmit multiple beams that overlap with one another in space, a form of transmission called spatial multiplexing. And at that level, there is no fundamental difference between the approaches. Practically, however, they rely on very distinct ways of minimizing the possibility that one signal could be mistaken for another. While MIMO uses a digital signal processor to undo the effect of such crosstalk, OAM multiplexing relies on the inherent orthogonality of the beams, which can emanate from a single location, to prevent crosstalk from ever taking place. The two technologies are not only different, they can actually coexist; a few years ago we used two different millimeter-wave antennas—multiple inputs—to &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&amp;amp;arnumber=7037403"&gt;transmit data on four OAM beams&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that the dust has settled on the debate over the novelty of OAM, communications engineers are starting to get more comfortable with the technology. Experiments continue to explore its potential. The research can be roughly divided into three fields: free-space optical links, traditional radio-frequency wireless transmissions, and fiber-optic communications.&lt;/p&gt;
&lt;p&gt;The first two, free-space optical and radio, are the farthest along. In the optical category, in 2014, Anton Zeilinger of the University of Vienna and colleagues reported that they had used OAM light to &lt;a shape="rect" href="http://iopscience.iop.org/article/10.1088/1367-2630/16/11/113028"&gt;send data&lt;/a&gt;, including a grayscale image of Mozart, between two sites in Vienna separated by 3 kilometers.&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="/image/Mjc5MTI1Mg.jpeg" src="http://spectrum.ieee.org/image/Mjc5MTI1Mg.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photos: Jian Wang and Alan E. Willner/Nature Photonics (4)&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;Spiral Power:&lt;/strong&gt; These experimental interferograms, which mix OAM beams and ordinary light, show OAM values of 4, 8, –8, and 16 [left to right].  The sign of the OAM value indicates the direction of the beam’s rotation.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Although the data rate was extremely slow—just four pixels per second—the demonstration was important because it helped establish that free-space optical OAM communication works despite random variations in air pressure and temperature. Such atmospheric turbulence can alter the phase of the miniwaves in the optical beam by varying amounts, so that a beam transmitted with one OAM value may appear to have different values of OAM at the receiver.&lt;/p&gt;
&lt;p&gt;Digital signal processing and adaptive optics—which can use a separate beam to measure the atmospheric effect so it can be corrected—can help with this problem. And a recent experiment by the Vienna group has demonstrated OAM transmission across an even longer distance. But it’s unclear yet whether OAM will find its first practical application in optical or radio communications. On one hand, since light in and around the visible range oscillates at much higher frequencies than radio waves do, optical OAM beams can have much higher data rates. However, the utility of optical links is limited by the fact that visible and infrared wavelengths are more sensitive to atmospheric conditions.&lt;/p&gt;
&lt;p&gt;That said, there are challenges to radio-frequency communication as well. Because of the fundamental rules of wave propagation, those longer-wavelength beams spread out more than optical beams do. This behavior reduces the amount of power a receiver might get, especially for beams that traverse long distances. Longer wavelengths also mean that both the transmitting and receiving antennas need to be fairly large in order to create the radio beams and pick them up efficiently, an added challenge for creating compact—and possibly even hand-held—systems.&lt;/p&gt;
&lt;p&gt;Importantly, for both optical and RF, a key challenge is recovering the signal. If only a small fraction of the beam—say, a 30-degree wedge—is captured, it is difficult to definitively say how much twist the OAM beam has. The center of the beam has the full range of phase change in a small space, but as you may recall, that part of the beam has very little signal. Adding to the difficulty is that while the number of OAM beams is theoretically infinite, higher-order OAM beams—those with more twist—spread out faster than lower-order ones. Careful system design will be needed for practical deployment, and some type of beam steering—which allows a receiver to detect when it isn’t centered on the beam and correct the positioning—will likely be required.&lt;/p&gt;
&lt;p&gt;Fiber communications presents its own challenges. When OAM signals are transmitted through glass, temperature changes and bends in the fiber can easily alter the beam’s phase profile so that some of its power shows up as a wave with a different value of OAM. One possible way of mitigating this issue is with a “vortex fiber,” &lt;a shape="rect" href="http://science.sciencemag.org/content/340/6140/1545.abstract"&gt;developed several years ago&lt;/a&gt; by a team based at Boston University. Instead of using a uniform glass core to guide waves, as normal fiber does, this special fiber contains a waveguide that looks like a ring in cross section. It turns out that the size of an OAM beam depends on the amount of twist it has, so beams with different values of OAM will have different levels of overlap with the waveguide. The level of overlap alters the speed at which an OAM beam can travel down the fiber, which helps reduce interaction between beams with different values of OAM.&lt;/p&gt;
&lt;p&gt;Early results are encouraging. But don’t expect OAM-optimized fibers to be used in transoceanic cables anytime soon; if they’re adopted in fiber systems, they’ll likely emerge first in short links inside high-performance computing and data centers.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;strong&gt;Whether OAM links&lt;/strong&gt; catch on or not will depend quite a bit on the hardware that’s used to exploit them. Today’s demonstrations tend to use bulky and expensive elements that are not necessarily optimized for OAM operation. But any practical deployment of an OAM-based system will require new, specially designed, cost-effective, and compact components. Several research groups are working on the elements, such as transmitters and receivers, needed to make this happen. A team at Fujitsu, for example, has reported an array of flat “patch” antennas that can generate and pick up OAM radio waves and which could fit inside a portable system. Others are working on integrated &lt;a shape="rect" href="http://science.sciencemag.org/content/338/6105/363"&gt;OAM&lt;/a&gt;
&lt;a shape="rect" href="http://advances.sciencemag.org/content/1/9/e1500396"&gt;generators&lt;/a&gt; and &lt;a shape="rect" href="http://science.sciencemag.org/content/334/6054/333"&gt;antenna systems&lt;/a&gt; for optical waves.&lt;/p&gt;
&lt;p&gt;At the same time, OAM technology is advancing on other fronts. One example is in the field of &lt;a shape="rect" href="http://www.anes.ucla.edu/sted/principle.html"&gt;fluorescence microscopy&lt;/a&gt;. Here, an ordinary light beam can be used to excite fluorescent molecules, while a ring-shaped OAM beam can be used to de-excite those molecules, except for those that fall in the dim center of the beam. This technique, pioneered by Nobel laureate Stefan Hell, allows the microscope to resolve features in a smaller region than is otherwise possible.&lt;/p&gt;
&lt;p&gt;Quantum communication, which can be used to send information far more securely than traditional systems do, can also take advantage of OAM. Today quantum bits, or qubits, can be made of photons that are constructed from a superposition of two possible polarization states—vertical and horizontal. But OAM isn’t just a property of electromagnetic waves; it’s also a quantum property of single photons. A single photon can have many possible values of OAM, which can be used to increase the capacity of a quantum link. Robert Boyd of the University of Rochester, in New York, has demonstrated quantum communication systems that can carry more than one bit per photon by using OAM.&lt;/p&gt;
&lt;p&gt;For a field that’s not even 25 years old, OAM research has come a long way. But it’s only just getting started. Ten years from now, we’ll have a much clearer picture of OAM’s potential and a new list of possible applications. And if things go as well as we hope, you’ll be reading about those applications over a connection that has a twist somewhere along the line.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;em&gt;The author would like to dedicate this article to Les Allen, who passed away earlier this year. He would also like to express his heartfelt appreciation to his outstanding students and colleagues for their keen wisdom and selfless help.&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the August 2016 print issue as “Communication With a Twist.” &lt;/em&gt;
&lt;/p&gt;
&lt;div id="biogrp"&gt;
&lt;h2&gt;About the Author&lt;/h2&gt;
&lt;p&gt;
&lt;a shape="rect" href="http://ee.usc.edu/faculty_staff/faculty_directory/willner.htm"&gt;Alan E. Willner&lt;/a&gt;
&lt;span&gt; is a professor of electrical engineering at the University of Southern California. &lt;/span&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Thu, 4 Aug 2016 19:30:00 GMT</pubDate>
<dc:creator>Alan E. Willner</dc:creator>
<guid>http://spectrum.ieee.org/telecom/wireless/twisted-light-could-dramatically-boost-data-rates</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MTIwOQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MTIwNw.jpg" height="225" width="300"/>
</item>
<item>
<title>Apps Enable Faster Product Development</title>
<link>http://spectrum.ieee.org/computing/software/apps-enable-faster-product-development</link>
<description>Hear from simulation experts about their experience with computational apps and how they are creating the future of numerical simulation</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Hear from simulation experts about their experience with computational apps and how they are creating the future of numerical simulation&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc1MTc4MQ.png"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MDY4NQ.png"/&gt;
&lt;/div&gt;
&lt;p&gt;In this keynote presentation at the COMSOL Conference in Boston, Jeff Crompton, cofounder and principal of AltaSim Technologies, speaks about using computational analysis to improve and develop new technologies, products and processes, and to bring products to market faster with reduced development costs.&lt;/p&gt;
&lt;p&gt;Generally, in the past, new processes and product development was led by larger companies, there was a heavy bias towards time-consuming physical prototypes, and innovation was incremental and evolutionary. At a certain point within companies, it became clear that it would not be possible to continue to crash cars worth thousands of dollars just to get incremental improvements in the impact resistance. Nor would it be feasible to test nuclear power designs by evaluation and fixing the flaws as you go along.&lt;/p&gt;
&lt;p&gt;Mathematical modeling and numerical simulation, made possible by rapid improvements in computer hardware, has benefited a diverse spectrum of industries, from potato chip manufacturers who prevented chips from flying off the conveyor belt, to airline wing engineers who achieved a tenfold reduction in the number of physical prototypes necessary. AltaSim makes use of the Application Builder within COMSOL Multiphysic® software to create simulation apps that streamline simulation communication with the client.&lt;/p&gt;
&lt;p&gt;One of such computational apps deployed is HeatSinkSim, which contains a heat sink design for electronic circuits and provides analysis of conduction, convection and radiation. The analysis contains two levels: the first a parametric geometry sweep to determine optimum fin size spacing, followed by a recommendation as to whether component temperatures will be exceeded. The second is the option for a full heat transfer analysis, which allows the user to calculate temperature distribution over the entire heat sink. The app is deployed from the COMSOL Server, has an easy-to-use GUI for user inputs, and is cluster-computing compatible.&lt;/p&gt;
&lt;p&gt;Computational apps extend the power of numerical simulation to the rest of the design team while concealing the complexity of the underlying mathematical modeling in an easy-to-use app. With an app, it is possible to make expert knowledge widely available, fostering innovation at every level. As an app is much more straightforward to use than any simulation software, the simulation reaches a much greater audience.&lt;/p&gt;
&lt;p&gt;The return on investment of modeling and simulation is significant, as companies have reported important reductions in time, expenses, and prototyping as well as increases in quality, productivity, and overall development. As new products and technologies account for a significant portion of company revenues, time to market and development costs are more important than ever. As products are increasingly more complex, multiphysics simulation allows companies to continually innovate, stay ahead of the curve, and be commercially successful.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a shape="rect" href="http://comsol.com/c/3wvz"&gt;http://comsol.com/c/3wvz&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 3 Aug 2016 20:51:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/apps-enable-faster-product-development</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc1MTc4OQ.png" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc1MTc4Nw.png" height="225" width="300"/>
</item>
<item>
<title>Hong Kong Bitcoin Exchange BitFinex Loses Nearly 120,000 Bitcoins in Hack</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/internet/bitfinex-hong-kong-bitcoin-exchange-loses-nearly-120000-bitcoin-in-hack</link>
<description>$72 million worth of coins stolen, Bitcoin price plummets, individual customer accounts hacked despite multisignature protection</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;$72 million worth of coins stolen, Bitcoin price plummets, individual customer accounts hacked despite multisignature protection&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NTk5Ng.jpeg"/&gt;
&lt;figcaption&gt;Illustration: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Yesterday afternoon, BitFinex, a Bitcoin exchange in Hong Kong, disabled its customer deposits and withdrawals functions and replaced the trading engine on its website with &lt;a shape="rect" href="https://www.bitfinex.com/"&gt;notification of a major security breach&lt;/a&gt;. Later in the day, &lt;a shape="rect" href="https://www.linkedin.com/in/zane-tackett-48914435"&gt;Zane Tackett&lt;/a&gt;, the “Director of Community and Product Development” took to Reddit (under the username “zanetackett”) to confirm that an attack had occurred and that &lt;a shape="rect" href="https://www.reddit.com/r/Bitcoin/comments/4vupa6/p2shinfo_shows_movement_out_of_multisig_wallets/d61oe33"&gt;nearly 120,000 bitcoins had been stolen&lt;/a&gt; from individual customer accounts.&lt;/p&gt;
&lt;p&gt;This latest hack, which amounts to a loss of around US $72 million, is the biggest plundering of a Bitcoin exchange since 2014 when &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/networks/the-mt-gox-bitcoin-debacle-an-update"&gt;850,000 bitcoins disappeared&lt;/a&gt; from the books during Mark Karpeles’s tenure as CEO of Mt. Gox. As was the case in 2014, the value of the currency is now crashing. The market price of bitcoin, which had begun to steadily increase at the beginning of the summer, &lt;a shape="rect" href="http://coinmarketcap.com/currencies/bitcoin/#charts"&gt;fell 15 percent&lt;/a&gt; on news of the BitFinex hack.&lt;/p&gt;
&lt;p&gt;The statement from BitFinex provides no details as to how the attack was conducted, but assures customers that “the theft is being reported to—and we are co-operating with—law enforcement.”&lt;/p&gt;
&lt;p&gt;Statements from Tackett on social media seem to rule out the possibility of an inside job. As a result, much speculation is being placed on the key management strategy that BitFinex had setup with its partner, &lt;a shape="rect" href="https://www.bitgo.com/"&gt;BitGo&lt;/a&gt;, a Bitcoin wallet provider that uses multisignature transactions for security.&lt;/p&gt;
&lt;p&gt;Multisignature transactions allow Bitcoin users to assign multiple private keys—the cryptographic proof required to initiate a transaction on the network—to a single Bitcoin address. In order to strengthen security, the keys attached to a multisignature address can be divied up among parties such that no one entity has full license to the spend the coins in that address. The measure is designed to provide an alternative to the single point of failure where one person holding a master key stands to lose everything in the event of a hack. If used correctly, multi-signature transactions can also limit the amount of trust in the relationship between cryptocurrency traders and exchanges.&lt;/p&gt;
&lt;p&gt;BitFinex was compelled to set up multi-signature addresses for each of its trading customers after an investigation into its operations by the &lt;a shape="rect" href="http://www.cftc.gov/index.htm"&gt;Commodity Futures Trading Commission&lt;/a&gt;. Among other things, the regulatory commission &lt;a shape="rect" href="https://www.scribd.com/doc/314629876/Bitfinex-Order?keyword=4417&amp;amp;content=10079&amp;amp;ad_group=Online+Tracking+Link&amp;amp;campaign=Skimbit%2C+Ltd.&amp;amp;source=impactradius&amp;amp;medium=affiliate&amp;amp;irgwc=1"&gt;faulted BitFinex&lt;/a&gt; for holding client funds in an internal address that was exclusively controlled by the exchange. In order to comply with the Commission, BitFinex turned to BitGo. Each customer was then assigned a separate Bitcoin address to hold their deposits with three keys assigned. One key was held by BitGo. Two were held by BitFinex—one offline and one online. For any transaction to go through, any two of these keys would have to be presented.&lt;/p&gt;
&lt;p&gt;As a holder of two of the keys, BitFinex, or a hacker with access to both the company’s keys, could have initiated the fraudulent transactions. Or, the hack could have involved a breach of both the BitGo and BitFinex security apparatus.&lt;/p&gt;
&lt;p&gt;However, both scenarios make it clear that multisignature wallets are not a magic solution to the problem of rampant robbery of Bitcoin exchanges. Even the strongest security tools are useless when improperly implemented, as seems to be the case once again.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 3 Aug 2016 19:30:00 GMT</pubDate>
<dc:creator>Morgen E. Peck</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/internet/bitfinex-hong-kong-bitcoin-exchange-loses-nearly-120000-bitcoin-in-hack</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NjAwNQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NjAwMw.jpg" height="225" width="300"/>
</item>
<item>
<title>CP/M Creator Gary Kildall’s Memoirs Released as Free Download</title>
<link>http://spectrum.ieee.org/view-from-the-valley/geek-life/history/cpm-creator-gary-kildalls-memoirs-released-as-free-download</link>
<description>Gary Kildall’s story shows how he paved a path for the start-up culture, say his children</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Gary Kildall’s story shows how he paved a path for the start-up culture, say his children&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NTk3MQ.jpeg"/&gt;
&lt;figcaption&gt;Randi Klett Photo: Tom Munnecke/Getty Images; Journal: The Kildall Family/Computer History Museum&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;The year before his death in 1994, Gary Kildall—inventor of the early microcomputer operating system CP/M—wrote a draft of a memoir, “Computer Connections: People, Places, and Events in the Evolution of the Personal Computer Industry.” He distributed copies to family and friends, but died before realizing his plans to release it as a book.&lt;/p&gt;
&lt;p&gt;This week, the Computer History Museum in Mountain View, with the permission of Kildall’s children, released the &lt;a shape="rect" href="http://www.computerhistory.org/_static/atchm/in-his-own-words-gary-kildall/"&gt;first portion of that memoir&lt;/a&gt;. You can download it &lt;a shape="rect" href="http://www.computerhistory.org/_static/atchm/in-his-own-words-gary-kildall/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wrote Scott and Kristin Kildall in an introductory letter: “In this excerpt, you will read how Gary and Dorothy started from modest means as a young married couple, paved a new path for start-up culture, and embraced their idea of success to become leaders in the industry. Our father embodied a definition of success that we can all learn from: one that puts inventions, ideas, and a love of life before profits as the paramount goal.”&lt;/p&gt;
&lt;p&gt;Later chapters, they indicated, did “not reflect his true self,” but rather his struggles with alcoholism, and will remain unpublished.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 3 Aug 2016 19:06:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/geek-life/history/cpm-creator-gary-kildalls-memoirs-released-as-free-download</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NTk4Mg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NTk4MA.jpg" height="225" width="300"/>
</item>
<item>
<title>How to Detect a GPS Spoof on a Superyacht</title>
<link>http://spectrum.ieee.org/video/computing/software/how-to-detect-a-gps-spoof-on-a-superyacht</link>
<description>In a live demo, a detector deploys direction-of-arrival sensing to alert users on board a superyacht to GPS spoofing</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;In a live demo, a detector deploys direction-of-arrival sensing to alert users on board a superyacht to GPS spoofing&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NTM1OQ.jpeg"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Every day, smartphone users, cruise ship captains, and airplane pilots rely on the Global Positioning System (GPS) to safely navigate to their destinations. But attackers can dupe a receiver into thinking it’s somewhere that it’s not by imitating signals emitted from the constellation of GPS satellites operated by the U.S. Air Force. With the right know-how, executing such a spoof requires only about $2,000 worth of equipment. This threat is not purely theoretical: &lt;a shape="rect" href="http://www.bbc.com/news/world-middle-east-16098562"&gt;Iran once claimed&lt;/a&gt; to have used GPS spoofing to divert a highly classified CIA drone from its route to Afghanistan. &lt;/p&gt;
&lt;p&gt;Now, researchers and some manufacturers are developing GPS spoofing detectors&lt;span&gt; to prevent users from being unwittingly steered off course. In June 2014, aerospace engineer &lt;/span&gt;
&lt;a shape="rect" href="https://gps.mae.cornell.edu/"&gt;Mark L. Psiaki &lt;/a&gt;
&lt;span&gt;of Virginia Tech (formerly of Cornell University) successfully demonstrated one such spoofing detector on board the &lt;/span&gt;
&lt;em&gt;White Rose of Drachs, &lt;/em&gt;
&lt;span&gt;a US $80 million superyacht. His detector used direction-of-arrival sensing, which relies on principles of interferometry to monitor the directions from which GPS signals arrive, and to identify inconsistencies in those angles. It alerted the ship’s crew to a spoofing attack launched as the &lt;/span&gt;ship cruised off the coast of Italy. The spoof was coordinated by a group under the direction of &lt;a shape="rect" href="http://www.ae.utexas.edu/faculty/faculty-directory/humphreys"&gt;Todd E. Humphreys&lt;/a&gt;, who leads the Radionavigation Laboratory at the University of Texas at Austin. &lt;/p&gt;
&lt;p&gt;Read More: &lt;a shape="rect" href="http://spectrum.ieee.org/telecom/security/protecting-gps-from-spoofers-is-critical-to-the-future-of-navigation"&gt;Protecting GPS From Spoofers Is Critical to the Future of Navigation&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 2 Aug 2016 17:30:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/video/computing/software/how-to-detect-a-gps-spoof-on-a-superyacht</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NTM3NQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NTM3Nw.jpg" height="225" width="300"/>
</item>
<item>
<title>Pokemon Go Is AR’s Foot in the Door to Our World</title>
<link>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gaming/pokemon-go-is-ars-foot-in-the-door-of-our-world</link>
<description>Pokemon Go may just be AR-light, but it is preparing its users for a time in which AR relationships help people deal with real-world challenges</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Pokemon Go may just be AR-light, but it is preparing its users for a time in which AR relationships help people deal with real-world challenges&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NTA0OQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Noriko Hayashi/Bloomberg/Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Taking only 13 hours to reach the top of the highest grossing app chart in the United States, &lt;a shape="rect" href="http://www.pokemon.com/us/pokemon-video-games/pokemon-go/"&gt;Pokémon Go&lt;/a&gt; broke a number of world records in its first week, and is now the phone app with which people spend the most time daily, surpassing Facebook, Snapchat, and Twitter. Even &lt;a shape="rect" href="https://www.nianticlabs.com/"&gt;Niantic&lt;/a&gt;, the company behind Pokémon Go, wasn’t prepared for this level of popularity, and its servers are struggling. Many non-players, walking around those determined to “catch them all,” are wondering when &lt;span&gt;(and hoping that) &lt;/span&gt;this craze will end. But even they will agree that the launch of Pokémon Go will mark a place in history, and it is likely to influence the direction of game development and player interactions for years to come.&lt;/p&gt;
&lt;p&gt;Technically, Pokémon Go is not an &lt;a shape="rect" href="http://spectrum.ieee.org/biomedical/bionics/augmented-reality-in-a-contact-lens"&gt;augmented reality (AR)&lt;/a&gt; game—in that it is not really integrating the Pokémons into the user’s perception of the world—though some believe the overlay of the cartoon characters on the camera’s view of reality might be the earliest glimpse to what AR games would look like in the future. The game, as it stands, is more of a location-based game. True AR will come in the form of headsets that seamlessly fuse the virtual and the real from companies like &lt;a shape="rect" href="https://www.metavision.com/"&gt;Meta&lt;/a&gt; and &lt;a shape="rect" href="http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gaming/magic-leap-hiring-software-engineers-for-new-development-lab-on-lucasfilms-san-francisco-campus"&gt;Magic Leap&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; Pokémon Go, however, is indeed playing a critical role in democratizing the &lt;em&gt;idea&lt;/em&gt; of AR, so that everyday users will be more prepared to adopt the technology when it is later introduced.  Shifting the public’s understanding of what is “normal” is critical for the adoption of emerging technology. Remember when the hands-free feature first became available for cellular devices? People walking in the streets, ostensibly mumbling and laughing to themselves, confused many nonusers. Nowadays, people barely even notice this. Likewise, today’s Pokemon Go players look odd to outsiders, but norms will shift here as well.&lt;/p&gt;
&lt;p&gt;Briefly, if you haven’t yet played the game, the premise starts with the Pokemon characters, a library of some 700 creatures that have been featured in video games, trading cards, television shows, and movies over a span of more than twenty years. The player creates an avatar, and then visits real-world points of interest to collect balls and other tools that help catch virtual creatures. A map helps the player track down the Pokemon characters, some of which are common, some rare, and most appropriate to their environments—with water creatures found near lakes and ponds, for example. To catch a Pokemon, the player looks through the camera on the mobile device to see the character as an overlay on the real-world image, and then tries to accurately hit the character with a virtual ball. Collect enough characters, and then the player can join a team and go to a pre-determined location in the real world, like a coffee shop, that serves as a virtual gym, where they can set characters loose to battle opponents.&lt;/p&gt;
&lt;aside class="inlay pullquote lt med-lrg"&gt;In the future, other television shows may be extended and integrated into viewers’ lives, allowing them to develop stronger relationships with the media characters&lt;/aside&gt;
&lt;p&gt;The desire to interact closely with fictional characters has been documented time and again in media scholarship since the 1950’s. “Parasocial interactions” refer to the ways in which users of mass media perceive a level of social interactions with media figures or characters; this fosters perceptions of companionship and social support. And perceived levels of parasocial interaction turn out to be one of the strongest predictors of media content consumption.&lt;/p&gt;
&lt;p&gt;So individuals consume media because they want to enjoy the social interactions that they perceive between themselves and the characters they like. For instance, children speak to animated characters on television as if they might be able to hold conversations with the characters; fans of all ages can become very upset if their favorite character in a television series meets a sudden, unexpected death.&lt;/p&gt;
&lt;p&gt;Games like Pokémon Go can take the concept of parasocial interaction to a different level, because users become an active participant and interact with the characters, and parasocial interactions become more like true social relationships—bidirectional. Fans of the original Pokémon franchise, who have been following the animations and video game releases, can now become trainers themselves, catching Pokémon and helping them battle against others. It is little wonder why the game can be so engaging.&lt;/p&gt;
&lt;p&gt;When AR technology is truly able to integrate digital elements into the real world, above and beyond the current location-based mobile games, we may be looking at a different form of media consumption that shifts seamlessly from one media platform to another. Pokémon shows the path here; it moved from video games, to television, then to our mobile phones, creating constant streams of more interactive and more intimate media consumption experiences. In the future, other television show may be extended and integrated into viewers’ lives, allowing them to develop stronger relationships with the media characters. For hardcore fans, this might be a dream come true; for others, having the presence of media characters interweaved throughout their virtual and physical worlds may seem rather intrusive.&lt;/p&gt;
&lt;p&gt;But the true potential of AR technology lies beyond gaming and entertainment applications, as tools to assist people in navigating various aspects their lives, affecting education, health, and environmental sustainability. For instance, my laboratory has tested preliminary applications of “mixed reality” &lt;a shape="rect" href="http://onlinelibrary.wiley.com/doi/10.1002/9781118952788.ch12/summary"&gt;health programs&lt;/a&gt; that incorporate &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&amp;amp;arnumber=6777460"&gt;virtual pets&lt;/a&gt;, wearable activity monitors like Fitbit, motion sensing input devices like Microsoft Kinect, and mobile phones to promote &lt;a shape="rect" href="http://www.tandfonline.com/doi/abs/10.1080/10810730.2015.1018597?journalCode=uhcm20"&gt;physical activity&lt;/a&gt; and &lt;a shape="rect" href="http://online.liebertpub.com/doi/abs/10.1089/cyber.2015.0224?journalCode=cyber"&gt;healthier eating&lt;/a&gt; choices for children. Our findings suggest that even without using gaming mechanics, children can be motivated to make substantially healthier lifestyle choices with the help of the parasocial interaction with the virtual pet.&lt;/p&gt;
&lt;aside class="inlay pullquote rt med-lrg"&gt;Imagine a virtual buddy who will be able to provide encouragement and support anywhere and anytime...a Pikachu that can walk a student throug a difficult STEM assignment...and can also spend time with the student after school.&lt;/aside&gt;
&lt;p&gt;Much of the initial positive buzz around Pokemon Go has centered on its ability to get former couch potatoes out walking, sometimes for hours each day. However, emerging evidence, including my own findings, suggests that when games designed to foster physical activity rely on extrinsic rewards, such as points and unlocking levels, and people start exercising purely to obtain the rewards, this actually takes away the inherent pleasures of exercising (becoming fit and healthy) and people stop exercising as soon as the rewards stop. So if people start to exercise purely for the sake of earning points and leveling up in Pokémon Go, they may stop exercising as soon as the novelty of the game wears off and they no longer wish to “catch ‘em all.” On the other hand, games that allow players to enjoy intrinsic rewards, such as social interactions among players and pleasures of interacting with game characters, will engage players for longer periods. Considering that the Pokémon franchise has successfully kept long-time fans, it’s quite possible that many of them will continue to play the game even after the novelty has worn off, contributing to constructing a lasting player base for the game.  &lt;/p&gt;
&lt;p&gt;The true positive potential of Pokemon Go and its AR brethren that are coming soon, goes beyond fitness. AR offers potentials for &lt;a shape="rect" href="http://spectrum.ieee.org/telecom/internet/virtual-reality-and-social-networks-will-be-a-powerful-combination"&gt;stronger parasocial interactions&lt;/a&gt; that lead to parasocial &lt;em&gt;relationships&lt;/em&gt;, allowing virtual characters to become a source of social support, particularly for vulnerable or underserved populations who may not have regular access to the social support they need. Imagine a virtual buddy who will be able to provide encouragement and support anywhere and anytime one needs help or companionship—a Pikachu that can walk a student through a difficult STEM assignment during a lab session in school and can also spend time with the student after school, for instance.&lt;/p&gt;
&lt;p&gt;Of course, the technology still has a myriad of challenges to address before this future can be realized. But recall that the first massive, bulky mobile phones were sold for almost $4,000 in the 80’s. By early 2000s, people started walking around with slim computers in their palms. The scenes from science fiction movies just a decade ago are already being realized. Even if the game’s popularity does not last, the worldwide phenomenon of Pokémon Go may be a historical moment heralding a future where the virtual is organically intertwined with the real, and social relationships are mixed with parasocial ones. &lt;/p&gt;
&lt;p/&gt;
&lt;p/&gt;
&lt;p&gt;
&lt;em&gt;
&lt;a shape="rect" href="http://grady.uga.edu/directory/profile/ahn"&gt;Sun Joo (Grace) Ahn &lt;/a&gt;
&lt;/em&gt;
&lt;em&gt;has long studied the virtual world. Ahn currently is an assistant professor of advertising at the Grady College of Journalism &amp;amp; Mass Communication at the University of Georgia, and is founding director of the Games and Virtual Environments Lab. Her main area of research looks at how virtual experiences through digital devices change the way people think and behave in the physical world. She received her masters and doctoral degrees at Stanford University, where she worked in &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/consumer-electronics/portable-devices/virtual-reality-pioneer-looks-beyond-entertainment"&gt;the Virtual Human Interaction Lab&lt;/a&gt; researching how user experiences in virtual worlds can influence their attitudes and behaviors in the physical world.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 1 Aug 2016 18:15:00 GMT</pubDate>
<dc:creator>Sun Joo (Grace) Ahn</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gaming/pokemon-go-is-ars-foot-in-the-door-of-our-world</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NTA2MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NTA1OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Deploy Simulation Apps Across a Larger Team</title>
<link>http://spectrum.ieee.org/computing/software/deploy-simulation-apps-across-a-larger-team</link>
<description>Build a custom application based on your mathematical models and let your colleagues and customers benefit from your expertise</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Build a custom application based on your mathematical models and let your colleagues and customers benefit from your expertise&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc1MTc1MQ.jpeg"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MDY3MQ.jpeg"/&gt;
&lt;/div&gt;
&lt;p&gt;The traditional use of numerical simulation in product development involves creating a mathematical model, identifying the range of parameters that will be used to simulate the device or process, performing the computations and preparing reports to communicate the results. Making any changes to the simulation requires going back to previous steps and making changes, which requires intimate knowledge of the original model.&lt;/p&gt;
&lt;p&gt;With the Application Builder, the tool within COMSOL Multiphysics® that wraps the mathematical model in an easy-to-use app, the simulation expert is able to create apps that colleagues can use to run their own design ideas. This distributes the analysis capabilities throughout a team or company, and allows the simulation experts to spend more time adding complexity to the models or creating new ones.&lt;/p&gt;
&lt;p&gt;Users of the app, such as the rest of the design team or colleagues in another department, can benefit from the simulation without being overwhelmed with complexity. Additionally, proprietary aspects of the simulation can be hidden, even as the app is used.&lt;/p&gt;
&lt;p&gt;The author of the app can manage its deployment with COMSOL Server. Engineers other than the creator of the model can investigate the benefits of using different materials or geometries without having to master the intricacy of the complete simulation. Professionals who are not scientists or engineers can make use of apps to demonstrate the potential of a certain technology to new investors or collaborators.&lt;/p&gt;
&lt;p&gt;With the COMSOL Multiphysics® Application Builder and COMSOL Server you can overcome the disconnect between simulation specialists and design engineers and leverage a larger workforce, reducing overall product-to-market time.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a shape="rect" href="http://comsol.com/c/3wvp"&gt;http://comsol.com/c/3wvp&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 29 Jul 2016 20:40:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/deploy-simulation-apps-across-a-larger-team</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc1MTc1OQ.jpeg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc1MTc1Nw.jpeg" height="225" width="300"/>
</item>
<item>
<title>Cybersecurity Startup: If the DNC Had Our Security Tech, It Could Have Bricked Those Files</title>
<link>http://spectrum.ieee.org/view-from-the-valley/at-work/start-ups/cybersecurity-startup-says-if-the-dnc-had-its-tech-it-could-have-bricked-those-files</link>
<description>Silicon Valley’s ThinAir Labs aims to bring fraud detection tools from the credit card industry onto your computer</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Silicon Valley’s ThinAir Labs aims to bring fraud detection tools from the credit card industry onto your computer&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5NDE2Mg.jpeg"/&gt;
&lt;figcaption&gt;Photo: Tekla Perry&lt;/figcaption&gt;
&lt;figcaption&gt;ThinAir founder and CEO Tony Gauda, with the company’s logo—a pyramid, chosen because a pyramid is a stable structure, and the more you press down, i.e., hack it, the stronger it gets.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;I generally cover consumer technology, not enterprise. And particularly not enterprise IT security, which can be extremely arcane. But given this week’s political developments, with the Wikileaks release of the Democratic National Committee’s email (suspected to have been purloined by Russian hackers), security has certainly been on my mind.&lt;/p&gt;
&lt;p&gt;Silicon Valley startup &lt;a shape="rect" href="https://thinair.com/"&gt;ThinAir Labs&lt;/a&gt; made me an interesting promise: that they could explain their security approach so concretely and succinctly that I would understand the gist of it in less than an hour, if not minutes. So I made a quick trip to their downtown Palo Alto offices to check it out. And company founder and CEO &lt;a shape="rect" href="https://www.linkedin.com/in/tonygauda"&gt;Tony Gauda&lt;/a&gt; was right; it’s an easy concept to grasp (though it took him three years to implement).&lt;/p&gt;
&lt;aside class="inlay pullquote lt med-sm"&gt;“In the case of the DNC emails, the system would have noticed that tons of data were being exfiltrated…It could see where they were going—to the Kremlin, perhaps?”&lt;span class="pq-attrib"&gt;—Tony Gauda, CEO ThinAir Labs&lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;Gauda spent much of his career building fraud prediction systems for Mastercard—systems that, he says, were the first to detect the massive &lt;a shape="rect" href="http://spectrum.ieee.org/telecom/security"&gt;security&lt;/a&gt; breaches at Target and Home Depot. He then went on to found &lt;a shape="rect" href="https://www.bitcasa.com/"&gt;BitCasa&lt;/a&gt;, a company that offered encrypted cloud storage for consumers. ThinAir Labs, founded in 2013, combines both approaches, he says.&lt;/p&gt;
&lt;p&gt;Generally, when the company’s software, ThinAir, is running on a computer, it’s monitoring behavior, in the same way credit card fraud prediction systems monitor behavior, Gauda explains. If the system spots something odd, say, a number of files are being rapidly opened in succession, at a pace far faster than you’d be able to do manually, or screen shots are being taken of documents, or documents are being copied and sent somewhere you’ve never had contact with previously, you’ll get an alert on your phone that will not only tell you what is happening with your computer, but where.&lt;/p&gt;
&lt;p&gt;“People’s behavior is usually consistent, they open things at a human speed, they run normal applications. Malware and hackers have different behavior. Copying two terabytes of data isn’t normal behavior,” says Gauda.&lt;/p&gt;
&lt;figure class="rt med-lrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5NDE2MQ.jpeg"/&gt;Photo: ThinAir Labs&lt;/figure&gt;
&lt;p&gt;In the case of the DNC emails, the system, Gauda says, would have spotted that files were being copied and showed on a map where they were going to so a system administrator could have stopped the export and given investigators a good idea of where the hack originated.&lt;/p&gt;
&lt;p&gt;For confidential files, Gauda says, ThinAir has another layer of security: a virtual vault on the user’s computer. To protect a file, the user puts it into the “safe.” That locks the file so only authorized users can access it. It stays protected when it comes out of the safe, whether dragged, copied, or emailed. Others will be able to open the file if they are people that the user normally allows to open these types of files (that behavior tracking again); when new correspondents get the file and try to open it, the documents creator will get an alert asking for authorization. If files are stolen, the document owner can lock them all up instantly. The system works with any kind of file, even ones it has never seen before. “You could be a spook agency with an app that has never seen the light of day, and this would still work out of the box to protect it,” Gauda says.&lt;/p&gt;
&lt;p&gt;“We deeply protect files in the safe, and monitor the files outside the safe,” Gauda says. “If one of the protected files gets stolen, we have a record of everyone who touched it, we can understand the scope of the breach, we know where it happened geographically, and we can go back and undo it. In the case of the DNC emails, the system would have noticed that tons of data were being exfiltrated because files were being opened. It could see where they were going—to the Kremlin, perhaps? And the DNC might have received an alert indicating, say, that Putin has requested access.”&lt;/p&gt;
&lt;p&gt;The simplicity, Gauda indicates, is hugely important, because people don’t use things that aren’t simple. “People and organizations make bad security decisions all the time, especially in the government,” he says, “just turn on the news any night to see it.”&lt;/p&gt;
&lt;p&gt;“We make the default behavior secure, rather than requiring people to change their behavior.”&lt;/p&gt;
&lt;p&gt;ThinAir will be launching next week at the Black Hat USA conference in Las Vegas. The company plans to charge a monthly fee; the price has yet to be announced.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 29 Jul 2016 14:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/at-work/start-ups/cybersecurity-startup-says-if-the-dnc-had-its-tech-it-could-have-bricked-those-files</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5NDE3OA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5NDE3Ng.jpg" height="225" width="300"/>
</item>
<item>
<title>The Manager’s Guide to Multiphysics Simulation</title>
<link>http://spectrum.ieee.org/computing/software/the-managers-guide-to-multiphysics-simulation</link>
<description>Multiphysics simulation reduces risk and shortens product development time</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Multiphysics simulation reduces risk and shortens product development time&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc1MTczMg.png"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MDYwOA.png"/&gt;
&lt;/div&gt;
&lt;p&gt;The Manager’s Guide is a window into ways that leading tech companies have incorporated multiphysics simulation into their workflow. This resource illustrates the mathematical modeling functionality of COMSOL Multiphysics® software and how it can be used by simulation experts to overcome design challenges that are not always possible to resolve experimentally. Read to learn how engineers across a wide range of industries used multiphysics simulation to improve critical aspects of product design.&lt;/p&gt;
&lt;p&gt;One story highlights how Boeing used multiphysics simulation to model thermal expansion in protective coatings for aircraft composites, which allowed engineers to use a lighter material, and still maintain sufficient lightning protection. An electrically conductive expanded metal foil (EMF) inner layer was added to the composite structure such that excess current and heat would dissipate rapidly in the event of lightning. However, thermal cycling induces stress that can result in cracks in the coating and is not ideal for the EMF. Engineers at Boeing used COMSOL Multiphysics® software to optimize EMF layer design; balancing current-carrying capacity, displacement due to thermally induced movement of protective layers, and weight; to create an optimized design that is both lightweight and protective against lightning.&lt;/p&gt;
&lt;p&gt;Another article describes how the car manufacturer, Toyota, optimized topology for improved cooling in their hybrid vehicles. Toyotas hybrid vehicles contain complex electrical systems that contain power semiconductor devices such as diodes and insulated gate bipolar transistors that are used for power conversion. To keep the systems within thermal operating conditions, the devices are mounted on aluminum heat sinks, around which a water/glycol coolant mixture is pumped. Engineers at the Toyota Research Institute of North America (TRI-NA) in Ann Arbor, Michigan used simulation to redesign the topology of the aluminum heat sink to reduce the size by half while dissipating the same amount of heat.&lt;/p&gt;
&lt;p&gt;Miele, the German manufacturer of induction stoves, is also highlighted in the Managers Guide. Engineers at MieleTec FH Bielefeld (a joint research laboratory between Miele &amp;amp; Cie. KG and the University of Applied Sciences Bielefeld, Germany) used COMSOL Multiphysics® to solve important design challenges in the development of their induction stoves. Simulating the induction heating process involved solving heat transfer concurrently with electromagnetics to determine the best operating conditions, materials, and geometry. Making use of simulation, the engineers were able to reduce the number of experiments needed to finalize their designs by 80%.&lt;/p&gt;
&lt;p&gt;Continue reading to see how companies in industries ranging from auto manufacturing to graphene and plasmonics incorporated multiphysics simulation into their work flow to optimize production design.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a shape="rect" href="http://comsol.com/c/3wvf"&gt;http://comsol.com/c/3wvf&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 27 Jul 2016 19:06:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/the-managers-guide-to-multiphysics-simulation</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc1MTc0MA.png" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc1MTczOA.png" height="225" width="300"/>
</item>
<item>
<title>Now You Too Can Buy Cloud-Based Deep Learning</title>
<link>http://spectrum.ieee.org/computing/software/now-you-too-can-buy-cloudbased-deep-learning</link>
<description>Cloud-computing services deliver AI to the rest of us</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Cloud-computing services deliver AI to the rest of us&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc4OTcxMQ.jpeg"/&gt;
&lt;figcaption&gt;Illustration: iStockphoto&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;style type="text/css"&gt;&amp;lt;!--
.article-detail aside.ct, .article-detail audio.ct, .article-detail canvas.ct, .article-detail div.ct, .article-detail figure.ct, .article-detail figcaption.ct, .article-detail form.ct, .article-detail iframe.ct, .article-detail img.ct, .article-detail object.ct, .article-detail nav.ct, .article-detail table.ct, .article-detail video.ct, .article-detail .inlay.ct {
    display: block;
    margin: 0 auto;
}
 ul, ol {
    margin: 1em 0;
    padding: 0 0 0 15px;
}
  .netNeutral {
	width:100%;
    margin: 0 0 0;
  background-color: #D7DCDB;
rules: rows;
  frame:hsides;
}

.netNeutral tr {
	border-bottom-width: 1px;
	border-bottom-style: solid;
	border-bottom-color: gray;
	padding-bottom: 10px;
}
.netNeutral tbody td:nth-child(odd) {
  background-color: #cbdbf4;
}
.netNeutral tbody td:nth-child(even) {
  background-color: #ecedf9;
}
.netNeutral tbody tr td {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
	line-height: 1.25em;
	vertical-align: top;
	padding: 10px 10px 10px 10px;
	font-weight: normal;
	width: 25%;
	border:2px solid white;
  	text-align: left;
}

.netNeutralInner tbody tr td {
font-family: Arial, Helvetica, sans-serif;
font-size: 14px;
line-height: 1.25em;
vertical-align: top;
padding: 10px 10px 10px 10px;
font-weight: normal;
width: 25%;
border: 0;
background: #E0E0E0;
text-align: left;
}

.article-detail h3.netNeutral {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 24px;
	color: #00AEE9;
	text-align: left;
	padding: 8px 0;
    line-height: 1em;
    margin:0;
	padding-bottom: 10px;
}

.netNeutral thead tr td {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
	line-height: 1.25em;
	font-weight: 700;
	vertical-align: top;
	margin: 10px 20px 10px 20px;
	padding: 10px 10px 10px 10px;
	border:2px solid white;
	background-color:#000;
	color:#FFF;
	text-align: center;
}
.netNeutral tbody tr td .innerTable tbody tr td {
	border-top-width: 0px;
	border-right-width: 0px;
	border-bottom-width: 0px;
	border-left-width: 0px;
}

.netNeutral tbody tr td ul li {
text-align: left;
list-style-position: outside;
list-style-type: disc;
text-indent: 0px;
}

--&amp;gt;
&lt;/style&gt;
&lt;figure class="rt med" role="img"&gt;
&lt;img alt="/image/Mjc4OTcxMA.jpeg" src="http://spectrum.ieee.org/image/Mjc4OTcxMA.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: iStockphoto&lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Facebook’s deep-learning&lt;strong/&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/robotics/artificial-intelligence"&gt;artificial intelligence&lt;/a&gt; systems have learned to recognize your friends in your photos, and Google’s AI has learned to anticipate what you’ll be searching for. But there’s no need to feel left out, even if your company’s computers haven’t learned much lately.&lt;/p&gt;
&lt;p&gt;A growing number of tech giants and startups have begun offering machine learning as a cloud service. That means other companies and startups do not need to develop their own specialized hardware or software to apply &lt;a shape="rect" href="http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning"&gt;deep ­learning&lt;/a&gt;—the high-powered version du jour of machine learning—to their specific business needs.&lt;/p&gt;
&lt;p&gt;“Deep-learning algorithms dominate other machine-learning methods when data sets are large,” says Zachary Chase Lipton, a deep-learning researcher in the &lt;a shape="rect" href="http://ai.ucsd.edu/"&gt;Artificial Intelligence Group at the University of California, San Diego&lt;/a&gt;, who has examined cloud AI services from companies such as Amazon and IBM. “Thus any company or application that has well-formed prediction problems—such as forecasting demand or translating between languages—could benefit from deep learning.”&lt;/p&gt;
&lt;p&gt;With &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/software/nervana-systems-puts-deep-learning-ai-in-the-cloud"&gt;cloud-based deep learning&lt;/a&gt;, companies can simply select a cloud service and browse its online offerings of application programming interfaces for software tasks such as recognizing images of corgi dogs or &lt;a shape="rect" href="http://googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html"&gt;automatically translating a restaurant menu.&lt;/a&gt; Some services will even tailor their machine-learning tools to the data and needs of individual companies.&lt;/p&gt;
&lt;p&gt;According to Lipton, the rise of cloud services for machine learning hinges on at least two factors: first, a continued rise in the demand for machine learning as the technology has matured in its ability to solve a wide variety of problems with economic value; and second, the relative scarcity of machine-learning talent, which makes it tough for every company to build its own machine-learning team. Competition for talent has become even tougher with startups trying to compete with tech giants like Microsoft and IBM, which can afford to vacuum up the best and brightest.&lt;/p&gt;
&lt;p/&gt;
&lt;p&gt;Most commercial applications&lt;strong/&gt;of machine learning rely on supervised learning. This involves algorithms that can observe correctly labeled examples and learn to perform certain tasks through imitation. Artificial neural networks are currently the most popular and successful algorithms for supervised machine learning on large data sets. They learn by passing information through an interconnected network of multiple nodes (also known as neurons). The connections between these nodes each have adjustable weights that influence the flow of information through the graph. Nodes are generally arranged in layers. But historically it was feasible to train networks with only one hidden layer of neurons in addition to the input and output layers.&lt;/p&gt;
&lt;p&gt;Deep learning takes these methods to the next level by filtering the data through multiple layers of neurons, ­Lipton explains. At each layer, the network can learn successively more abstract representations of relationships between data points. With enough layers and enough nodes, deep neural networks can perform a host of functions.&lt;/p&gt;
&lt;p&gt;The challenge in building a neural network is training it for specific tasks. Starting from a random setting of the weights, examples from the data set are presented to the neural network one after the next. Each time, the neural network’s weights are tuned slightly to bring the network’s output closer to the correct output.&lt;/p&gt;
&lt;div&gt;
&lt;aside class="inlay xlrg"&gt;
&lt;h3 class="sb-hed"&gt;The Ivy League of Deep Learning&lt;/h3&gt;
&lt;table class="netNeutral"&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan="1" rowspan="1"/&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Cloud&lt;br clear="none"/&gt;
			machine-learning service&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Open-source&lt;br clear="none"/&gt;
			machine-learning&lt;br clear="none"/&gt;
			tools&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Deep-learning&lt;br clear="none"/&gt;
			startup acquisitions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Amazon&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Amazon Machine Learning&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;DSSTNE&lt;/strong&gt;
&lt;br clear="none"/&gt;
			Deep Scalable Sparse&lt;br clear="none"/&gt;
			Tensor Network Engine;&lt;br clear="none"/&gt;
			library for building deep-learning models&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Orbeus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Facebook&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;None&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Tools for deep-learning&lt;br clear="none"/&gt;
			models released through the open-source Torch library&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Wit. AI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Google&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Google Cloud Machine Learning&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;TensorFlow&lt;/strong&gt;
&lt;br clear="none"/&gt;
			Library for developing deep-learning models and&lt;br clear="none"/&gt;
			more general machine-learning models&lt;br clear="none"/&gt;
			       &lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Dark Blue Labs, DeepMind, DNNresearch, Moodstocks,&lt;br clear="none"/&gt;
			Vision Factory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;IBM&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;IBM Watson&lt;br clear="none"/&gt;
			Analytics&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;IBM SystemML&lt;/strong&gt;
&lt;br clear="none"/&gt;
			Optimization platform for general machine-learning&lt;br clear="none"/&gt;
			models on the open-source Apache Spark library&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;AlchemyAPI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;Microsoft&lt;/strong&gt;
&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;Microsoft Azure&lt;br clear="none"/&gt;
			Machine Learning&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;
&lt;strong&gt;CNTK&lt;/strong&gt;
&lt;br clear="none"/&gt;
			Computational Network&lt;br clear="none"/&gt;
			Toolkit; library for building deep-learning models&lt;/td&gt;
&lt;td colspan="1" rowspan="1"&gt;SwiftKey&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/aside&gt;
&lt;/div&gt;
&lt;p&gt;A number of startups seem primarily interested in demonstrating their deep-learning research in order to draw the attention of larger companies that might acquire them, Lipton notes. Salesforce.com and Twitter have acquired ­startups, including &lt;a shape="rect" href="https://www.metamind.io/"&gt;MetaMind&lt;/a&gt; and &lt;a shape="rect" href="http://www.whetlab.com/"&gt;Whetlab&lt;/a&gt;, respectively. Some of these acquisitions have been done to swell the ranks of these tech giants’ own deep-learning teams.&lt;/p&gt;
&lt;p&gt;But some startups have focused their efforts on applying deep learning to very niche industry needs. For example, San Francisco–based Enlitic is using deep learning to help physicians spot signs of certain diseases or health conditions in medical images taken by X-ray or MRI machines. And Atomwise, in Mountain View, Calif., has applied deep learning to the discovery of new pharmaceuticals.&lt;/p&gt;
&lt;p&gt;Other startups aim to build platforms for broader categories that apply to many industries. Seattle-based Dato has created a deep-learning tool kit used by developers at companies such as Cisco and PayPal. &lt;a shape="rect" href="https://www.clarifai.com/"&gt;Clarifai&lt;/a&gt;, a startup in New York City, currently provides tools for automatically filtering and tagging images and video segments; its deep-learning technology is beneficial to end users as diverse as travel-photography sites, real estate agents, and online-comment moderators looking to filter out pornography.&lt;/p&gt;
&lt;p&gt;“When you think of everything from your email inbox to the advertisements you see with search results to image tagging, every possible aspect of these products is already benefiting from machine learning or will very soon,” says Matthew Zeiler, Clarifai’s founder and CEO.&lt;/p&gt;
&lt;p&gt;The challenge for deep-learning startups will be finding their niche in a crowded space. They would be wise to avoid directly challenging the tech giants in providing machine-learning services, says Naveen Rao, cofounder and CEO of &lt;a shape="rect" href="https://www.nervanasys.com/"&gt;Nervana Systems&lt;/a&gt;. His company has bet on building an optimized &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/software/nervana-systems-puts-deep-learning-ai-in-the-cloud"&gt;deep-learning platform&lt;/a&gt; that can provide results in record time when running neural networks on standard GPU hardware. Nervana has also been developing its own specialized chips that could give deep learning an additional performance boost.&lt;/p&gt;
&lt;p&gt;Ersatz Labs, a startup in Pacifica, Calif., found out just how perilous it can be to take on the industry’s Goliaths. Ersatz suspended development of its cloud machine-learning ambitions last year after falling short on fundraising. Dave Sullivan, the company’s CEO, points to the difficulties of selling services or products that enable people to do deep learning when many of those people work for large tech companies that prefer building their own tools and hiring internally.&lt;/p&gt;
&lt;p&gt;The truth of that statement was borne out at the 2016 Google I/O conference, where Google announced that it has, for more than a year, been using its own custom-built microchips—named Tensor Processing Units—to boost its machine-learning applications. Such specialized chips provide the hardware to run the &lt;a shape="rect" href="https://cloud.google.com/products/machine-learning/?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=2016-q1-cloud-na-ML-skws-freetrial&amp;amp;gclid=CI_V7I70k84CFUFkhgodK9kE4g"&gt;Google Cloud Machine Learning platform&lt;/a&gt;, which launched in 2016.&lt;/p&gt;
&lt;p&gt;“Everyone’s waiting for that killer app in deep learning right now, and nobody has figured it out yet,” Sullivan says. “It’s not platforms, though.”&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the August 2016 print issue as “For Sale: Deep Learning.”&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 27 Jul 2016 15:00:00 GMT</pubDate>
<dc:creator>Jeremy Hsu</dc:creator>
<guid>http://spectrum.ieee.org/computing/software/now-you-too-can-buy-cloudbased-deep-learning</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc4OTcyMA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc4OTcxOA.jpg" height="225" width="300"/>
</item>
<item>
<title>The 2016 Top Programming Languages</title>
<link>http://spectrum.ieee.org/computing/software/the-2016-top-programming-languages</link>
<description>C is No. 1, but big data is still the big winner</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;C is No. 1, but big data is still the big winner&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MjI0Nw.jpeg"/&gt;
&lt;figcaption&gt;Photo-illustration: The Kletts&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Welcome to &lt;em&gt;IEEE Spectrum&lt;/em&gt;’s &lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;third interactive ranking&lt;/a&gt; of the most popular programming languages. As it’s impossible to look over the shoulder of every programmer, &lt;em&gt;Spectrum&lt;/em&gt; uses various metrics as proxies for gauging the current use of a language. Working with data journalist &lt;a shape="rect" href="http://www.nickdiakopoulos.com/"&gt;Nick Diakopoulos,&lt;/a&gt; from 10 online sources we’ve chosen 12 metrics, each one of which gives insight along a different axis of popularity. Combining the metrics produces a single ranking.&lt;/p&gt;
&lt;p&gt;Because each of you may value different things—What’s hot with the folks in open source? What are employers demanding?—&lt;em&gt;Spectrum&lt;/em&gt;’s &lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;Interactive Top Programming Languages&lt;/a&gt; lets you adjust the weight of each metric in contributing to the ranking. You also have the option to filter the results: for example, looking only at languages used in embedded systems or mobile applications. You can customize each weighting yourself or use one of our presets.&lt;/p&gt;
&lt;p&gt;The default preset is intended to echo the interests of the average IEEE member. So what are &lt;em&gt;Spectrum&lt;/em&gt;’s Top Ten Languages for 2016?&lt;/p&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MjI0Ng.png"/&gt;
&lt;/a&gt;
&lt;/figure&gt;
&lt;p&gt;After two years in second place, &lt;a shape="rect" href="http://www.c-faq.com/"&gt;C&lt;/a&gt; has finally edged out &lt;a shape="rect" href="https://www.oracle.com/java/index.html"&gt;Java&lt;/a&gt; for the top spot. Staying in the top five, &lt;a shape="rect" href="https://www.python.org/"&gt;Python&lt;/a&gt; has swapped places with &lt;a shape="rect" href="https://isocpp.org/"&gt;C++&lt;/a&gt; to take the No. 3 position, and &lt;a shape="rect" href="https://msdn.microsoft.com/en-us/library/67ef8sbd.aspx"&gt;C#&lt;/a&gt; has fallen out of the top five to be replaced with &lt;a shape="rect" href="https://www.r-project.org/about.html"&gt;R.&lt;/a&gt; R is following its momentum from previous years, as part of a positive trend in general for modern big-data languages that &lt;span&gt;Diakopoulos&lt;/span&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/top-programming-languages-trends-the-rise-of-big-data"&gt;analyses in more detail here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Google and Apple are also making their presence felt, with Google’s &lt;a shape="rect" href="https://golang.org/"&gt;Go&lt;/a&gt; just beating out Apple’s &lt;a shape="rect" href="https://developer.apple.com/swift/"&gt;Swift&lt;/a&gt; for inclusion in the Top Ten. Still, Swift’s rise is impressive, as it’s jumped five positions to 11th place since last year, when it first entered the rankings. Several other languages also debuted last year, a marked difference from this year, with no new languages entering the rankings.&lt;/p&gt;
&lt;aside class="inlay rt sm"&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;
&lt;figure role="img"&gt;
&lt;img alt="graphic link to app page" src="http://spectrum.ieee.org/image/Mjc5MjY2Mw"/&gt;
&lt;figcaption&gt;
&lt;strong&gt;Explore the Interactive Rankings&lt;/strong&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;Other notable changes include &lt;a shape="rect" href="http://www.ladder-logic.com/hello-world/"&gt;Ladder Logic,&lt;/a&gt; rising five positions to 34th place. Ladder Logic is used in programmable logic controllers, especially those used in factories. Although manufacturing may seem like a narrow niche for a language, its relative popularity indicates just how big that niche really is. &lt;a shape="rect" href="https://www.w3.org/html/"&gt;HTML&lt;/a&gt; also continues to be popular, rising to 16th place, despite the horror of some previous users of the Top Programming Languages that it’s included at all (for the record, we take a pragmatic approach and define a programming language as a distinct syntax that is used to give a computer instructions, even if those are just instructions on how data should be structured; Turing completeness is not required).&lt;/p&gt;
&lt;p&gt;On the losing side, perhaps the most surprising is that Shell programming—a catchall term for the creation of scripts of the sort beloved by system administrators, using shells such as &lt;a shape="rect" href="https://www.gnu.org/software/bash/"&gt;bash&lt;/a&gt;—has declined in popularity, falling seven positions to 19th place. This may reflect the development of more sophisticated systems to manage cloud-based data centers, but we’ll have to wait to see if the trend continues next year or if this is just a statistical fluke.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 26 Jul 2016 16:00:00 GMT</pubDate>
<dc:creator>Stephen Cass</dc:creator>
<guid>http://spectrum.ieee.org/computing/software/the-2016-top-programming-languages</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MjI2MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MjI1OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Top Programming Languages Trends: The Rise of Big Data</title>
<link>http://spectrum.ieee.org/computing/software/top-programming-languages-trends-the-rise-of-big-data</link>
<description>Languages like Go, Julia, R, Scala, and even Python are riding the number-crunching wave</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Languages like Go, Julia, R, Scala, and even Python are riding the number-crunching wave&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MjU1OQ.jpeg"/&gt;
&lt;figcaption&gt;Illustration: Grzegorz Knec/Alamy&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MjU1OA.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Illustration: Grzegorz Knec/Alamy&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Now that &lt;em&gt;IEEE Spectrum&lt;/em&gt; is into the &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/the-2016-top-programming-languages"&gt;third year&lt;/a&gt; of &lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;annually ranking languages&lt;/a&gt;, we can start looking at some trends over time. What languages are ascendant? Which are losing traction? And which of the data sources that we use to create our rankings are contributing the most to these shifts?&lt;/p&gt;
&lt;p&gt;In this article I’m going to focus on so-called big-data languages, such as &lt;a shape="rect" href="http://julialang.org/"&gt;Julia&lt;/a&gt;, &lt;a shape="rect" href="https://www.python.org/"&gt;Python&lt;/a&gt;, &lt;a shape="rect" href="https://www.r-project.org/"&gt;R&lt;/a&gt;, and &lt;a shape="rect" href="http://www.scala-lang.org/"&gt;Scala&lt;/a&gt;. Most of these are purpose-built for handling large amounts of numeric data, with stables of packages that can be tapped for quick big-data analytic prototyping. These languages are increasingly important, as they facilitate the mining of the huge data sets that are now routinely collected across practically all sectors of government, science, and commerce.&lt;/p&gt;
&lt;p&gt;The biggest mover in this category was &lt;a shape="rect" href="https://golang.org/"&gt;Go&lt;/a&gt;, an open source language created by Google to help solve the company’s issues with scaling systems and concurrent programming back in 2007. In the &lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;default &lt;em&gt;Spectrum&lt;/em&gt; ranking&lt;/a&gt;, it’s moved up 10 positions since 2014 to settle into 10th place this year. Other big-data languages that saw moves since 2014 in the &lt;em&gt;Spectrum&lt;/em&gt; ranking were R and Scala, with R ascending 4 spots and Scala moving up 2 (although down from 2015, when it was up 4 places from its 2014 position). Julia was added to the list of languages we track in 2015, and in the past year it’s moved from rank 40 to 33, still a marginal player but clearly possessing some momentum in its growth.&lt;/p&gt;
&lt;p&gt;The chief reason for Go’s quick rise in our ranking is the large increase in related activity on the &lt;a shape="rect" href="https://github.com/"&gt;GitHub&lt;/a&gt; source code archive. Since 2014, the total number of repositories on GitHub that list Go as the primary language went up by a factor of more than four. If we look at just &lt;em&gt;active&lt;/em&gt; GitHub repositories, then there are almost five times as many. There’s also a fair bit more chatter about the language on &lt;a shape="rect" href="https://www.reddit.com"&gt;Reddit&lt;/a&gt;, with our data showing a threefold increase in the number of posts on that site mentioning the language.&lt;/p&gt;
&lt;aside class="inlay rt sm"&gt;
&lt;a shape="rect" href="http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016"&gt;
&lt;figure role="img"&gt;
&lt;img alt="graphic link to app page" src="http://spectrum.ieee.org/image/Mjc5MjY2Mw"/&gt;
&lt;figcaption&gt;
&lt;strong&gt;Explore the Interactive Rankings&lt;/strong&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;Another language that has continued to move up the rankings since 2014 is R, now in fifth place. R has been lifted in our rankings by racking up more questions on &lt;a shape="rect" href="http://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;—about 46 percent more since 2014. But even more important to R’s rise is that it is increasingly mentioned in scholarly research papers. The &lt;em&gt;Spectrum&lt;/em&gt; default ranking is heavily weighted toward data from &lt;a shape="rect" href="http://ieeexplore.ieee.org/Xplore/home.jsp"&gt;IEEE Xplore&lt;/a&gt;, which indexes millions of scholarly articles, standards, and books in the IEEE database. In our 2015 ranking there were a mere 39 papers talking about the language, whereas this year we logged 244 papers.&lt;/p&gt;
&lt;p&gt;Contrary to the substantial gains in the rankings seen by open source languages such as Go, Julia, R, and Scala, proprietary data-analysis languages such as &lt;a shape="rect" href="http://www.mathworks.com/products/matlab/"&gt;Matlab&lt;/a&gt; and &lt;a shape="rect" href="http://www.sascommunity.org/wiki/Main_Page"&gt;SAS&lt;/a&gt; have seen a drop-off: Matlab fell four places in the rankings since 2014 and SAS has fallen seven. However, it’s important to note that both of those languages are still growing; it’s just that they’re not growing as fast as some of the languages that are displacing them.&lt;/p&gt;
&lt;p&gt;When we weight the rankings toward jobs, we continue to see heavily used languages like &lt;a shape="rect" href="https://java.com"&gt;Java&lt;/a&gt; and Python dominate. But recruiters are much more interested in R and Scala in 2016 then they were in 2014. When we collected data in 2014, there were only 136 jobs listed for Scala on CareerBuilder and Dice. But by 2016 there was more than a fourfold increase, to 631 jobs.&lt;/p&gt;
&lt;p&gt;This growth invites the question &lt;a shape="rect" href="http://www.kdnuggets.com/2015/05/r-vs-python-data-science.html"&gt;whether R can ever unseat Python&lt;/a&gt; or Java as the top languages for big data. But while R has seen huge gains over the last few years, Python and Java really are 800-pound gorillas. For instance, we found roughly 15 times as many job listings for pythonistas as for R developers. And while we measured about 63,000 new GitHub repositories in the last year for R, there were close to 458,000 for Python. Although R may be great for visualization and exploratory analysis and is clearly popular with academics writing research papers, Python has significant advantages for users in production environments: It’s more easily integrated into production data pipelines, and as a general purpose language it simply has a broader array of uses.&lt;/p&gt;
&lt;p&gt;These data illustrate that despite the desire of some coders to evaluate languages on purely internal merits—the elegance of their syntax, or the degree and nature of the abstractions used—a big driver for a language’s popularity will always be the domains that it targets, either by design or through the availability of supporting libraries.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 26 Jul 2016 16:00:00 GMT</pubDate>
<dc:creator>Nicholas Diakopoulos</dc:creator>
<guid>http://spectrum.ieee.org/computing/software/top-programming-languages-trends-the-rise-of-big-data</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MjU3MQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MjU2OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Interactive: The Top Programming Languages 2016</title>
<link>http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016</link>
<description>Find the programming languages that are most important to you</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Find the programming languages that are most important to you&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MjY1Mg.jpeg"/&gt;
&lt;figcaption&gt;Screenshot: Mark Montgomery&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;link id="tpl" rel="stylesheet" type="text/css" href="http://spectrum.ieee.org/ns/IEEE_TPL_2016/CSS/styles.css"/&gt;
&lt;style type="text/css"&gt;#container {
margin-top: 0px;
margin-bottom: 25px;
}
&lt;/style&gt;
&lt;script charset="utf-8" src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/lib/d3.v3.js" type="text/javascript"/&gt;
&lt;script src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/lib/underscore.js" type="text/javascript"/&gt;
&lt;script src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/lib/backbone.js" type="text/javascript"/&gt;
&lt;link rel="stylesheet" href="http://spectrum.ieee.org/ns/IEEE_TPL_2016/lib/tooltips/tipr/tipr.css"/&gt;
&lt;script src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/lib/tooltips/tipr/tipr.js"/&gt;
&lt;script src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/data/DataModels_2016.js" type="text/javascript"/&gt;
&lt;script src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/MainView.js" type="text/javascript"/&gt;
&lt;script src="http://spectrum.ieee.org/ns/IEEE_TPL_2016/AppRouter.js" type="text/javascript"/&gt;
&lt;script id="mainTemplate" type="text/template"&gt;
			&amp;lt;div id="mainview"&amp;gt;
				&amp;lt;div class="headline"&amp;gt;&amp;lt;%= headline %&amp;gt;&amp;lt;/div&amp;gt;
				&amp;lt;div class="explainer"&amp;gt;&amp;lt;%= explainer %&amp;gt;&amp;lt;/div&amp;gt;
				&amp;lt;div&amp;gt;
					&amp;lt;div id="customization_controls" class="controls"&amp;gt;&amp;lt;/div&amp;gt;
					&amp;lt;div id="customization_controls_comparison" class="controls" style="display: none;"&amp;gt;&amp;lt;/div&amp;gt;
					&amp;lt;div id="filter_controls" class="controls"&amp;gt;&amp;lt;/div&amp;gt;
				&amp;lt;/div&amp;gt;
				&amp;lt;div class="editor"&amp;gt;&amp;lt;/div&amp;gt; 
				&amp;lt;div class="ranking"&amp;gt;&amp;lt;/div&amp;gt;
			&amp;lt;/div&amp;gt;
		&lt;/script&gt;
&lt;script id="customizationControls" type="text/template"&gt;
			&amp;lt;div&amp;gt;
				&amp;lt;div class="label"&amp;gt;Choose a Ranking&amp;lt;/div&amp;gt;
				&amp;lt;div class="label_sub"&amp;gt;(choose a weighting or make your own)&amp;lt;/div&amp;gt;
				&amp;lt;div&amp;gt;
					&amp;lt;div class='button_row_container'&amp;gt;
						&amp;lt;div class="selector_button tip" data-tip="Our all-round ranking." data-type="ieee-spectrum"&amp;gt;&amp;lt;div class="button_label"&amp;gt;IEEE Spectrum&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="Languages that are growing rapidly." data-type="trending"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Trending&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="Languages that are in demand by employers." data-type="jobs"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Jobs&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="Languages popular on open source hubs." data-type="open"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Open&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="A ranking of your own design." data-type="custom"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Custom&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;
					&amp;lt;/div&amp;gt;
					&amp;lt;div class='button_container'&amp;gt;
						&amp;lt;div class="buttonlink" data-type="reweight"&amp;gt;Edit Ranking&amp;lt;/div&amp;gt;&amp;lt;span class="buttondivider"&amp;gt;|&amp;lt;/span&amp;gt;&amp;lt;div class="buttonlink" data-type="compare"&amp;gt;Add a Comparison&amp;lt;/div&amp;gt;&amp;lt;span class="buttondivider" style="margin-right: 0px;"&amp;gt;|&amp;lt;/span&amp;gt;

						&amp;lt;a class="twitterbutton" target="_blank" href="http://www.twitter.com"&amp;gt;&amp;lt;img src='http://spectrum.ieee.org/ns/IEEE_TPL_2016/img/twitter_tiny.png'&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;/a&amp;gt;
						&amp;lt;a class="facebookbutton" target="_blank" href="http://www.facebook.com"&amp;gt;&amp;lt;img src='http://spectrum.ieee.org/ns/IEEE_TPL_2016/img/facebook_tiny.png'&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;/a&amp;gt;
					
					&amp;lt;/div&amp;gt;			
				&amp;lt;/div&amp;gt;					
			&amp;lt;/div&amp;gt;
		&lt;/script&gt;
&lt;script id="customizationControlsComparison" type="text/template"&gt;
			&amp;lt;div&amp;gt;
				&amp;lt;div class="label"&amp;gt;Choose a Comparison&amp;lt;/div&amp;gt;
				&amp;lt;div class="label_sub"&amp;gt;(choose a weighting or make your own)&amp;lt;/div&amp;gt;
				&amp;lt;div&amp;gt;
					&amp;lt;div class='button_row_container'&amp;gt;
						&amp;lt;div class="selector_button tip" data-tip="Our all-round ranking." data-type="ieee-spectrum"&amp;gt;&amp;lt;div class="button_label"&amp;gt;IEEE Spectrum&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="Languages that are growing rapidly." data-type="trending"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Trending&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="Languages that are in demand by employers." data-type="jobs"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Jobs&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="Languages popular on social media and open-source hubs." data-type="open"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Open&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button deselected tip" data-tip="A ranking of your own design." data-type="custom"&amp;gt;&amp;lt;div class="button_label"&amp;gt;Custom&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;
					&amp;lt;/div&amp;gt;
					&amp;lt;div class='button_container'&amp;gt;
						&amp;lt;div class="buttonlink" data-type="reweight"&amp;gt;Edit Ranking&amp;lt;/div&amp;gt;

					&amp;lt;/div&amp;gt;			
				&amp;lt;/div&amp;gt;					
			&amp;lt;/div&amp;gt;
		&lt;/script&gt;
&lt;script id="languageFiltersTemplate" type="text/template"&gt;
			&amp;lt;div&amp;gt;
				&amp;lt;div class="label"&amp;gt;Language Types&amp;lt;/div&amp;gt;
				&amp;lt;div class="label_sub"&amp;gt;(click to hide)&amp;lt;/div&amp;gt;
				&amp;lt;div&amp;gt;
					&amp;lt;div class='button_row_container'&amp;gt;
						&amp;lt;div class="selector_button tip" data-type="web" data-tip="Languages used for developing web sites and applications."&amp;gt;&amp;lt;img src='http://spectrum.ieee.org/ns/IEEE_TPL_2016/img/web_mini.png'&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;div class="button_label"&amp;gt;Web&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button tip" data-type="mobile" data-tip="Languages used for applications on mobile devices."&amp;gt;&amp;lt;img src='http://spectrum.ieee.org/ns/IEEE_TPL_2016/img/mobile_mini.png'&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;div class="button_label"&amp;gt;Mobile&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button tip" data-type="enterprise" data-tip="Languages used for enterprise, desktop, and scientific applications."&amp;gt;&amp;lt;img src='http://spectrum.ieee.org/ns/IEEE_TPL_2016/img/enterprise_mini.png'&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;div class="button_label"&amp;gt;Enterprise&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div class="selector_button tip" data-type="embedded" data-tip="Languages used to program device controllers."&amp;gt;&amp;lt;img src='http://spectrum.ieee.org/ns/IEEE_TPL_2016/img/embedded_mini.png'&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;div class="button_label"&amp;gt;Embedded&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;	
					&amp;lt;/div&amp;gt;					
				&amp;lt;/div&amp;gt;					
			&amp;lt;/div&amp;gt;
		&lt;/script&gt;
&lt;script type="text/javascript"&gt;
			$(document).ready(function() {
		
				window.DATA_ROOT = "http://spectrum.ieee.org/ns/IEEE_TPL_2016/data";	

				window.appRouter = new AppRouter();

				window.mainView = new MainView();

				$(window).on('orientationchange', function(event){
					orientationHandler();
				});

				$('.tip').tipr();
			});

			function orientationHandler(event) {
				window.mainView.updateOrientation();
			}

			function numberWithCommas(x) {				
			    return x.toString().replace(/\B(?=(\d{3})+(?!\d))/g, ",");
			}
			
		&lt;/script&gt;
&lt;div id="container"/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 26 Jul 2016 16:00:00 GMT</pubDate>
<dc:creator>Nick Diakopoulos and Stephen Cass</dc:creator>
<guid>http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MjY2Ng.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MjY2NA.jpg" height="225" width="300"/>
</item>
<item>
<title>EFF Sues U.S. Government Over DMCA</title>
<link>http://spectrum.ieee.org/tech-talk/computing/software/dmca-goes-on-trial</link>
<description>The Electronic Frontier Foundation, Andrew “bunnie” Huang, and Matthew Green say the Digital Millennium Copyright Act’s onerous copyright rules are unconstitutional</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The Electronic Frontier Foundation, Andrew “bunnie” Huang, and Matthew Green say the Digital Millennium Copyright Act’s onerous copyright rules are unconstitutional&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MjM4NQ.jpeg"/&gt;
&lt;figcaption&gt;Photo: Alamy&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Last Thursday lawyers from the &lt;a shape="rect" href="https://www.eff.org/press/releases/eff-lawsuit-takes-dmca-section-1201-research-and-technology-restrictions-violate"&gt;Electronic Frontier Foundation filed a lawsuit&lt;/a&gt; against the U.S. government on behalf of hardware guru &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Andrew_Huang_(hacker)"&gt;Andrew “bunnie” Huang&lt;/a&gt; and computer-security researcher &lt;a shape="rect" href="https://isi.jhu.edu/~mgreen/"&gt;Matthew Green&lt;/a&gt;. I&lt;span&gt;n a nutshell, t&lt;/span&gt;he pair allege that parts of the &lt;a shape="rect" href="http://www.copyright.gov/legislation/dmca.pdf"&gt;Digital Millennium Copyright Act&lt;/a&gt; are unconstitutional. Their objections center on Section 1201 of the DMCA, which makes it illegal to circumvent technical copy-protection schemes or to broadcast to others methods for doing so.&lt;/p&gt;
&lt;p&gt;At first blush you might think that these rules are quite fair. After all, it’s only reasonable that content producers should put technology in place to prevent the misappropriation of their intellectual property, and they should have the full force of the law behind them in their battle against people intent to rip them off, right?&lt;/p&gt;
&lt;p&gt;Perhaps. But, Huang, Green, and the EFF argue, the provisions of Section 1201 fail to balance the rights of the people who are using copyrighted works, which are protected in part by what’s known as &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Fair_use"&gt;fair-use&lt;/a&gt; doctrine. That’s why, to give a concrete example, &lt;em&gt;IEEE Spectrum&lt;/em&gt; could quote passages from a book we were reviewing without asking permission of the book’s publisher, who might not be so keen to allow that if we were, say, critically ridiculing the book.&lt;/p&gt;
&lt;figure class="lt med" role="img"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MjQwMw.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Photo: bunnie Huang&lt;/figcaption&gt;
&lt;figcaption&gt;Together with Matthew Green and the Electronic Frontier Foundation, bunnie Huang is suing the U.S. government over section 1201 of the Digital Millennium Copyright Act.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Indeed, we’re all pretty used to using copyrighted works in ways that the publishers of these works would probably prefer we didn’t. But thankfully, fair use puts limits on what content creators can object to. Section 1201, the plaintiffs argue, ignores those limits to the peril of our rights.&lt;/p&gt;
&lt;p&gt;Take the part of Section 1201 that makes it illegal to circumvent technical prevention measures to change the format of a digital work that you have lawfully obtained so that you can view it on a different device. There’s no copyright infringement here, yet the law forbids you from doing it. Were you to use your technical skills to accomplish such format-changing for someone else for a fee, you could incur stiff criminal penalties—up to 5 years in prison with a fine of as much as US $500,000.&lt;/p&gt;
&lt;p&gt;Worse in my view is the shadow Section 1201 casts over people who want to do things like hack their own digital devices, so that they can run whatever software they want on them. Such hacking can be viewed as circumvention of technical prevention measures protecting the device’s original software and is thus illegal according to the DMCA.&lt;/p&gt;
&lt;p&gt;The legislators who created the DMCA did, however, build in something of a safety valve: They stipulated that every three years the Librarian of Congress would carve out certain exemptions to the law’s anti-circumvention provisions. The &lt;a shape="rect" href="http://www.theverge.com/2015/10/27/9622066/jailbreak-unlocked-tablet-smart-tvs-dmca-exemption-library-of-congress"&gt;last set of exemptions&lt;/a&gt; allow for such things as jailbreaking phones and tablets but not e-readers or gaming devices. Those exemptions were issued in October 2015, so if you’re hoping for additional ones, you have to be patient and wait for 2018’s rulings. But there’s no guarantee that the exceptions allowed now will be renewed in 2018, so entrepreneurs can’t base any long-term business decisions on the Librarian’s current stance on these questions.&lt;/p&gt;
&lt;aside class="inlay pullquote lt med"&gt;Matthew Green’s concerns with the DMCA are similar to those of many computer-security researchers: Basically, this law makes it very hard for them to do their jobs&lt;/aside&gt;
&lt;p&gt;It’s no wonder that the EFF teamed up with bunnie Huang to argue the absurdity of the current situation. Huang, who last wrote for &lt;em&gt;Spectrum&lt;/em&gt; about his &lt;a shape="rect" href="http://spectrum.ieee.org/consumer-electronics/portable-devices/novena-a-laptop-with-no-secrets"&gt;open-source laptop project&lt;/a&gt;, is also the creator of something called &lt;a shape="rect" href="https://www.adafruit.com/product/609"&gt;NeTV&lt;/a&gt;, which allows its users to overlay web content on an HDMI video stream. To build the device, Huang had to accomplish the seemingly impossible and make it work &lt;a shape="rect" href="https://rdist.root.org/2011/09/13/the-magic-inside-bunnies-new-netv/"&gt;without ever decrypting the video&lt;/a&gt;, which is scrambled as it passes between, say, a DVD player and a television, using something called High-bandwidth Digital Content Protection, or &lt;a shape="rect" href="https://en.wikipedia.org/wiki/High-bandwidth_Digital_Content_Protection"&gt;HDCP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Huang would like his NeTV to do more—to be what he calls a NeTVCR, a device that could, for example, allow you to record a video stream and play it back later, just like the good old days of tape-based VCRs. Or it could allow you to show live coverage of an event from multiple news sources on one screen. Indeed, there are lots of non-infringing uses for such a device, probably more than even Huang can think of right now. But we’ll never know if he’s prevented from building and selling the device because of the DMCA.&lt;/p&gt;
&lt;p&gt;Huang is the perfect guy to be on this suit for another reason—he’s very smart, not just about technology but also about copyright. I can attest to that through personal experience. While editing &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/design/the-death-of-moores-law-will-spur-innovation"&gt;an article he wrote&lt;/a&gt; for &lt;em&gt;Spectrum&lt;/em&gt; about Moore’s Law, he pointed out to me that one of the drawings he provided didn’t require permission from the original publisher (in this case Apple Computer), because it would be used in this context to parody the company—parody being a recognized fair use. (In the end, we didn’t include that drawing, but boy was I impressed by his grasp of copyright.)&lt;/p&gt;
&lt;p&gt;Matthew Green, a computer scientist at Johns Hopkins University in Baltimore, is also a most appropriate plaintiff. His concerns with the DMCA are similar to those of many computer-security researchers: Basically, this law makes it very hard for them to do their jobs. That’s because often product manufacturers would rather not have security vulnerabilities in their computerized devices come to light. And they can use Section 1201 of the DMCA to discourage researchers from probing their equipment too closely by threatening legal action.&lt;/p&gt;
&lt;p&gt;Green, moreover, would like to write a book that includes information about circumventing technical-prevention measures. But he is prevented from doing that by the DMCA, under which he could be subject to criminal penalties if he collects royalties. Green is also interested in studying security flaws in, for example, medical devices, toll-collection systems, and wireless communication systems that connect vehicles with one another and with highway infrastructure. But the chilling effects of the DMCA put a damper on such activity to the detriment of all of us.&lt;/p&gt;
&lt;p&gt;If you haven’t figured it out already, I’m a biased observer and would take great pleasure in seeing Huang, Green, and the EFF prevail. That, in my view, would help restore some much-needed balance to the world of copyright law.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Mon, 25 Jul 2016 18:00:00 GMT</pubDate>
<dc:creator>David Schneider</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/software/dmca-goes-on-trial</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MjQwMg.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MjQwMA.jpg" height="225" width="300"/>
</item>
<item>
<title>Ethereum Blockchain Forks to Return Stolen Funds</title>
<link>http://spectrum.ieee.org/tech-talk/telecom/internet/ethereum-blockchain-forks-to-return-stolen-funds</link>
<description>Advocates of an Ethereum "hard fork" got what they wanted, and Ether is now rebounding on the markets. But the future of the technology remains unclear</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Advocates of an Ethereum "hard fork" got what they wanted, and Ether is now rebounding on the markets. But the future of the technology remains unclear&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MTkxNg.jpeg"/&gt;
&lt;figcaption&gt;Illustration: Hermin Utomo/Alamy&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Remember that $60 million dollar blockchain heist that made headlines last month? You know, the one that threatened to throw the Ethereum project (the most successful cryptocurrency after Bitcoin) off the tracks? Well, as of today, it has all been magically fixed. On Wednesday, the users, miners, developers, exchanges, and everyone else who matters on the network embraced a fork of the Ethereum software. That effectively confiscated all of the stolen funds and placed them into a new account programmed to automatically reimburse victims of the theft. &lt;/p&gt;
&lt;p&gt;The maneuver, &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/networks/hacked-blockchain-fund-the-dao-chooses-a-hard-fork-to-redistribute-funds"&gt;which was the focus of much philosophical and technical debate&lt;/a&gt;, seems to have worked well enough to call it a success. However, not everyone in the network went along with the fork. There are now two versions of the Ethereum blockchain growing in tandem—one (ETH) with the updates to the stolen funds and one (ETHC) that keeps everything as it was. You can see both chains growing &lt;a shape="rect" href="http://fork.ethstats.net/"&gt;here&lt;/a&gt;. About 15 percent of miners have continued to mine new blocks on the original Ethereum blockchain. However, none of the major online exchanges are listing or trading the coins generated on the un-forked chain. And so, it could be argued that at this point that those coins have no real value.&lt;/p&gt;
&lt;p&gt;The forked chain, on the other hand, is &lt;a shape="rect" href="http://coinmarketcap.com/"&gt;performing well on exchanges&lt;/a&gt;. The price of Ethereum’s native currency, Ether, crashed right after The DAO—a smart contract-enabled investment fund—was hacked and drained of 3.6 million Ether. It now seems to be making a slow recovery.&lt;/p&gt;
&lt;p&gt;A 13-percent increase in the market value of Ether does indeed signal a renewed faith in the overall viability of the Ethereum project. But, even as it seems to have been a success, the bailout of The DAO is likely to generate a lot of much needed discussion. The hard fork Ethereum pulled off this week marks the first time a protocol change has been written and adopted with the explicit goal of confiscating funds. Next week, we will take a look at what kinds of precedents this could set, both in terms of how the Ethereum community makes controversial decisions and what kinds of social intervention users will accept.&lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 22 Jul 2016 21:00:00 GMT</pubDate>
<dc:creator>Morgen E. Peck</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/telecom/internet/ethereum-blockchain-forks-to-return-stolen-funds</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MTkyNw.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MTkyNQ.jpg" height="225" width="300"/>
</item>
<item>
<title>Transistors Will Stop Shrinking in 2021, Moore’s Law Roadmap Predicts</title>
<link>http://spectrum.ieee.org/tech-talk/computing/hardware/transistors-will-stop-shrinking-in-2021-moores-law-roadmap-predicts</link>
<description>The last ITRS report forecasts an end to traditional 2D scaling</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The last ITRS report forecasts an end to traditional 2D scaling&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MTM2Nw.png"/&gt;
&lt;figcaption&gt;Illustration: Erik Vrielink&lt;/figcaption&gt;
&lt;figcaption&gt;The trajectory of transistor feature sizes (the physical gate length of transistors in high-performance logic is shown here) could take a sharp turn in 2021.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;After more than 50 years of miniaturization, the transistor could stop shrinking in just five years. That is the prediction of the 2015 International Technology Roadmap for Semiconductors, which was &lt;a shape="rect" href="http://www.semiconductors.org/news/2016/07/08/press_releases_2015/international_technology_roadmap_for_semiconductors_examines_next_15_years_of_chip_innovation/"&gt;officially released&lt;/a&gt; earlier this month. &lt;/p&gt;
&lt;p&gt;After 2021, the report forecasts, it will no longer be economically desirable for companies to continue to shrink the dimensions of transistors in microprocessors. Instead, chip manufacturers will turn to other means of boosting density, namely turning the transistor from a horizontal to a vertical geometry and building multiple layers of circuitry, one on top of another. &lt;/p&gt;
&lt;p&gt;For some, this change will likely be interpreted as another death knell for Moore’s Law, the repeated doubling of transistor densities that has given us the extraordinarily capable computers we have today. Compounding the drama is the fact that this is the last ITRS roadmap, the end to a more-than-20-year-old coordinated planning effort that began in the United States and was then expanded to include the rest of the world.&lt;/p&gt;
&lt;p&gt;Citing waning industry participation and an interest in pursuing other initiatives, the &lt;a shape="rect" href="http://www.semiconductors.org/about_us/about_sia/"&gt;Semiconductor Industry Association&lt;/a&gt;—a U.S. trade group that represents the interests of IBM, Intel, and other companies in Washington and a key ITRS sponsor—will do its own work, in collaboration with another industry group, the &lt;a shape="rect" href="https://www.src.org/"&gt;Semiconductor Research Corporation&lt;/a&gt;, to &lt;a shape="rect" href="http://www.semiconductors.org/clientuploads/Resources/RITR%20WEB%20version%20FINAL.pdf"&gt;identify research priorities&lt;/a&gt; for government- and industry-sponsored programs. Other ITRS participants are expected to continue on with a new roadmapping effort under a new name, which will be conducted as part of an IEEE initiative called &lt;a shape="rect" href="http://rebootingcomputing.ieee.org/"&gt;Rebooting Computing&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;These roadmapping shifts may seem like trivial administrative changes. But “this is a major disruption, or earthquake, in the industry,” says analyst Dan Hutcheson, of the firm &lt;a shape="rect" href="https://www.vlsiresearch.com/"&gt;VLSI Research&lt;/a&gt;. U.S. semiconductor companies had reason to cooperate and identify common needs &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=500069"&gt;in the early 1990’s&lt;/a&gt;, at the outset of the &lt;a shape="rect" href="https://www.src.org/src/story/timeline/1992/"&gt;roadmapping effort&lt;/a&gt; that eventually led to the ITRS’s creation in 1998. Suppliers had a hard time identifying what the semiconductor companies needed, he says, and it made sense for chip companies to collectively set priorities to make the most of limited R&amp;amp;D funding. &lt;/p&gt;
&lt;p&gt;But the difficulty and expense associated with maintaining the leading edge of Moore’s Law has since resulted in significant consolidation. By Hutcheson’s count, 19 companies were developing and manufacturing logic chips with leading-edge transistors in 2001. Today, there are just four: Intel, TSMC, Samsung, and GlobalFoundries. (Until recently, IBM was also part of that cohort, but its chip fabrication plants were sold to GlobalFoundries.)&lt;/p&gt;
&lt;p&gt;These companies have their own roadmaps and can communicate directly to their equipment and materials suppliers, Hutcheson says. What’s more, they’re fiercely competitive. “They don’t want to sit in a room and talk about what their needs are,” Hutcheson says. “It’s sort of like everything’s fun and games when you start off at the beginning of the football season, but by the time you get down to the playoffs it’s pretty rough.”&lt;/p&gt;
&lt;p&gt;“The industry has changed,” agrees Paolo Gargini, chair of the ITRS, but he highlights other shifts. Semiconductor companies that no longer make leading-edge chips in house rely on the foundries that make their chips to provide advanced technologies. What’s more, he says, chip buyers and designers—companies such as Apple, Google, and Qualcomm—are increasingly dictating the requirements for future chip generations. “Once upon a time,” Gargini says, “the semiconductor companies decided what the semiconductor features were supposed to be. This is no longer the case.”&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;This final ITRS report&lt;/strong&gt; is titled &lt;a shape="rect" href="http://www.itrs2.net/"&gt;ITRS 2.0&lt;/a&gt;. The name reflects the idea that improvements in computing are no longer driven from the bottom-up, by tinier switches and denser or faster memories. Instead, it takes a more top-down approach, focusing on the applications that now drive chip design, such as data centers, the Internet of Things, and mobile gadgets. &lt;/p&gt;
&lt;p&gt;The new IEEE roadmap—the International Roadmap for Devices and Systems—will also take this approach, but it will add computer architecture to the mix, allowing for “a comprehensive, end-to-end view of the computing ecosystem, including devices, components, systems, architecture, and software,” according to a recent &lt;a shape="rect" href="http://rebootingcomputing.ieee.org/images/files/pdf/rc_irds.pdf"&gt;press release&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Transistor miniaturization was still a part of the long-term forecast as recently as 2014, when the penultimate ITRS report &lt;a shape="rect" href="http://www.semiconductors.org/news/2014/04/01/press_releases_2013/international_technology_roadmap_for_semiconductors_explores_next_15_years_of_chip_technology/"&gt;was released&lt;/a&gt;. That report predicted that the physical gate length of transistors—an indicator of how far current must travel in the device—and other key logic chip dimensions would continue to shrink until at least 2028. But since then, 3D concepts have gained momentum. The memory industry has &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/design/chipmakers-push-memory-into-the-third-dimension"&gt;already turned to 3D architectures&lt;/a&gt; to ease miniaturization pressure and boost the capacity of NAND Flash. &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/design/the-rise-of-the-monolithic-3d-chip"&gt;Monolithic 3D integration&lt;/a&gt;, which would build layers of devices one on top of another, connecting them with a dense forest of wires, has also been an increasingly popular subject of discussion.&lt;/p&gt;
&lt;p&gt;The new report embraces these trends, predicting an end to traditional scaling—the shrinking of chip features—by the early 2020’s. But the idea that we’re now facing an end to Moore’s Law “is completely wrong,” Gargini says. “The press has invented multiple ways of defining Moore’s Law but there is only one way: The number of transistors doubles every two years.” &lt;/p&gt;
&lt;p&gt;Moore’s Law, he emphasizes, is simply a prediction about how many transistors can fit in a given area of IC—whether it’s done, as it has been for decades, in a single layer or by stacking multiple layers. If a company really wanted to, Gargini says, it could continue to make transistors smaller well into the 2020s, “but it’s more economic to go 3-D. That’s the message we wanted to send.” &lt;/p&gt;
&lt;p&gt;There are other changes on the horizon. In the coming years, before 3-D integration is adopted, the ITRS predicts that leading-edge chip companies will move away from the transistor structure used now in high-performance chips: the FinFET. This device has a gate draped around three sides of a horizontal, fin-shaped channel to control the flow of current. According to the roadmap, chipmakers will leave that in favor of a &lt;a shape="rect" href="http://electroiq.com/blog/2016/06/imec-demonstrates-gate-all-around-mosfets-with-lateral-silicon-nanowires-at-scaled-dimensions/"&gt;lateral, gate-all-around device&lt;/a&gt; that has a horizontal channel like the FinFET but is surrounded by a gate that extends underneath as well. After that, transistors will become vertical, with their channels taking the form of pillars or nanowires standing up on end. The traditional silicon channel will also be replaced by channels made with alternate materials, namely silicon germanium, germanium, and compounds drawn from columns III and V of the old periodic table.&lt;/p&gt;
&lt;p&gt;These changes will allow companies to pack more transistors in a given area and so adhere to the letter of Moore’s Law. But keeping to the spirit of Moore’s Law—the steady improvement in computing performance—is another matter. &lt;/p&gt;
&lt;p&gt;The doubling of transistor densities hasn’t been linked to improvements in computing performance for some time, notes &lt;a shape="rect" href="http://www.scs.gatech.edu/people/9719/thomas-contes"&gt;Tom Conte&lt;/a&gt;, the 2015 president of the IEEE Computer Society and a co-leader of the IEEE Rebooting Computing Initiative.&lt;/p&gt;
&lt;p&gt;For a long time, shrinking transistors meant faster speeds. But in the mid-1990’s, Conte says, the extra metal layers that were added to wire up increasing numbers of transistors were adding significant delays, and engineers redesigned &lt;a shape="rect" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=372029"&gt;chip microarchitectures&lt;/a&gt; to improve performance. A decade later, transistor densities were so high that their heat limited clock speeds. Companies began &lt;a shape="rect" href="http://spectrum.ieee.org/semiconductors/processors/multicore-cpu-processor-proliferation"&gt;packing multiple cores on chips&lt;/a&gt; to keep things moving. &lt;/p&gt;
&lt;p&gt;“We’ve been living in this bubble where the computing industry could rely on the device side to do their job, and so the computer industry and the device industry really had this very nice wall between them,” says Conte. “That wall really started to crumble in 2005, and since that time we’ve been getting more transistors but they’re really not all that much better.” &lt;/p&gt;
&lt;p&gt;This crumbling wall was a strong motivation for the IEEE Rebooting Computing Initiative to begin collaborating with the ITRS last year, before the launch of the IRDS. “I like to say we could see the light at the end of the tunnel, and we knew it was an oncoming train,” says Conte.&lt;/p&gt;
&lt;p&gt;The initiative held a summit last December that covered a gamut of potential future computing technologies, including new kinds of transistors and memory devices, neuromorphic computing, superconducting circuitry, and processors that use approximate instead of exact answers.&lt;/p&gt;
&lt;p&gt;The first international &lt;a shape="rect" href="http://www.cvent.com/events/2016-ieee-international-conference-on-rebooting-computing-icrc-/event-summary-195d6afcb7a649f3ac3cb5780ccec4d5.aspx"&gt;Rebooting Computing conference&lt;/a&gt; will be held in October this year; IRDS meetings will coincide with such events, Conte says. The IRDS will still track “Moore’s Law to the bitter end,” Conte explains. But the roadmapping focus has changed: “This isn’t saying this is the end of Moore’s Law,” he says. “It’s stepping back and saying what really matters here—and what really matters here is computing.”&lt;/p&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 22 Jul 2016 16:00:00 GMT</pubDate>
<dc:creator>Rachel Courtland</dc:creator>
<guid>http://spectrum.ieee.org/tech-talk/computing/hardware/transistors-will-stop-shrinking-in-2021-moores-law-roadmap-predicts</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MTM3OA.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MTM3Ng.jpg" height="225" width="300"/>
</item>
<item>
<title>Silencing Transformers with Numerical Simulation</title>
<link>http://spectrum.ieee.org/computing/software/silencing-transformers-with-numerical-simulation</link>
<description>At ABB, engineers develop multiphysics models and custom applications to minimize noise level in transformers</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;At ABB, engineers develop multiphysics models and custom applications to minimize noise level in transformers&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc1MTY0MA.png"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc1MTYyOA"/&gt;
&lt;/div&gt;
&lt;p&gt;Of the extensive equipment required for power flow control and safety across the grid, transformers are needed for adjusting voltage levels in power transmission lines. However, unwanted noise is produced by vibrations in the transformer and by the fans and pumps used in the cooling system. This sound is often noticeable as a faint humming or buzzing, audible to passersby who get close enough.&lt;/p&gt;
&lt;p&gt;Reducing this noise is necessary and desirable due to both safety regulations and comfort. ABB, a major transformer manufacturer with offices throughout the world, makes use of numerical simulation in COMSOL Multiphysics® software to analyze the multiple sources of sound from transformers. In their endeavors to understand and optimize ABB's transformer designs for minimal hum, Romain Haettel, Anders Daneryd, and Mustafa Kavasoglu of ABB Corporate Research Center (CRC) modeled the mechanical, acoustic, and electromagnetic behavior of the transformer components coupled together in one simulation.&lt;/p&gt;
&lt;p&gt;The transformer design in question comprises a magnetostrictive metal core with coils around different sections, a tank enclosure, and insulating oil in between. The CRC team modeled the magnetic flux generated in the core and coil windings; the mechanical strains and displacements in the core due to magnetostriction, which cause vibrations; and the Lorentz forces due to alternating currents in the coils. This allowed them to understand how all these factors influence the acoustic pressure in the tank, and ultimately the levels of sound emitted.&lt;/p&gt;
&lt;p&gt;They also analyzed the relationships between different design parameters, such as tank thickness and material properties, and the resulting hum. Putting all of their newfound understanding together, they adapted the geometry and configuration of the transformer components to minimize noise and provide the best possible designs.&lt;/p&gt;
&lt;p&gt;Using the Application Builder tool available in COMSOL Multiphysics® the team built a simulation app to extend the knowledge gained from their simulations to the rest of the designers at ABB as well as to the business unit. The customized apps created from their simulation simplify the testing and verification process for other departments by letting app users check how different designs would affect the final transformer hum without needing to be familiar with the original mathematical models. Haettel, Kavasoglu, and Daneryd made their simulation available to their colleagues, spreading their analytical capabilities throughout the company and expanding the reach of simulation for design improvements.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a shape="rect" href="http://comsol.com/c/3wzb"&gt;http://comsol.com/c/3wzb&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Fri, 22 Jul 2016 15:48:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/silencing-transformers-with-numerical-simulation</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc1MTY0OA.png" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc1MTY0Ng.png" height="225" width="300"/>
</item>
<item>
<title>Biometrics Researcher Asks: Is That Eyeball Dead or Alive?</title>
<link>http://spectrum.ieee.org/the-human-os/biomedical/imaging/biometric-researcher-asks-is-that-eyeball-alive-or-dead</link>
<description>Iris scanners provide excellent biometric identification, but they can be spoofed</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Iris scanners provide excellent biometric identification, but they can be spoofed&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MDM4Nw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Joseph McNally/Getty Images&lt;/figcaption&gt;
&lt;figcaption&gt;Every human's iris is unique. Biometric identification systems look at the pattern of fibers in the iris. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Imagine a high-security facility where government official X must verify her identity by positioning her eyes in front of an iris scanner. This kind of biometric identification is far more accurate than fingerprint scanning, but there’s still a catch. A miscreant may be able to fool the system by simply holding up a high-res photo of X’s eye. Or, in a grislier scenario, the imposter could come equipped with X&lt;span&gt;’&lt;/span&gt;s actual eyeball, which had been plucked from its owner&lt;span&gt;’&lt;/span&gt;s head.&lt;br clear="none"/&gt;
&lt;br clear="none"/&gt;
Biometrics researcher Adam Czajka is on a mission to prevent both of these kinds of failures. He studies iris “liveness” detection at both the &lt;a shape="rect" href="https://engineering.nd.edu/profiles/aczajka"&gt;University of Notre Dame&lt;/a&gt; and &lt;a shape="rect" href="http://zbum.ia.pw.edu.pl/EN/node/37"&gt;Warsaw University of Technology&lt;/a&gt;, inventing technologies to ensure that iris scanners won’t get duped.&lt;/p&gt;
&lt;p&gt;He also studies post-mortem eyes, and has learned enough to upset the conventional wisdom, which held that dead eyes are useless for iris identification. Even many hours after death, Czajka found, irises are perfectly viable, which makes the grisly scenario mentioned earlier perfectly plausible. &lt;/p&gt;
&lt;p&gt;First, the basics: The &lt;a shape="rect" href="https://en.wikipedia.org/wiki/Iris_(anatomy)"&gt;iris&lt;/a&gt;, the colored ring of muscle that controls the contraction and dilation of the pupil, is composed of tiny fibers that form an intricate and unique pattern in each individual’s eye. Iris scanners use both visible and near-infrared light to look at hundreds of points within these patterns, then try to match them with a registered profile. &lt;/p&gt;
&lt;aside class="inlay pullquote lt med"&gt;The most basic challenge for iris scanners: distinguishing a real iris from a paper printout. &lt;/aside&gt;
&lt;p&gt;This identification method is catching on because it has some key advantages over fingerprint-based methods. Iris scanning is useful for airport immigration stations, because it’s fast and it doesn’t require the user to touch anything. It also relies on a stable and protected body part, while fingerprints can be damaged or &lt;a shape="rect" href="http://www.thedailybeast.com/articles/2013/09/12/new-iphone-a-problem-for-people-who-lack-fingerprints.html"&gt;worn down by manual labor&lt;/a&gt;. (That’s why the Indian government is registering both fingerprints and irises as part of its &lt;a shape="rect" href="http://spectrum.ieee.org/computing/software/indias-big-bet-on-identity"&gt;massively ambitious biometrics project&lt;/a&gt;, whose aim is to give every one of India’s 1.2 billion citizens a unique ID number.) &lt;/p&gt;
&lt;p&gt;Biometrics experts know that recognizing a live eye is a real issue. Czajka helped run an &lt;a style="font-family: Georgia, serif; font-size: 18px; line-height: 25px;" shape="rect" href="http://iris2015.livdet.org/index.php"&gt;iris liveness detection competition&lt;/a&gt;
&lt;span&gt; at the &lt;a shape="rect" href="http://icb2016.hh.se/Welcome"&gt;International Conference on Biometrics&lt;/a&gt; last month. The organizers presented the most basic challenge: distinguishing a real iris from a paper printout or a printed contact lens. “A printout is enough to fool some commercial systems we had in the lab,” Czajka says in an interview.&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;To spot a printout, common methods look for the reflection from a moist cornea, use thermal imaging to detect the real eye’s warmth, or look for microscopic evidence of the dot-matrix patterns produced by printers. While the results of this competition have yet to be published, Czajka says they’re a big improvement over &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6996283"&gt;results from the previous competition&lt;/a&gt;, held in 2013. In that first contest, the best algorithm rejected 29 percent of live samples and erroneously accepted 6 percent of printouts. “I’m happy to see that the field is evolving,” Czajka says.&lt;/p&gt;
&lt;p&gt;But those methods wouldn’t necessarily distinguish a real live eye from a real dead eye. So Czajka employs a more sophisticated approach. In his recent work, he used &lt;a shape="rect" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=7029052"&gt;pupil dynamics&lt;/a&gt; to both identify an individual and detect liveness.&lt;/p&gt;
&lt;p&gt;Each person’s pupil contracts or dilates in slightly different ways in response to changes in light conditions, he found. So when initially enrolling people in an ID system, Czajka’s scanner records their pupils’ unique responses to flashes of light. When they later present themselves for identification, the iris scanner can emit a pulse of light to see if their pupil dynamics match those in their profiles.&lt;/p&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;figure&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MDM4NQ.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Image: Adam Czajka&lt;/figcaption&gt;
&lt;figcaption&gt;The experimental system automatically measures a pupil's response to a light stimulus.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;His result: perfect performance in under 3 seconds. Of course, Czajka adds, the scanner was working in lab conditions, so it’s not yet clear how it will perform in the real world, where lighting conditions vary. He’s also interested in studying various complicating factors, like whether pupil dynamics change if the person has had a couple of drinks or is under stress. &lt;/p&gt;
&lt;p&gt;Czajka next decided to expand his research to the properties of &lt;a shape="rect" href="http://zbum.ia.pw.edu.pl/PAPERS/Trokielewicz_Czajka_Maciejewicz_ICB2016.pdf"&gt;post-mortem eyes&lt;/a&gt; because he’d seen many biometrics scientists and companies make a certain assumption: “It’s a common belief that just a few minutes or an hour after someone dies, the iris is completely unusable for biometrics,” he says. “We decided to check this.”&lt;/p&gt;
&lt;p&gt;He collaborated with a colleague from the University of Warsaw’s medical school, &lt;a shape="rect" href="https://www.researchgate.net/profile/Piotr_Maciejewicz"&gt;Piotr Maciejewicz&lt;/a&gt;, who obligingly photographed eyes in the morgue. They used both infrared and visible light cameras to collect images, getting the first set at 5 hours after death, and doing two other sessions at about 16 and 27 hours after death. Czajka and PhD student &lt;a shape="rect" href="http://zbum.ia.pw.edu.pl/PL/node/72"&gt;Mateusz Trokielewicz&lt;/a&gt; then tested whether several commercial products and one open-source iris matching system worked on the dead eyes.&lt;/p&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;figure&gt;&amp;lt;&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MDM4Ng.jpeg"/&gt;
&lt;figcaption class="hi-cap"&gt;Image: Adam Czajka&lt;/figcaption&gt;
&lt;figcaption&gt;This post-mortem eye was photographed in both infrared and visible light over the course of about a day.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;span&gt;He found that the dead eyes were easily recognizable in the first session, and some could still be used for identification as late as 27 hours after the person’s demise. While the cornea gradually became cloudy over time as the corneal cells broke down, that opaqueness didn’t interfere with the infrared imaging. Czajka’s now conducting research using eyes that are several days or even weeks past their expiration date. &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The upshot: “We are sure that irises can be used as biometric identifiers for at least a few days after death,” says Czajka. “That’s why liveness detection is so important.” &lt;/p&gt;
&lt;p&gt;You heard him, people. Until he cracks the problem, hold onto your eyeballs. &lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 20 Jul 2016 18:30:00 GMT</pubDate>
<dc:creator>Eliza Strickland</dc:creator>
<guid>http://spectrum.ieee.org/the-human-os/biomedical/imaging/biometric-researcher-asks-is-that-eyeball-alive-or-dead</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MDQwMQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MDM5OQ.jpg" height="225" width="300"/>
</item>
<item>
<title>How the World’s Most Powerful Supercomputer Inched Toward the Exascale</title>
<link>http://spectrum.ieee.org/computing/hardware/how-the-worlds-most-powerful-supercomputer-inched-toward-the-exascale</link>
<description>The powerful Sunway TaihuLight supercomputer makes some telling trade-offs in pursuit of efficiency</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;The powerful Sunway TaihuLight supercomputer makes some telling trade-offs in pursuit of efficiency&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc4OTczNw.jpeg"/&gt;
&lt;figcaption&gt;Photo: Jack Dongarra&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;figure class="xlrg" role="img"&gt;
&lt;img alt="/image/Mjc4OTczNg.jpeg" src="http://spectrum.ieee.org/image/Mjc4OTczNg.jpeg"/&gt;
&lt;div class="ai"&gt;
&lt;figcaption class="hi-cap"&gt;Photo: Jack Dongarra&lt;/figcaption&gt;
&lt;figcaption&gt;
&lt;strong&gt;An Efficient Engine:&lt;/strong&gt; Sunway TaihuLight’s computations-per-watt improvement is even more impressive than its raw computing power.     &lt;/figcaption&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;In June, the ranks of the &lt;a shape="rect" href="https://www.top500.org/lists/2016/06/"&gt;Top500&lt;/a&gt; list were rearranged, and the title of world’s most powerful supercomputer was handed off to a new machine—China’s &lt;a shape="rect" href="http://spectrum.ieee.org/tech-talk/computing/hardware/china-beats-worlds-mostpowerful-supercomputer-with-a-new-one"&gt;Sunway TaihuLight&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Wuxi-based machine can perform the Linpack Benchmark—a long-standing arbiter of supercomputer prowess—at a rate of 93 petaflops, or 93 quadrillion floating-point operations per second. This performance is more than twice that of the previous record holder, China’s Tianhe-2. What’s more, TaihuLight achieves this capacity while consuming 2.4 megawatts less power than Tianhe-2.&lt;/p&gt;
&lt;p&gt;Such efficiency gains are important if supercomputer designers hope to reach exascale operation, somewhere in the realm of 1,000 Pflops. Computers with that capability could be a boon for advanced manufacturing and national security, among many other applications. China, Europe, Japan, and the United States are all pushing toward the exascale range. Some countries are reportedly setting their sights on doing so &lt;a shape="rect" href="https://www.top500.org/news/the-four-way-race-to-exascale/"&gt;by 2020&lt;/a&gt;; the United States is targeting the early 2020s. But two questions loom over those efforts: How capable will those computers be? And can we make them energy efficient enough to be economical?&lt;/p&gt;
&lt;p&gt;We can get to the exascale now “if you’re willing to pay the power bill,” says &lt;a shape="rect" href="http://www3.nd.edu/~kogge/"&gt;Peter Kogge&lt;/a&gt;, a professor at the University of Notre Dame. Scaling up a supercomputer with today’s technology to create one that is 10 times as big would demand at least 10 times as much power, Kogge explains. And the difference between 20 MW and 200 MW, he says, “is the difference [between having] a substation or a nuclear power plant next to you.”&lt;/p&gt;
&lt;p&gt;Kogge, who led &lt;a shape="rect" href="http://spectrum.ieee.org/computing/hardware/nextgeneration-supercomputers"&gt;a 2008 study on reaching the exascale&lt;/a&gt;, is updating power projections to cover the three categories of supercomputers built today: those with “heavyweight” high-performance CPUs; those that use “lightweight” microprocessors that are slower but cooler, and so can be packed more densely; and those that take advantage of graphics processing units to accelerate computation.&lt;/p&gt;
&lt;aside class="inlay pullquote rt med-lrg"&gt;“They produced a processor that can deliver high arithmetic performance but is very weak in terms of data movement”&lt;span class="pq-attrib"&gt;—Jack Dongarra, University of Tennessee&lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;TaihuLight follows the lightweight approach, and it has made some sacrifices in pursuit of energy efficiency. Based on its hardware specs, TaihuLight can, in theory, crunch numbers at a rate of 125 Pflops. The machine reaches 74 percent of this peak theoretical capacity when running Linpack. But it does not fare as well on a new alternative benchmark, &lt;a shape="rect" href="http://www.hpcg-benchmark.org/index.html"&gt;High Performance Conjugate Gradients&lt;/a&gt; (HPCG), which is designed to reflect how well a computer can perform more memory- and communications-intensive, real-world applications. When it runs HPCG, TaihuLight utilizes just 0.3 percent of its theoretical peak abilities, which means that only 3 out of every 1,000 possible floating-point operations are actually used by the computer. By comparison, Tianhe-2 and the United States’ Titan, the second- and third-fastest supercomputers in the Top500 rankings, respectively, can take advantage of just over 1 percent of their computing capacity. Japan’s K computer, currently ranked fifth on the list, achieved 4.9 percent with the HPCG metric.&lt;/p&gt;
&lt;p&gt;“Everything is a balancing act,” says &lt;a shape="rect" href="http://www.netlib.org/utk/people/JackDongarra/"&gt;Jack Dongarra&lt;/a&gt;, a professor at the University of Tennessee, Knoxville, and one of the organizers of the &lt;a shape="rect" href="https://www.top500.org/"&gt;Top500&lt;/a&gt;. “They produced a processor that can deliver high arithmetic performance but is very weak in terms of data movement.” But he notes that the TaihuLight team has developed applications that take advantage of the architecture; he says that three projects that were finalists for this year’s ACM Gordon Bell Prize, a prestigious supercomputing award, were designed to run on the machine.&lt;/p&gt;
&lt;p&gt;TaihuLight uses DDR3, an older, slower memory, to save on power. Its architecture also uses small amounts of local memory near each core instead of a more traditional memory hierarchy, explains &lt;a shape="rect" href="http://www.cs.manchester.ac.uk/about-us/staff/profile/?ea=john.goodacre"&gt;John Goodacre&lt;/a&gt;, a professor of computer architectures at the University of Manchester, in England. He says that while today’s applications can execute between 1 and 10 floating-point operations for every byte of main memory accessed, that ratio needs to be far higher for applications to run efficiently on TaihuLight. The design cuts down on a big expense in a supercomputer’s power budget: the amount of energy consumed shuttling data back and forth.&lt;/p&gt;
&lt;p&gt;“I think what they’ve done is build a machine that changes some of the design rules that people have assumed are part of the requirements” for moving toward the exascale, Goodacre says. Further progress will depend, as the TaihuLight team has shown, on end-to-end design, he says. That includes looking not only at changes to hardware—a number of experts point to 3D stacking of logic and memory—but also to the fundamental programming paradigms we use to take advantage of the machines.&lt;/p&gt;
&lt;p&gt;
&lt;em&gt;This article appears in the August 2016 print issue as “China Inches Toward the Exascale.”&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 20 Jul 2016 15:00:00 GMT</pubDate>
<dc:creator>Rachel Courtland</dc:creator>
<guid>http://spectrum.ieee.org/computing/hardware/how-the-worlds-most-powerful-supercomputer-inched-toward-the-exascale</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc4OTc0Ng.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc4OTc0NA.jpg" height="225" width="300"/>
</item>
<item>
<title>Optimized Induction Stove Design at Miele</title>
<link>http://spectrum.ieee.org/computing/software/optimized-induction-stove-design-at-miele</link>
<description>Numerical simulation helps engineers to design safer and more efficient induction stoves</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;Numerical simulation helps engineers to design safer and more efficient induction stoves&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc5MDMwNw.png"/&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;div class="imgWrapper xlrg"&gt;
&lt;img alt="img" src="http://spectrum.ieee.org/image/Mjc5MDMwMA.png"/&gt;
&lt;/div&gt;
&lt;p/&gt;
&lt;/div&gt;
&lt;p&gt;Miele, a leading manufacturer of induction stoves, set out to combine the efficiency of induction stoves with a level of precision that would make the experience surpass cooking with a gas stove. Induction stoves, commonly used in the laboratory before their introduction as a home appliance, are inherently more efficient than either gas or traditional electric stove tops, as the pan itself is heated, rather than the atmosphere surrounding the pan. The unparalleled efficiency of induction stoves would easily make them the preferred stovetop for the kitchen.&lt;/p&gt;
&lt;p&gt;Engineers at Mieletec FH Bielefeld, a joint research laboratory between Miele &amp;amp; Cie. KG and the &lt;span&gt;Bielefeld University of Applied Sciences in Germany, &lt;/span&gt;used COMSOL Multiphysics® software to optimize their stove design based on the inductive heating effect. A pot or pan made of a conducting material is placed above copper coils such that the magnetic field produced by the coils induces a current within the pot or pan. The heating effect is due to Joule heating, or resistive heating throughout the pot or pan.&lt;/p&gt;
&lt;p&gt;"Initial adjustments to the induction stove design were made by trial and error. However, building new prototypes for incremental gains to the system is costly. Simulation offers an alternative: Simulation allows you to extract data that you would never be able to get from an experiment, with simulation, you can get a better idea about what is going on inside a coil or pot, so that you know what it is you need to optimize.” says Christian Schroder, cofounder and scientific director of Mieletec&lt;/p&gt;
&lt;p&gt;Schroder and his team used COMSOL Multiphysics® software to simulate the induction heating process to address challenges such as the noise emitted from the pots due to magnetostriction, to eliminating movement of the pot due to the external and eddy current magnetic field interacting. “We needed to know the size, shape, and what materials to use &lt;span&gt;to make the coils&lt;/span&gt;”, says Schroder.&lt;/p&gt;
&lt;p&gt;When it comes to comparing simulations with the physical prototype, one test Mieletec uses is cooking a pancake to assess even heating across the surface of a pan. “If the standard pancake comes out burnt in some places, and undercooked in others, then you know the stove isn’t heating effectively, &lt;span&gt;y&lt;/span&gt;
&lt;span&gt;ou want the pancake to be one smooth color and evenly cooked”, &lt;/span&gt;Schroder explains.  The researchers at Mieletec FH Bielefeld were able to reduce the number of experiments needed to finalize their design by 80%, creating a stove with both the precision and responsiveness that chefs demand along with an efficiency that renders the induction stove good for the planet.&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a shape="rect" href="http://comsol.com/c/3wv5"&gt;http://comsol.com/c/3wv5&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Wed, 20 Jul 2016 14:23:00 GMT</pubDate>
<guid>http://spectrum.ieee.org/computing/software/optimized-induction-stove-design-at-miele</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc5MDMxNQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc5MDMxMw.jpg" height="225" width="300"/>
</item>
<item>
<title>Magic Leap Hiring Software Engineers for New Development Lab on Lucasfilm’s San Francisco Campus</title>
<link>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gaming/magic-leap-hiring-software-engineers-for-new-development-lab-on-lucasfilms-san-francisco-campus</link>
<description>What to do with Magic Leap’s augmented reality technology? Developers can ask Yoda.</description>
<content:encoded>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;html&gt;
&lt;body&gt;What to do with Magic Leap’s augmented reality technology? Developers can ask Yoda.&lt;figure&gt;
&lt;img src="http://spectrum.ieee.org/image/Mjc4OTg0NA.jpeg"/&gt;
&lt;figcaption&gt;Image: Lucasfilm&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;
&lt;p&gt;Magic Leap, &lt;a shape="rect" href="http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/audiovideo/magic-leap-draws-from-the-bay-area-to-assemble-its-brain-trust"&gt;the stealthy Florida-based company&lt;/a&gt; that has so impressed investors with its augmented reality demos that they’ve ponied up $1.4 billion to date, is beefing up its efforts to create content. The company has established a development lab on the Lucasfilm campus in San Francisco, located, &lt;a shape="rect" href="http://venturebeat.com/2016/07/17/magic-leap-will-soon-have-a-developer-testing-lab-at-lucasfilm-in-san-francisco/"&gt;Venture Beat reported&lt;/a&gt;, close to the Yoda fountain.&lt;/p&gt;
&lt;p&gt;Last week, &lt;a shape="rect" href="https://www.magicleap.com/#/team"&gt;Magic Leap&lt;/a&gt; founder and CEO Rony Abovitz announced a partnership with Lucasfilm to develop &lt;em&gt;Star Wars-&lt;/em&gt;related apps for Magic Leap. They’ll be using what the company calls its “Mixed Reality Lightfield” technology, the details of which are still under wraps. That project will clearly be happening in the San Francisco Lab. But the company isn’t betting that &lt;em&gt;Star Wars&lt;/em&gt; games alone will be enough to make the technology take off. It plans to draw all sorts of developers into its San Francisco lab to work on applications for the new augmented reality technology.&lt;/p&gt;
&lt;p&gt;Magic Leap may have some serious competition for augmented reality developers; with Pokemon Go capturing the imagination of gamers all over the world, it turns out that the AR explosion didn’t wait for Magic Leap to reveal its technology. AR games can be engaging even if you have to hold a phone up in front of you to peek into an AR world. Pokemon Go could be great news for Magic Leap, in the sense that it’s getting players used to AR technology and whetting their appetites for an AR interface that they don’t have to hold out in front of them. Or it could be a challenge: Can Magic Leap create compelling enough content to justify purchasing an expensive new device.&lt;/p&gt;
&lt;p&gt;To support the developers it attracts, Magic Leap is in the process of hiring a team to work with those developers. Its job listings on LinkedIn currently include a number of posts that will be part of its “advanced content research group in San Francisco.” These include software engineers, infrastructure engineer, interaction engineer, and technical director. Some of these positions also are listed on &lt;a shape="rect" href="https://www.magicleap.com/#/wizards-wanted?loc=San%20Francisco,%20CA"&gt;the company’s internal hiring site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;No word yet as to how Magic Leap will reach out to developers for this lab, though it has made a few forays into the development community, for example, &lt;a shape="rect" href="http://www.theverge.com/2016/5/25/11772102/magic-leap-augmented-mixed-reality-twilio-developer-partnership"&gt;through a contest at Twilio’s Signal developer conference&lt;/a&gt;. Perhaps it could set out a &lt;a shape="rect" href="http://www.bustle.com/articles/172078-how-to-use-lure-modules-in-pokemon-go"&gt;Pokemon Go lure.&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</content:encoded>
<pubDate>Tue, 19 Jul 2016 17:00:00 GMT</pubDate>
<dc:creator>Tekla S. Perry</dc:creator>
<guid>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gaming/magic-leap-hiring-software-engineers-for-new-development-lab-on-lucasfilms-san-francisco-campus</guid>
<media:content url="http://spectrum.ieee.org/image/Mjc4OTg1NQ.jpg" height="373" width="620"/>
<media:thumbnail url="http://spectrum.ieee.org/image/Mjc4OTg1Mw.jpg" height="225" width="300"/>
</item>
</channel>
</rss>
